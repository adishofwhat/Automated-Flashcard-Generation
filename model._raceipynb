{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install rouge_score absl-py\n",
    "# pip install bert_score\n",
    "# pip install evaluate\n",
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"Chelsea's mini-revival continued with a third victory in a row as they consigned struggling Leicester City to a fifth consecutive defeat.\n",
    "Buoyed by their Champions League win over Borussia Dortmund, Chelsea started brightly and Ben Chilwell volleyed in from a tight angle against his old club.\n",
    "Chelsea's Joao Felix and Leicester's Kiernan Dewsbury-Hall hit the woodwork in the space of two minutes, then Felix had a goal ruled out by the video assistant referee for offside.\n",
    "Patson Daka rifled home an excellent equaliser after Ricardo Pereira won the ball off the dawdling Felix outside the box.\n",
    "But Kai Havertz pounced six minutes into first-half injury time with an excellent dinked finish from Enzo Fernandez's clever aerial ball.\n",
    "Mykhailo Mudryk thought he had his first goal for the Blues after the break but his effort was disallowed for offside.\n",
    "Mateo Kovacic sealed the win as he volleyed in from Mudryk's header.\n",
    "The sliding Foxes, who ended with 10 men following Wout Faes' late dismissal for a second booking, now just sit one point outside the relegation zone.\n",
    "\"\"\".replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Token and Position Embeddings The matrix X (of shape [N × d]) has an embedding for  each word in the context.  This embedding is created by adding two distinct  embedding for each input • token embedding • positional embedding Token Embeddings Embedding matrix E has shape [|V | × d ].  • One row for each of the |V | tokens in the vocabulary.  • Each word is a row vector of d dimensions Given: string \"Thanks for all the\" 1. Tokenize with BPE and convert into vocab indices w = [5,4000,10532,2224]  2. Select the corresponding rows from E, each row an embedding • (row 5, row 4000, row 10532, row 2224). Position Embeddings There are many methods, but we\\'ll just describe the simplest: absolute  position. Goal: learn a position embedding matrix Start with randomly initialized embeddings • one for each integer up to some maximum length.  • i.e., just as we have an embedding for token fish, we’ll have an  embedding for position 3 and position 17. • As with word embeddings, these position embeddings are learned along  with other parameters during training. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,   # Limit length of generated text\n",
    "    do_sample=True,   # Enable sampling\n",
    "    top_k=50,         # Consider top 50 tokens at each step\n",
    "    top_p=0.95,       # Enable nucleus sampling (tokens with cumulative probability >= 0.95)\n",
    "    temperature=0.7   # Adjust randomness; lower is less random\n",
    ")\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "question_answer = output_text.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "question, answer = question_answer.split(tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the matrix X of shape of?\n",
      "answer:  N\n"
     ]
    }
   ],
   "source": [
    "print(\"question:\", question)\n",
    "print(\"answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: <pad> How many embeddings are needed for each input?<sep> two</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> What's the goal of position embeddings?<sep> learn a position embedding matrix</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> What is the matrix of shape of?<sep> N</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> How are the embeddings created?<sep> by adding two distinct embedding for each input</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> What is the goal of learning a position embedding matrix?<sep> learn a position embedding matrix</s>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    print(\"Generated QA Pair:\", output_text)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f786c5af8a7f4e3fa2196760d4a7b8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub\\models--potsawee--t5-large-generation-race-QuestionAnswer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fccd99677274763a24a0567cf6db95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090fd033874e456998e25e6386e89ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df296e875f19410da76a11ba30e7970b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de82f9c01ed44758e9daca4e93b13c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe52fa192bc4ca0a87ec76fa4dd1ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4117a068543a4b6fb467738289b7744c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0319fe52b2b45038e7881db6631e582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-race-QuestionAnswer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-race-QuestionAnswer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,   # Limit length of generated text\n",
    "    do_sample=True,   # Enable sampling\n",
    "    top_k=50,         # Consider top 50 tokens at each step\n",
    "    top_p=0.95,       # Enable nucleus sampling (tokens with cumulative probability >= 0.95)\n",
    "    temperature=0.7   # Adjust randomness; lower is less random\n",
    ")\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "question_answer = output_text.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "question, answer = question_answer.split(tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  According to the passage, how do you learn an embedding?\n",
      "answer:  By working out the embeddings.\n"
     ]
    }
   ],
   "source": [
    "print(\"question:\", question)\n",
    "print(\"answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: <pad> What is the purpose of the passage?<sep> To tell you how to create embeddings.</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> What can we infer from the passage?<sep> The word \"Thanks\" has two embeddings in the embedding matrix E.</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> The word \"thanks\" is _ .<sep> a token</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> Token Embeddings and positional embeddings are similar in the way that they _ .<sep> are created by adding two distinct embeddings</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> The first step to create an embedding is to _ .<sep> create a matrix X</s>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    print(\"Generated QA Pair:\", output_text)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: {'bleu': 0.0, 'precisions': [0.5, 0.14285714285714285, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.6, 'translation_length': 8, 'reference_length': 5}\n",
      "ROUGE score: {'rouge1': 0.5454545454545454, 'rouge2': 0.2222222222222222, 'rougeL': 0.5454545454545454, 'rougeLsum': 0.5454545454545454}\n",
      "BERTScore: {'precision': [0.7796052694320679], 'recall': [0.8943084478378296], 'f1': [0.8330268859863281], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Example predictions and references\n",
    "references = [[\"What is position embedding?\"]]\n",
    "predictions = [\"What is the goal of position embeddings?\"]\n",
    "\n",
    "# BLEU score\n",
    "bleu_result = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU score:\", bleu_result)\n",
    "\n",
    "# ROUGE score\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "print(\"ROUGE score:\", rouge_result)\n",
    "\n",
    "# BERTScore\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, model_type=\"bert-base-uncased\")\n",
    "print(\"BERTScore:\", bertscore_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdfplumber\n",
    "import pdfplumber\n",
    "import re\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    laparams = LAParams(line_margin=0.1)  # Adjust line margin to help with word separation\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text(x_tolerance=2, y_tolerance=3, laparams=laparams)  # Fine-tune tolerances\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'Page \\d+|Header text|Footer text', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "pdf_path = \"D:/dell data/rutgers/data viz/assignment5/9.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "cleaned_text = clean_text(pdf_text)\n",
    "\n",
    "# print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All\\nrights reserved. Draft of August 20, 2024.\\nCHAPTER\\n9 The Transformer\\n“The true art of memory is the art of attention ”\\nSamuel Johnson, Idler #74, September 1759\\nIn this chapter we introduce the transformer, the standard architecture for build-\\ning large language models. Transformer-based large language models have com-\\npletely changed the field of speech and language processing. Indeed, every subse-\\nquent chapter in this textbook will make use of them. We’ll focus for now on left-\\nto-right (sometimes called causal or autoregressive) language modeling, in which\\nwe are given a sequence of input tokens and predict output tokens one by one by\\nconditioning on the prior context.\\nThe transformer is a neural network with a specific structure that includes a\\nmechanismcalledself-attentionormulti-headattention.1 Attentioncanbethought\\nof as a way to build contextual representations of a token’s meaning by attending to\\nand integrating information from surrounding tokens, helping the model learn how\\ntokens relate to each other over large spans.\\nNext token long and thanks for all\\nLanguage …\\nModeling logits logits logits logits logits\\nU U U U U\\nHead\\n… … … … …\\nStacked\\nTransformer …\\nBlocks\\n…\\nx1 x2 x3 x4 x5\\n+ 1 + 2 + 3 + 4 + 5\\nInput …\\nEncoding E E E E E\\nInput tokens So long and thanks for\\nFigure 9.1 The architecture of a (left-to-right) transformer, showing how each input token\\nget encoded, passed through a set of stacked transformer blocks, and then a language model\\nhead that predicts the next token.\\nFig. 9.1 sketches the transformer architecture. A transformer has three major\\ncomponents. At the center are columns of transformer blocks. Each block is a\\nmultilayer network (a multi-head attention layer, feedforward networks and layer\\nnormalization steps) that maps an input vector x in column i (corresponding to input\\ni\\n1 Althoughmulti-headattentiondevelopedhistoricallyfromtheRNNattentionmechanism(Chapter8),\\nwe’lldefineattentionfromscratchhereforreaderswhohaven’tyetreadChapter8.2 CHAPTER 9 • THE TRANSFORMER\\ntoken i) to an output vector h . The set of n blocks maps an entire context window\\ni\\nof input vectors (x ,...,x ) to a window of output vectors (h ,...,h ) of the same\\n1 n 1 n\\nlength. A column might contain from 12 to 96 or more stacked blocks.\\nThe column of blocks is preceded by the input encoding component, which pro-\\ncesses an input token (like the word thanks) into a contextual vector representation,\\nusing an embedding matrix E and a mechanism for encoding token position. Each\\ncolumn is followed by a language modeling head, which takes the embedding out-\\nput by the final transformer block, passes it through an unembedding matrix U and\\na softmax over the vocabulary to generate a single token for that column.\\nTransformer-based language models are complex, and so the details will unfold\\nover the next 5 chapters. In the next sections we’ll introduce multi-head attention,\\nthe rest of the transformer block, and the input encoding and language modeling\\nhead components. Chapter 10 discusses how language models are pretrained, and\\nhow tokens are generated via sampling. Chapter 11 introduces masked language\\nmodeling and the BERT family of bidirectional transformer encoder models. Chap-\\nter 12 shows how to prompt LLMs to perform NLP tasks by giving instructions and\\ndemonstrations, and how to align the model with human preferences. Chapter 13\\nwill introduce machine translation with the encoder-decoder architecture.\\n9.1 Attention\\nRecall from Chapter 6 that for word2vec and other static embeddings, the repre-\\nsentation of a word’s meaning is always the same vector irrespective of the context:\\nthe word chicken, for example, is always represented by the same fixed vector. So\\na static vector for the word it might somehow encode that this is a pronoun used\\nfor animals and inanimate entities. But in context it has a much richer meaning.\\nConsider it in one of these two sentences:\\n(9.1) The chicken didn’t cross the road because it was too tired.\\n(9.2) The chicken didn’t cross the road because it was too wide.\\nIn (9.1) it is the chicken (i.e., the reader knows that the chicken was tired), while\\nin (9.2) it is the road (and the reader knows that the road was wide).2 That is, if\\nwe are to compute the meaning of this sentence, we’ll need the meaning of it to be\\nassociated with the chicken in the first sentence and associated with the road in\\nthe second one, sensitive to the context.\\nFurthermore, consider reading left to right like a causal language model, pro-\\ncessing the sentence up to the word it:\\n(9.3) The chicken didn’t cross the road because it\\nAt this point we don’t yet know which thing it is going to end up referring to! So a\\nrepresentation of it at this point might have aspects of both chicken and road as\\nthe reader is trying to guess what happens next.\\nThis fact that words have rich linguistic relationships with other words that may\\nbe far away pervades language. Consider two more examples:\\n(9.4) The keys to the cabinet are on the table.\\n(9.5) I walked along the pond, and noticed one of the trees along the bank.\\n2 We say that in the first example it corefers with the chicken, and in the second it corefers with the\\nroad;we’llreturntothisinChapter23.9.1 • ATTENTION 3\\nIn (9.4), the phrase The keys is the subject of the sentence, and in English and many\\nlanguages, must agree in grammatical number with the verb are; in this case both are\\nplural. In English we can’t use a singular verb like is with a plural subject like keys\\n(we’ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers\\nto the side of a pond or river and not a financial institution because of the context,\\nincluding words like pond. (We’ll discuss word senses more in Chapter 11.)\\nThe point of all these examples is that these contextual words that help us com-\\npute the meaning of words in context can be quite far away in the sentence or para-\\ngraph. Transformers can build contextual representations of word meaning, contex-\\ncontextual tual embeddings, by integrating the meaning of these helpful contextual words. In a\\nembeddings\\ntransformer, layer by layer, we build up richer and richer contextualized representa-\\ntions of the meanings of input tokens. At each layer, we compute the representation\\nof a token i by combining information about i from the previous layer with infor-\\nmation about the neighboring tokens to produce a contextualized representation for\\neach word at each position.\\nAttention is the mechanism in the transformer that weighs and combines the\\nrepresentations from appropriate other tokens in the context from layer k−1 to build\\nthe representation for tokens in layer k.\\nehT\\nehT\\nnekcihc\\nnekcihc\\nt’ndid\\nt’ndid\\nssorc\\nssorc\\neht\\neht\\ndaor\\ndaor\\nesuaceb\\nesuaceb\\nti\\nti\\nsaw\\nsaw\\noot\\noot\\nderit\\nderit\\ncolumns corresponding to input tokens\\nLayer k+1\\nself-attention distribution\\nLayer k\\nFigure 9.2 The self-attention weight distribution α that is part of the computation of the\\nrepresentation for the word it at layer k+1. In computing the representation for it, we attend\\ndifferently to the various words at layer l, with darker shades indicating higher self-attention\\nvalues. Note that the transformer is attending highly to the columns corresponding to the\\ntokenschickenandroad,asensibleresult,sinceatthepointwhereitoccurs,itcouldplausibly\\ncorefers with the chicken or the road, and hence we’d like the representation for it to draw on\\nthe representation for these earlier words. Figure adapted from Uszkoreit (2017).\\nFig. 9.2 shows a schematic example simplified from a transformer (Uszkoreit,\\n2017). The figure describes the situation when the current token is it and we need\\ntocomputeacontextualrepresentationforthistokenatlayerk+1ofthetransformer,\\ndrawing on the representations (from layer k) of every prior token. The figure uses\\ncolor to represent the attention distribution over the contextual words: the tokens\\nchicken and road both have a high attention weight, meaning that as we are com-\\nputing the representation for it, we will draw most heavily on the representation for\\nchicken and road. This will be useful in building the final representation for it,\\nsince it will end up coreferring with either chicken or road.\\nLet’s now turn to how this attention distribution is represented and computed.4 CHAPTER 9 • THE TRANSFORMER\\n9.1.1 Attention more formally\\nAs we’ve said, the attention computation is a way to compute a vector representation\\nfor a token at a particular layer of a transformer, by selectively attending to and\\nintegrating information from prior tokens at the previous layer. Attention takes an\\ninput representation x corresponding to the input token at position i, and a context\\ni\\nwindow of prior inputs x ..x , and produces an output a .\\n1 i−1 i\\nIn causal, left-to-right language models, the context is any of the prior words.\\nThat is, when processing x , the model has access to x as well as the representations\\ni i\\nof all the prior tokens in the context window (context windows consist of thousands\\nof tokens) but no tokens after i. (By contrast, in Chapter 11 we’ll generalize attention\\nso it can also look ahead to future words.)\\nFig. 9.3 illustrates this flow of information in an entire causal self-attention layer,\\nin which this same attention computation happens in parallel at each token position\\ni. Thus a self-attention layer maps input sequences (x ,...,x ) to output sequences\\n1 n\\nof the same length (a ,...,a ).\\n1 n\\na1 a2 a3 a4 a5\\nSelf-Attention\\nattention attention attention attention attention\\nLayer\\nx1 x2 x3 x4 x5\\nFigure 9.3 Information flow in causal self-attention. When processing each input x i, the\\nmodel attends to all the inputs up to, and including x i.\\nSimplified version of attention At its heart, attention is really just a weighted\\nsum of context vectors, with a lot of complications added to how the weights are\\ncomputed and what gets summed. For pedagogical purposes let’s first describe a\\nsimplified intuition of attention, in which the attention output a at token position i\\ni\\nis simply the weighted sum of all the representations x , for all j ≤ i; we’ll use α\\nj ij\\nto mean how much x should contribute to a :\\ni j\\n(cid:88)\\nSimplified version: a i = α ijx j (9.6)\\nj≤i\\nEach α is a scalar used for weighing the value of input x when summing up\\nij j\\nthe inputs to compute a . How shall we compute this α weighting? In attention we\\ni\\nweight each prior embedding proportionally to how similar it is to the current token\\ni. So the output of attention is a sum of the embeddings of prior tokens weighted\\nby their similarity with the current token embedding. We compute similarity scores\\nvia dot product, which maps two vectors into a scalar value ranging from −∞ to\\n∞. The larger the score, the more similar the vectors that are being compared. We’ll\\nnormalize these scores with a softmax to create the vector of weights α , j ≤ i.\\nij\\nSimplified Version: score(x i,x j) = x i·x j (9.7)\\nα ij = softmax(score(x i,x j)) ∀j ≤ i (9.8)\\nThus in Fig. 9.3 we compute a by computing three scores: x ·x , x ·x and x ·x ,\\n3 3 1 3 2 3 3\\nnormalizing them by a softmax, and using the resulting probabilities as weights\\nindicating each of their proportional relevance to the current position i. Of course,9.1 • ATTENTION 5\\nthe softmax weight will likely be highest for x , since x is very similar to itself,\\ni i\\nresulting in a high dot product. But other context words may also be similar to i, and\\nthe softmax will also assign some weight to those words. Then we use these weights\\nas the α values in Eq. 9.6 to compute the weighted sum that is our a .\\n3\\nThe simplified attention in equations 9.6 – 9.8 demonstrates the attention-based\\napproach to computing a : compare the x to prior vectors, normalize those scores\\ni i\\ninto a probability distribution used to weight the sum of the prior vector. But now\\nwe’re ready to remove the simplifications.\\nA single attention head using query, key, and value matrices Now that we’ve\\nattentionhead seen a simple intuition of attention, let’s introduce the actual attention head, the\\nhead version of attention that’s used in transformers. (The word head is often used in\\ntransformers to refer to specific structured layers). The attention head allows us to\\ndistinctly represent three different roles that each input embedding plays during the\\ncourse of the attention process:\\n• As the current element being compared to the preceding inputs. We’ll refer to\\nquery this role as a query.\\n• In its role as a preceding input that is being compared to the current element\\nkey to determine a similarity weight. We’ll refer to this role as a key.\\nvalue • And finally, as a value of a preceding element that gets weighted and summed\\nup to compute the output for the current element.\\nTo capture these three different roles, transformers introduce weight matrices\\nWQ, WK, and WV. These weights will project each input vector x into a represen-\\ni\\ntation of its role as a key, query, or value:\\nq i = x iWQ; k i = x iWK; v i = x iWV (9.9)\\nGiven these projections, when we are computing the similarity of the current ele-\\nment x with some prior element x , we’ll use the dot product between the current\\ni j\\nelement’s query vector q and the preceding element’s key vector k . Furthermore,\\ni j\\nthe result of a dot product can be an arbitrarily large (positive or negative) value, and\\nexponentiating large values can lead to numerical issues and loss of gradients during\\ntraining. To avoid this, we scale the dot product by a factor related to the size of the\\nembeddings, via diving by the square root of the dimensionality of the query and\\nkey vectors (d ). We thus replace the simplified Eq. 9.7 with Eq. 9.11. The ensuing\\nk\\nsoftmax calculation resulting in α remains the same, but the output calculation for\\nij\\na is now based on a weighted sum over the value vectors v (Eq. 9.13).\\ni\\nHere’s a final set of equations for computing self-attention for a single self-\\nattention output vector a from a single input vector x . This version of attention\\ni i\\ncomputes a by summing the values of the prior elements, each weighted by the\\ni\\nsimilarity of its key to the query from the current element:\\nq i = x iWQ; k j = x jWK; v j = x jWV (9.10)\\nq ·k\\ni j\\nscore(x i,x j) = √ (9.11)\\nd\\nk\\nα ij = softmax(score(x i,x j)) ∀j ≤ i (9.12)\\n(cid:88)\\na i = α ijv j (9.13)\\nj≤i\\nWe illustrate this in Fig. 9.4 for the case of calculating the value of the third output\\na in a sequence.\\n36 CHAPTER 9 • THE TRANSFORMER\\na\\nOutput of self-attention 3\\n6. Sum the weighted\\nvalue vectors\\n4. Turn into weights via softmax\\ni,j\\n𝛼\\n2. Compare x3’s query with\\nthe keys for x1, x2, and x3\\nWk k k k\\n1. Generate\\nWq q q q\\nkey, query, value\\nvectors x v x v x Wv v\\n1 2 3\\n× × 5. Weigh each value vector\\n𝛼3,1 𝛼3,2 𝛼3,3\\n÷ ÷ ÷\\n3. Divide score by √d\\nk √dk √dk √dk\\nWk Wk\\nWq Wq\\nWv Wv\\nFigure 9.4 Calculating the value of a , the third element of a sequence using causal (left-\\n3\\nto-right) self-attention.\\nLet’s talk shapes. The input to attention x and the output from attention a both\\ni i\\nhave the same dimensionality 1×d (We often call d the model dimensionality,\\nand indeed as we’ll discuss in Section 9.2 the output h of each transformer block,\\ni\\nas well as the intermediate vectors inside the transformer block also have the same\\ndimensionality 1×d.).\\nWe’ll have a dimension d for the key and query vectors. The query vector and\\nk\\nthe key vector are both dimensionality 1×d , so we can take their dot product q ·k .\\nk i j\\nWe’ll have a separate dimension d for the value vectors. The transform matrix WQ\\nv\\nhas shape [d ×d ], WK is [d ×d ], and WV is [d ×d ]. In the original transformer\\nk k v\\nwork (Vaswani et al., 2017), d was 512, d and d were both 64.\\nk v\\nMulti-head Attention Equations 9.11-9.13 describe a single attention head. But\\nactually, transformers use multiple attention heads. The intuition is that each head\\nmight be attending to the context for different purposes: heads might be special-\\nized to represent different linguistic relationships between context elements and the\\ncurrent token, or to look for particular kinds of patterns in the context.\\nmulti-head So in multi-head attention we have h separate attention heads that reside in\\nattention\\nparallel layers at the same depth in a model, each with its own set of parameters that\\nallows the head to model different aspects of the relationships among inputs. Thus\\neach head i in a self-attention layer has its own set of key, query and value matrices:\\nWKi, WQi and WVi. These are used to project the inputs into separate key, value,\\nand query embeddings for each head.\\nWhen using multiple heads the model dimension d is still used for the input\\nand output, the key and query embeddings have dimensionality d , and the value\\nk\\nembeddings are of dimensionality d (again, in the original transformer paper d =\\nv k\\nd = 64, h = 8, and d = 512). Thus for each head i, we have weight layers WQi of\\nv\\nshape [d×d ], WKi of shape [d×d ], and WVi of shape [d×d ].\\nk k v\\nBelow are the equations for attention augmented with multiple heads; Fig. 9.59.2 • TRANSFORMER BLOCKS 7\\nshows an intuition.\\nqc\\ni\\n= x iWQc; kc\\nj\\n= x jWKc; vc\\nj\\n= x jWVc; ∀ c 1 ≤ c ≤ h (9.14)\\nqc·kc\\nscorec(x i,x j) = √i j (9.15)\\nd\\nk\\nα ic\\nj\\n= softmax(scorec(x i,x j)) ∀j ≤ i (9.16)\\n(cid:88)\\nheadc = αcvc (9.17)\\ni ij j\\nj≤i\\na i = (head1⊕head2...⊕headh)WO (9.18)\\nMultiHeadAttention(x i,[x 1,··· ,x N]) = a i (9.19)\\nThe output of each of the h heads is of shape 1×d , and so the output of the\\nv\\nmulti-head layer with h heads consists of h vectors of shape 1×d . These are con-\\nv\\ncatenated to produce a single output with dimensionality 1×hd . Then we use yet\\nv\\nanother linear projection WO ∈ Rhdv×d to reshape it, resulting in the multi-head\\nattention vector a with the correct output shape [1xd] at each input i.\\ni\\na\\ni\\n[1 x d]\\nProject down to d WO [hd x d]\\nv\\n…\\n[1 x hd ]\\nv\\nConcatenate Outputs\\n[1 x d v ] [1 x d v ]\\nEach head\\nHead 1 Head 2 … Head 8\\nattends differently\\nWK WV WQ WK WV WQ WK WV WQ\\nto context 1 1 1 2 2 2 8 8 8\\n… x x x x\\ni-3 i-2 i-1 i [1 ax d]\\ni\\nFigure 9.5 The multi-head attention computation for input x i, producing output a i. A multi-head attention\\nlayer has h heads, each with its own key, query and value weight matrices. The outputs from each of the heads\\nare concatenated and then projected down to d, thus producing an output of the same size as the input.\\n9.2 Transformer Blocks\\nThe self-attention calculation lies at the core of what’s called a transformer block,\\nwhich, in addition to the self-attention layer, includes three other kinds of layers: (1)\\na feedforward layer, (2) residual connections, and (3) normalizing layers (colloqui-\\nally called “layer norm”).\\nFig. 9.6 illustrates a transformer block, sketching a common way of thinking\\nresidualstream about the block that is called the residual stream (Elhage et al., 2021). In the resid-\\nual stream viewpoint, we consider the processing of an individual token i through\\nthe transformer block as a single stream of d-dimensional representations for token\\nposition i. This residual stream starts with the original input vector, and the various8 CHAPTER 9 • THE TRANSFORMER\\nh h h\\ni-1 i Residual i+1\\nStream\\n+\\nFeedforward\\nLayer Norm\\n… …\\n+\\nMultiHead\\nAttention\\nLayer Norm\\nx x x\\ni-1 i i+1\\nFigure 9.6 The architecture of a transformer block showing the residual stream. This\\nfigureshowstheprenormversionofthearchitecture,inwhichthelayernormshappenbefore\\nthe attention and feedforward layers rather than after.\\ncomponents read their input from the residual stream and add their output back into\\nthe stream.\\nThe input at the bottom of the stream is an embedding for a token, which has\\ndimensionality d. This initial embedding gets passed up (by residual connections),\\nand is progressively added to by the other components of the transformer: the at-\\ntention layer that we have seen, and the feedforward layer that we will introduce.\\nBefore the attention and feedforward layer is a computation called the layer norm.\\nThus the initial vector is passed through a layer norm and attention layer, and\\nthe result is added back into the stream, in this case to the original input vector\\nx . And then this summed vector is again passed through another layer norm and a\\ni\\nfeedforward layer, and the output of those is added back into the residual, and we’ll\\nuse h to refer to the resulting output of the transformer block for token i. (In earlier\\ni\\ndescriptions the residual stream was often described using a different metaphor as\\nresidual connections that add the input of a component to its output, but the residual\\nstream is a more perspicuous way of visualizing the transformer.)\\nWe’ve already seen the attention layer, so let’s now introduce the feedforward\\nand layer norm computations in the context of processing a single input x at token\\ni\\nposition i.\\nFeedforward layer The feedforward layer is a fully-connected 2-layer network,\\ni.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights\\nare the same for each token position i , but are different from layer to layer. It\\nis common to make the dimensionality d of the hidden layer of the feedforward\\nff\\nnetwork be larger than the model dimensionality d. (For example in the original\\ntransformer model, d = 512 and d = 2048.)\\nff\\nFFN(x i) = ReLU(x iW 1+b 1)W 2+b 2 (9.20)\\nLayer Norm At two stages in the transformer block we normalize the vector (Ba\\nlayernorm et al., 2016). This process, called layer norm (short for layer normalization), is one9.2 • TRANSFORMER BLOCKS 9\\nof many forms of normalization that can be used to improve training performance\\nin deep neural networks by keeping the values of a hidden layer in a range that\\nfacilitates gradient-based training.\\nLayer norm is a variation of the z-score from statistics, applied to a single vec-\\ntor in a hidden layer. That is, the term layer norm is a bit confusing; layer norm\\nis not applied to an entire transformer layer, but just to the embedding vector of a\\nsingle token. Thus the input to layer norm is a single vector of dimensionality d\\nand the output is that vector normalized, again of dimensionality d. The first step in\\nlayer normalization is to calculate the mean, µ, and standard deviation, σ, over the\\nelements of the vector to be normalized. Given an embedding vector x of dimen-\\nsionality d, these values are calculated as follows.\\nd\\n1 (cid:88)\\nµ = x i (9.21)\\nd\\ni=1\\n(cid:118)\\n(cid:117) d\\n(cid:117)1 (cid:88)\\nσ = (cid:116) (x i−µ)2 (9.22)\\nd\\ni=1\\nGiven these values, the vector components are normalized by subtracting the mean\\nfrom each and dividing by the standard deviation. The result of this computation is\\na new vector with zero mean and a standard deviation of one.\\n(x−µ)\\nˆx = (9.23)\\nσ\\nFinally, in the standard implementation of layer normalization, two learnable param-\\neters, γ and β, representing gain and offset values, are introduced.\\n(x−µ)\\nLayerNorm(x) = γ +β (9.24)\\nσ\\nPutting it all together The function computed by a transformer block can be ex-\\npressed by breaking it down with one equation for each component computation,\\nusing t (of shape [1×d]) to stand for transformer and superscripts to demarcate\\neach computation inside the block:\\nt1\\ni\\n= LayerNorm(x i) (9.25)\\nt2 = MultiHeadAttention(t1,(cid:2) x1,··· ,x1 (cid:3) ) (9.26)\\ni i 1 N\\nt3 i = t2 i +x i (9.27)\\nt4 = LayerNorm(t3) (9.28)\\ni i\\nt5 = FFN(t4) (9.29)\\ni i\\nh i = t5 i +t3 i (9.30)\\nNotice that the only component that takes as input information from other tokens\\n(other residual streams) is multi-head attention, which (as we see from (9.27)) looks\\nat all the neighboring tokens in the context. The output from attention, however, is\\nthenaddedintothistoken’sembeddingstream. Infact, Elhageetal.(2021)showthat\\nwe can view attention heads as literally moving information from the residual stream\\nof a neighboring token into the current stream. The high-dimensional embedding\\nspace at each position thus contains information about the current token and about\\nneighboring tokens, albeit in different subspaces of the vector space. Fig. 9.7 shows\\na visualization of this movement.10 CHAPTER 9 • THE TRANSFORMER\\nToken A Token B\\nresidual residual\\nstream stream\\nFigure 9.7 An attention head can move information from token A’s residual stream into\\ntoken B’s residual stream.\\nCrucially, the input and output dimensions of transformer blocks are matched so\\nthey can be stacked. Each token vector x at the input to the block has dimensionality\\ni\\nd, and the output h also has dimensionality d. Transformers for large language\\ni\\nmodels stack many of these blocks, from 12 layers (used for the T5 or GPT-3-small\\nlanguage models) to 96 layers (used for GPT-3 large), to even more for more recent\\nmodels. We’ll come back to this issue of stacking in a bit.\\nEquation (9.27) and following are just the equation for a single transformer\\nblock, but the residual stream metaphor goes through all the transformer layers,\\nfrom the first transformer blocks to the 12th, in a 12-layer transformer. At the ear-\\nlier transformer blocks, the residual stream is representing the current token. At the\\nhighest transformer blocks, the residual stream is usually representing the following\\ntoken, since at the very end it’s being trained to predict the next token.\\nOnce we stack many blocks, there is one more requirement: at the very end of\\nthe last (highest) transformer block, there is a single extra layer norm that is run on\\nthe last h of each token stream (just below the language model head layer that we\\ni\\nwill define soon). 3\\n9.3 Parallelizing computation using a single matrix X\\nThis description of multi-head attention and the rest of the transformer block has\\nbeen from the perspective of computing a single output at a single time step i in\\na single residual stream. But as we pointed out earlier, the attention computation\\nperformed for each token to compute a is independent of the computation for each\\ni\\nother token, and that’s also true for all the computation in the transformer block\\ncomputing h from the input x . That means we can easily parallelize the entire\\ni i\\ncomputation, taking advantage of efficient matrix multiplication routines.\\nWe do this by packing the input embeddings for the N tokens of the input se-\\nquence into a single matrix X of size [N ×d]. Each row of X is the embedding of\\none token of the input. Transformers for large language models commonly have an\\ninput length N = 1K, 2K, or as many as 32K tokens (or more), so X typically has be-\\ntween 1K and 32K rows, each of the dimensionality of the embedding d (the model\\n3 Notethatweareusingthemostcommoncurrenttransformerarchitecture,whichiscalledtheprenorm\\narchitecture. TheoriginaldefinitionofthetransformerinVaswanietal.(2017)usedanalternativearchi-\\ntecture called the postnorm transformer in which the layer norm happens after the attention and FFN\\nlayers; it turns out moving the layer norm beforehand works better, but does require this one extra layer\\nattheend.9.3 • PARALLELIZING COMPUTATION USING A SINGLE MATRIX X 11\\ndimension).\\nParallelizing attention Let’s first see this for a single attention head and then turn\\nto multiple heads, and then add in the rest of the components in the transformer\\nblock. For one head we multiply X by the key, query, and value matrices WQ of\\nshape [d×d ], WK of shape [d×d ], and WV of shape [d×d ], to produce matrices\\nk k v\\nQ of shape [N ×d k], K ∈ RN×dk, and V ∈ RN×dv, containing all the key, query, and\\nvalue vectors:\\nQ = XWQ; K = XWK; V = XWV (9.31)\\nGiven these matrices we can compute all the requisite query-key comparisons simul-\\ntaneously by multiplying Q and K(cid:124) in a single matrix multiplication. The product is\\nof shape N×N, visualized in Fig. 9.8.\\nq1•k1 q1•k2 q1•k3 q1•k4\\nq2•k1 q2•k2 q2•k3 q2•k4\\nN\\nq3•k1 q3•k2 q3•k3 q3•k4\\nq4•k1 q4•k2 q4•k3 q4•k4\\nN\\nFigure 9.8 The N ×N QK(cid:124) matrix showing how it computes all q i·k j comparisons in a\\nsingle matrix multiple.\\nOnce we have this\\nQK(cid:124)\\nmatrix, we can very efficiently scale these scores, take\\nthe softmax, and then multiply the result by V resulting in a matrix of shape N ×d:\\na vector embedding representation for each token in the input. We’ve reduced the\\nentire self-attention step for an entire sequence of N tokens for one head to the\\nfollowing computation:\\n(cid:18) (cid:18) (cid:124)(cid:19)(cid:19)\\nQK\\nA = softmax mask √ V (9.32)\\nd\\nk\\nMaskingoutthefuture Youmayhavenoticedthatweintroducedamaskfunction\\nin Eq. 9.32 above. This is because the self-attention computation as we’ve described\\n(cid:124)\\nit has a problem: the calculation in QK results in a score for each query value\\nto every key value, including those that follow the query. This is inappropriate in\\nthe setting of language modeling: guessing the next word is pretty simple if you\\nalready know it! To fix this, the elements in the upper-triangular portion of the\\nmatrix are zeroed out (set to −∞), thus eliminating any knowledge of words that\\nfollow in the sequence. This is done in practice by adding a mask matrix M in\\nwhich M =−∞∀j >i (i.e. for the upper-triangular portion) and M =0 otherwise.\\nij ij\\n(cid:124)\\nFig. 9.9 shows the resulting masked QK matrix. (we’ll see in Chapter 11 how to\\nmake use of words in the future for tasks that need it).\\nFig. 9.10 shows a schematic of all the computations for a single attention head\\nparallelized in matrix form.\\nFig. 9.8 and Fig. 9.9 also make it clear that attention is quadratic in the length\\nof the input, since at each layer we need to compute dot products between each pair\\nof tokens in the input. This makes it expensive to compute attention over very long\\ndocuments (like entire novels). Nonetheless modern large language models manage\\nto use quite long contexts of thousands or tens of thousands of tokens.12 CHAPTER 9 • THE TRANSFORMER\\nq1•k1 −∞ −∞ −∞\\nq2•k1 q2•k2 −∞ −∞\\nN\\nq3•k1 q3•k2 q3•k3 −∞\\nq4•k1 q4•k2 q4•k3 q4•k4\\nN\\nFigure 9.9 The N×N QK(cid:124) matrix showing the q i·k j values, with the upper-triangle por-\\ntion of the comparisons matrix zeroed out (set to −∞, which the softmax will turn to zero).\\nq1\\nq2\\nq3\\nq4\\nk1 k2 k3 k4\\nX Q X K X V\\nToIn kp eu nt\\n1\\nWQ TQ oku ee nry\\n1\\nToIn kp eu nt\\n1\\nWK ToK kee ny\\n1\\nToIn kp eu nt\\n1\\nWV ToV ka elu ne\\n1\\nInput Query Input Key Input Value\\nToken 2 x = Token 2 Token 2 x = Token 2 Token 2 x = Token 2\\nInput Query Input Key Input Value\\nToken 3 Token 3 Token 3 Token 3 Token 3 Token 3\\nInput Query Input Key Input Value\\nToken 4 d x d Token 4 Token 4 d x d k Token 4 Token 4 d x d v Token 4\\nk\\nN x d N x d k N x d N x d k N x d N x d v\\nT\\nQ K QKT QKT masked V A\\nx = q1•k1 q1•k2 q1•k3 q1•k4 qqq111•••kkk111 −∞ −∞ −∞ v1 a1\\nmask q2•k1 q2•k2 q2•k3 q2•k4 = q2•k1 q2•k2 −∞ −∞ x v2 = a2\\nq3•k1 q3•k2 q3•k3 q3•k4 q3•k1 q3•k2 q3•k3 −∞ v3 a3\\nd k x N q4•k1 q4•k2 q4•k3 q4•k4 q4•k1 q4•k2 q4•k3 q4•k4 v4 a4\\nN x d k N x N N x N N x d v N x d v\\nFigure 9.10 Schematicoftheattentioncomputationforasingleattentionheadinparallel. Thefirstrowshows\\nthe computation of the Q, K, and V matrices. The second row shows the computation of QKT, the masking\\n(the softmax computation and the normalizing by dimensionality are not shown) and then the weighted sum of\\nthe value vectors to get the final attention vectors.\\nParallelizing multi-head attention In multi-head attention, as with self-attention,\\nthe input and output have the model dimension d, the key and query embeddings\\nhave dimensionality d , and the value embeddings are of dimensionality d (again,\\nk v\\nin the original transformer paper d = d = 64, h = 8, and d = 512). Thus for each\\nk v\\nhead i, we have weight layers WQ\\ni\\n∈ Rd×dk, WK\\ni\\n∈ Rd×dk, and WV\\ni\\n∈ Rd×dv, and\\nthese get multiplied by the inputs packed into X to produce Q ∈ RN×dk, K ∈ RN×dk,\\nand V ∈ RN×dv. The output of each of the h heads is of shape N ×d v, and so the\\noutputofthemulti-headlayerwithhheadsconsistsofhmatricesofshapeN×d . To\\nv\\nmake use of these matrices in further processing, they are concatenated to produce\\na single output with dimensionality N ×hd . Finally, we use yet another linear\\nv\\nprojection WO ∈ Rhdv×d, that reshape it to the original output dimension for each\\ntoken. Multiplying the concatenated N ×hd\\nv\\nmatrix output by WO ∈ Rhdv×d yields9.4 • THE INPUT: EMBEDDINGS FOR TOKEN AND POSITION 13\\nthe self-attention output A of shape [N×d].\\nQi = XWQi ; Ki = XWKi ; Vi = XWVi (9.33)\\n(cid:18) QiKi(cid:124)(cid:19)\\nhead i = SelfAttention(Qi,Ki,Vi) = softmax √ Vi (9.34)\\nd\\nk\\nMultiHeadAttention(X) = (head 1⊕head 2...⊕head h)WO (9.35)\\nPutting it all together with the parallel input matrix X The function computed\\nin parallel by an entire layer of N transformer block over the entire N input tokens\\ncan be expressed as:\\nO = LayerNorm(X+MultiHeadAttention(X)) (9.36)\\nH = LayerNorm(O+FFN(O)) (9.37)\\nOr we can break it down with one equation for each component computation, using\\nT (of shape [N ×d]) to stand for transformer and superscripts to demarcate each\\ncomputation inside the block:\\nT1 = MultiHeadAttention(X) (9.38)\\nT2 = X+T1 (9.39)\\nT3 = LayerNorm(T2) (9.40)\\nT4 = FFN(T3) (9.41)\\nT5 = T4+T3 (9.42)\\nH = LayerNorm(T5) (9.43)\\nHere when we use a notation like FFN(T3) we mean that the same FFN is applied\\nin parallel to each of the N embedding vectors in the window. Similarly, each of the\\nN tokens is normed in parallel in the LayerNorm. Crucially, the input and output\\ndimensions of transformer blocks are matched so they can be stacked. Since each\\ntoken x at the input to the block has dimensionality d, that means the input X and\\ni\\noutput H are both of shape [N×d].\\n9.4 The input: embeddings for token and position\\nLet’s talk about where the input X comes from. Given a sequence of N tokens (N is\\nembedding the context length in tokens), the matrix X of shape [N ×d] has an embedding for\\neach word in the context. The transformer does this by separately computing two\\nembeddings: an input token embedding, and an input positional embedding.\\nA token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-\\nmension d that will be our initial representation for the input token. (As we pass\\nvectors up through the transformer layers in the residual stream, this embedding\\nrepresentation will change and grow, incorporating context and playing a different\\nrole depending on the kind of language model we are building.) The set of initial\\nembeddings are stored in the embedding matrix E, which has a row for each of the\\n|V| tokens in the vocabulary. Thus each word is a row vector of d dimensions, and\\nE has shape [|V|×d].\\nGiven an input token string like Thanks for all the we first convert the tokens\\ninto vocabulary indices (these were created when we first tokenized the input using14 CHAPTER 9 • THE TRANSFORMER\\nBPE or SentencePiece). So the representation of thanks for all the might be w =\\n[5,4000,10532,2224]. Next we use indexing to select the corresponding rows from\\nE, (row 5, row 4000, row 10532, row 2224).\\nAnother way to think about selecting token embeddings from the embedding\\nmatrix is to represent tokens as one-hot vectors of shape [1×|V|], i.e., with one\\none-hotvector dimension for each word in the vocabulary. Recall that in a one-hot vector all the\\nelements are 0 except one, the element whose dimension is the word’s index in the\\nvocabulary, which hasvalue1. Soifthe word“thanks”has index5in thevocabulary,\\nx = 1, and x = 0 ∀i (cid:54)= 5, as shown here:\\n5 i\\n[0 0 0 0 1 0 0 ... 0 0 0 0]\\n1 2 3 4 5 6 7 ... ... |V|\\nMultiplyingbyaone-hotvectorthathasonlyonenon-zeroelementx =1simply\\ni\\nselects out the relevant row vector for word i, resulting in the embedding for word i,\\nas depicted in Fig. 9.11.\\nd\\n5\\n5 |V| d\\n1 0 0 0 0 1 0 0 … 0 0 0 0 ✕ E = 1\\n|V|\\nFigure 9.11 Selecting the embedding vector for word V by multiplying the embedding\\n5\\nmatrix E with a one-hot vector with a 1 in index 5.\\nWe can extend this idea to represent the entire token sequence as a matrix of one-\\nhot vectors, one for each of the N positions in the transformer’s context window, as\\nshown in Fig. 9.12.\\nd\\n|V| d\\n0 0 0 0 1 0 0 … 0 0 0 0\\n0 0 0 0 0 0 0 … 0 0 1 0 ✕ =\\n1 0 0 0 0 0 0 … 0 0 0 0 E\\n…\\nN\\nN 0 0 0 0 1 0 0 … 0 0 0 0\\n|V|\\nFigure 9.12 Selecting the embedding matrix for the input sequence of token idsW by mul-\\ntiplying a one-hot matrix corresponding toW by the embedding matrix E.\\nThese token embeddings are not position-dependent. To represent the position\\nof each token in the sequence, we combine these token embeddings with positional\\npositional embeddings specific to each position in an input sequence.\\nembeddings\\nWhere do we get these positional embeddings? The simplest method, called\\nabsolute absolute position, is to start with randomly initialized embeddings corresponding\\nposition\\nto each possible input position up to some maximum length. For example, just as\\nwe have an embedding for the word fish, we’ll have an embedding for the position 3.\\nAs with word embeddings, these positional embeddings are learned along with other\\nparameters during training. We can store them in a matrix Epos of shape [1×N].\\nTo produce an input embedding that captures positional information, we just\\nadd the word embedding for each input to its corresponding positional embedding.\\nThe individual token and position embeddings are both of size [1×d], so their sum is9.5 • THE LANGUAGE MODELING HEAD 15\\nTransformer Block\\nX = Composite\\nEmbeddings\\n(word + position)\\nJanet\\n1\\nwill\\n2\\nback\\n3\\nJanet will back the bill\\n+ + +\\nthe\\n+\\n4\\nbill\\n+\\n5\\nWord\\nEmbeddings\\nPosition\\nEmbeddings\\nFigure 9.13 A simple way to model position: add an embedding of the absolute position to\\nthe token embedding to produce a new embedding of the same dimensionality.\\nalso [1×d], This new embedding serves as the input for further processing. Fig. 9.13\\nshows the idea.\\nThe final representation of the input, the matrix X, is an [N×d] matrix in which\\neach row i is the representation of the ith token in the input, computed by adding\\nE[id(i)]—the embedding of the id of the token that occurred at position i—, to P[i],\\nthe positional embedding of position i.\\nA potential problem with the simple absolute position embedding approach is\\nthat there will be plenty of training examples for the initial positions in our inputs\\nand correspondingly fewer at the outer length limits. These latter embeddings may\\nbe poorly trained and may not generalize well during testing. An alternative ap-\\nproach to absolute positional embeddings is to choose a static function that maps\\ninteger inputs to real-valued vectors in a way that captures the inherent relation-\\nships among the positions. That is, it captures the fact that position 4 in an input is\\nmore closely related to position 5 than it is to position 17. A combination of sine\\nand cosine functions with differing frequencies was used in the original transformer\\nwork. Even more complex positional embedding methods exist, such as ones that\\nrepresent relative position instead of absolute position, often implemented in the\\nattention mechanism at each layer rather than being added once at the initial input.\\n9.5 The Language Modeling Head\\nThe last component of the transformer we must introduce is the language modeling\\nlanguage head. Here we are using the word head to mean the additional neural circuitry we\\nmodelinghead\\nhead add on top of the basic transformer architecture when we apply pretrained trans-\\nformer models to various tasks. The language modeling head is the circuitry we\\nneed to do language modeling.\\nRecallthatlanguagemodels, fromthesimplen-grammodelsofChapter3through\\nthe feedforward and RNN language models of Chapter 7 and Chapter 8, are word\\npredictors. Given a context of words, they assign a probability to each possible next\\nword. For example, if the preceding context is “Thanks for all the” and we want to\\nknow how likely the next word is “fish” we would compute:\\nP(fish|Thanks for all the)\\nLanguage models give us the ability to assign such a conditional probability to every\\npossible next word, giving us a distribution over the entire vocabulary. The n-gram16 CHAPTER 9 • THE TRANSFORMER\\nlanguage models of Chapter 3 compute the probability of a word given counts of\\nits occurrence with the n−1 prior words. The context is thus of size n−1. For\\ntransformer language models, the context is the size of the transformer’s context\\nwindow, which can be quite large: 2K, 4K, even 32K tokens for very large models.\\nThe job of the language modeling head is to take the output of the final trans-\\nformer layer from the last token N and use it to predict the upcoming word at posi-\\ntion N+1. Fig. 9.14 shows how to accomplish this task, taking the output of the last\\ntoken at the last layer (the d-dimensional output embedding of shape [1×d]) and\\nproducing a probability distribution over words (from which we will choose one to\\ngenerate).\\ny1 y2 … y|V| Word probabilities 1 x |V|\\nLanguage Model Head Softmax over vocabulary V\\nL\\ntakes h N and outputs a u1 u2 … u|V| Logits 1 x |V|\\ndistribution over vocabulary V\\nUnembedding layer\\nUnembedding layer d x |V|\\nU = ET\\nhL 1 hL 2 hL N 1 x d\\nLayer L\\nTransformer\\nBlock\\n…\\nw1 w2 w\\nN\\nFigure 9.14 The language modeling head: the circuit at the top of a transformer that maps from the output\\nembedding for token N from the last transformer layer (hL) to a probability distribution over words in the\\nN\\nvocabularyV.\\nThe first module in Fig. 9.14 is a linear layer, whose job is to project from the\\noutput hL, which represents the output token embedding at position N from the final\\nN\\nlogit block L, (hence of shape [1×d]) to the logit vector, or score vector, that will have a\\nsingle score for each of the |V| possible words in the vocabularyV. The logit vector\\nu is thus of dimensionality 1×|V|.\\nThis linear layer can be learned, but more commonly we tie this matrix to (the\\nweighttying transpose of) the embedding matrix E. Recall that in weight tying, we use the\\nsame weights for two different matrices in the model. Thus at the input stage of the\\ntransformer the embedding matrix (of shape [|V|×d]) is used to map from a one-hot\\nvector over the vocabulary (of shape [1×|V|]) to an embedding (of shape [1×d]).\\nAnd then in the language model head, ET, the transpose of the embedding matrix (of\\nshape [d×|V|]) is used to map back from an embedding (shape [1×d]) to a vector\\nover the vocabulary (shape [1×|V|]). In the learning process, E will be optimized to\\nbe good at doing both of these mappings. We therefore sometimes call the transpose\\nunembedding ET the unembedding layer because it is performing this reverse mapping.\\nA softmax layer turns the logits u into the probabilities y over the vocabulary.\\nu = hL ET (9.44)\\nN\\ny = softmax(u) (9.45)\\nWe can use these probabilities to do things like help assign a probability to a\\ngiven text. But the most important usage to generate text, which we do by sampling9.5 • THE LANGUAGE MODELING HEAD 17\\na word from these probabilities y. We might sample the highest probability word\\n(‘greedy’ decoding), or use another of the sampling methods we’ll introduce in Sec-\\ntion ??. In either case, whatever entry y we choose from the probability vector y,\\nk\\nwe generate the word that has that index k.\\n…\\nToken probabilities y1 y2 y|V| w\\ni+1\\nSample token to\\nLanguage softmax generate at position i+1\\nModeling\\nHead logits u1 u2 … u|V|\\nU\\nhL\\ni\\nfeedforward\\nLayer L layer norm\\nattention\\nlayer norm\\nhL-1 = xL\\n… i i\\nh2 = x3\\ni i\\nfeedforward\\nlayer norm\\nLayer 2\\nattention\\nlayer norm\\nh1 = x2\\ni i\\nfeedforward\\nLayer 1 layer norm\\nattention\\nlayer norm\\nx1\\ni\\n+ i\\nInput\\nEncoding E\\nInput token w i\\nFigure 9.15 A transformer language model (decoder-only), stacking transformer blocks\\nand mapping from an input token w i to to a predicted next token w i+1.\\nFig. 9.15 shows the total stacked architecture for one token i. Note that the input\\nto each transformer layer x is the same as the output from the preceding layer h−.\\ni i\\nNow that we see all these transformer layers spread out on the page, we can point\\nout another useful feature of the unembedding layer: as a tool for interpretability of\\nlogitlens the internals of the transformer that we call the logit lens (Nostalgebraist, 2020).\\nWe can take a vector from any layer of the transformer and, pretending that it is\\nthe prefinal embedding, simply multiply it by the unembedding layer to get logits,\\nand compute a softmax to see the distribution over words that that vector might\\nbe representing. This can be a useful window into the internal representations of\\nthe model. Since the network wasn’t trained to make the internal representations\\nfunction in this way, the logit lens doesn’t always work perfectly, but this can still\\nbe a useful trick.\\nA terminological note before we conclude: You will sometimes see a trans-\\nformer used for this kind of unidirectional causal language model called a decoder-\\ndecoder-only only model. This is because this model constitutes roughly half of the encoder-\\nmodel18 CHAPTER 9 • THE TRANSFORMER\\ndecoder model for transformers that we’ll see how to apply to machine translation\\nin Chapter 13. (Confusingly, the original introduction of the transformer had an\\nencoder-decoder architecture, and it was only later that the standard paradigm for\\ncausal language model was defined by using only the decoder part of this original\\narchitecture).\\n9.6 Summary\\nThis chapter has introduced the transformer and its components for the task of lan-\\nguage modeling. We’ll continue the task of language modeling including issues like\\ntraining and sampling in the next chapter.\\nHere’s a summary of the main points that we covered:\\n• Transformers are non-recurrent networks based on multi-head attention, a\\nkind of self-attention. A multi-head attention computation takes an input\\nvector x and maps it to an output a by adding in vectors from prior tokens,\\ni i\\nweighted by how relevant they are for the processing of the current word.\\n• A transformer block consists of a residual stream in which the input from\\nthe prior layer is passed up to the next layer, with the output of different com-\\nponents added to it. These components include a multi-head attention layer\\nfollowed by a feedforward layer, each preceded by layer normalizations.\\nTransformer blocks are stacked to make deeper and more powerful networks.\\n• The input to a transformer is a computing by adding an embedding (computed\\nwith an embedding matrix) to a positional encoding that represents the se-\\nquential position of the token in the window.\\n• Language models can be built out of stacks of transformer blocks, with a\\nlanguage model head at the top, which applies an unembedding matrix to\\nthe output H of the top layer to generate the logits, which are then passed\\nthrough a softmax to generate word probabilities.\\n• Transformer-based language models have a wide context window (as wide\\nas 32768 tokens for very large models) allowing them to draw on enormous\\namounts of context to predict upcoming words.\\nBibliographical and Historical Notes\\nThe transformer (Vaswani et al., 2017) was developed drawing on two lines of prior\\nresearch: self-attention and memory networks.\\nEncoder-decoder attention, the idea of using a soft weighting over the encodings\\nof input words to inform a generative decoder (see Chapter 13) was developed by\\nGraves (2013) in the context of handwriting generation, and Bahdanau et al. (2015)\\nfor MT. This idea was extended to self-attention by dropping the need for separate\\nencoding and decoding sequences and instead seeing attention as a way of weighting\\nthe tokens in collecting information passed from lower layers to higher layers (Ling\\net al., 2015; Cheng et al., 2016; Liu et al., 2016).\\nOther aspects of the transformer, including the terminology of key, query, and\\nvalue, came from memory networks, a mechanism for adding an external read-BIBLIOGRAPHICAL AND HISTORICAL NOTES 19\\nwrite memory to networks, by using an embedding of a query to match keys rep-\\nresenting content in an associative memory (Sukhbaatar et al., 2015; Weston et al.,\\n2015; Graves et al., 2014).\\nMORE HISTORY TBD IN NEXT DRAFT.20 Chapter 9 • The Transformer\\nBa,J.L.,J.R.Kiros,andG.E.Hinton.2016. Layernormal-\\nization. NeurIPSworkshop.\\nBahdanau, D., K. H. Cho, and Y. Bengio. 2015. Neural ma-\\nchinetranslationbyjointlylearningtoalignandtranslate.\\nICLR2015.\\nCheng, J., L. Dong, and M. Lapata. 2016. Long short-term\\nmemory-networksformachinereading. EMNLP.\\nElhage, N., N. Nanda, C. Olsson, T. Henighan, N. Joseph,\\nB.Mann,A.Askell,Y.Bai,A.Chen,T.Conerly,N.Das-\\nSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Her-\\nnandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse,\\nD. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCan-\\ndlish, and C. Olah. 2021. A mathematical framework for\\ntransformercircuits. Whitepaper.\\nGraves,A.2013.Generatingsequenceswithrecurrentneural\\nnetworks. ArXiv.\\nGraves, A., G. Wayne, and I. Danihelka. 2014. Neural Tur-\\ningmachines. ArXiv.\\nLing, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez,\\nS. Amir, L. Marujo, and T. Lu´ıs. 2015. Finding function\\ninform: Compositionalcharactermodelsforopenvocab-\\nularywordrepresentation. EMNLP.\\nLiu,Y.,C.Sun,L.Lin,andX.Wang.2016. Learningnatural\\nlanguage inference using bidirectional LSTM model and\\ninner-attention. ArXiv.\\nNostalgebraist. 2020. Interpreting gpt: the logit lens. White\\npaper.\\nSukhbaatar, S., A. Szlam, J. Weston, and R. Fergus. 2015.\\nEnd-to-endmemorynetworks. NeurIPS.\\nUszkoreit,J.2017. Transformer: Anovelneuralnetworkar-\\nchitecture for language understanding. Google Research\\nblogpost,ThursdayAugust31,2017.\\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\\nA. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Atten-\\ntionisallyouneed. NeurIPS.\\nWeston, J., S. Chopra, and A. Bordes. 2015. Memory net-\\nworks. ICLR2015.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All rights reserved. Draft of August 20, 2024. CHAPTER 9 The Transformer “The true art of memory is the art of attention ” Samuel Johnson, Idler #74, September 1759 In this chapter we introduce the transformer, the standard architecture for build- ing large language models. Transformer-based large language models have com- pletely changed the field of speech and language processing. Indeed, every subse- quent chapter in this textbook will make use of them. We’ll focus for now on left- to-right (sometimes called causal or autoregressive) language modeling, in which we are given a sequence of input tokens and predict output tokens one by one by conditioning on the prior context. The transformer is a neural network with a specific structure that includes a mechanismcalledself-attentionormulti-headattention.1 Attentioncanbethought of as a way to build contextual representations of a token’s meaning by attending to and integrating information from surrounding tokens, helping the model learn how tokens relate to each other over large spans. Next token long and thanks for all Language … Modeling logits logits logits logits logits U U U U U Head … … … … … Stacked Transformer … Blocks … x1 x2 x3 x4 x5 + 1 + 2 + 3 + 4 + 5 Input … Encoding E E E E E Input tokens So long and thanks for Figure 9.1 The architecture of a (left-to-right) transformer, showing how each input token get encoded, passed through a set of stacked transformer blocks, and then a language model head that predicts the next token. Fig. 9.1 sketches the transformer architecture. A transformer has three major components. At the center are columns of transformer blocks. Each block is a multilayer network (a multi-head attention layer, feedforward networks and layer normalization steps) that maps an input vector x in column i (corresponding to input i 1 Althoughmulti-headattentiondevelopedhistoricallyfromtheRNNattentionmechanism(Chapter8), we’lldefineattentionfromscratchhereforreaderswhohaven’tyetreadChapter8.2 CHAPTER 9 • THE TRANSFORMER token i) to an output vector h . The set of n blocks maps an entire context window i of input vectors (x ,...,x ) to a window of output vectors (h ,...,h ) of the same 1 n 1 n length. A column might contain from 12 to 96 or more stacked blocks. The column of blocks is preceded by the input encoding component, which pro- cesses an input token (like the word thanks) into a contextual vector representation, using an embedding matrix E and a mechanism for encoding token position. Each column is followed by a language modeling head, which takes the embedding out- put by the final transformer block, passes it through an unembedding matrix U and a softmax over the vocabulary to generate a single token for that column. Transformer-based language models are complex, and so the details will unfold over the next 5 chapters. In the next sections we’ll introduce multi-head attention, the rest of the transformer block, and the input encoding and language modeling head components. Chapter 10 discusses how language models are pretrained, and how tokens are generated via sampling. Chapter 11 introduces masked language modeling and the BERT family of bidirectional transformer encoder models. Chap- ter 12 shows how to prompt LLMs to perform NLP tasks by giving instructions and demonstrations, and how to align the model with human preferences. Chapter 13 will introduce machine translation with the encoder-decoder architecture. 9.1 Attention Recall from Chapter 6 that for word2vec and other static embeddings, the repre- sentation of a word’s meaning is always the same vector irrespective of the context: the word chicken, for example, is always represented by the same fixed vector. So a static vector for the word it might somehow encode that this is a pronoun used for animals and inanimate entities. But in context it has a much richer meaning. Consider it in one of these two sentences: (9.1) The chicken didn’t cross the road because it was too tired. (9.2) The chicken didn’t cross the road because it was too wide. In (9.1) it is the chicken (i.e., the reader knows that the chicken was tired), while in (9.2) it is the road (and the reader knows that the road was wide).2 That is, if we are to compute the meaning of this sentence, we’ll need the meaning of it to be associated with the chicken in the first sentence and associated with the road in the second one, sensitive to the context. Furthermore, consider reading left to right like a causal language model, pro- cessing the sentence up to the word it: (9.3) The chicken didn’t cross the road because it At this point we don’t yet know which thing it is going to end up referring to! So a representation of it at this point might have aspects of both chicken and road as the reader is trying to guess what happens next. This fact that words have rich linguistic relationships with other words that may be far away pervades language. Consider two more examples: (9.4) The keys to the cabinet are on the table. (9.5) I walked along the pond, and noticed one of the trees along the bank. 2 We say that in the first example it corefers with the chicken, and in the second it corefers with the road;we’llreturntothisinChapter23.9.1 • ATTENTION 3 In (9.4), the phrase The keys is the subject of the sentence, and in English and many languages, must agree in grammatical number with the verb are; in this case both are plural. In English we can’t use a singular verb like is with a plural subject like keys (we’ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers to the side of a pond or river and not a financial institution because of the context, including words like pond. (We’ll discuss word senses more in Chapter 11.) The point of all these examples is that these contextual words that help us com- pute the meaning of words in context can be quite far away in the sentence or para- graph. Transformers can build contextual representations of word meaning, contex- contextual tual embeddings, by integrating the meaning of these helpful contextual words. In a embeddings transformer, layer by layer, we build up richer and richer contextualized representa- tions of the meanings of input tokens. At each layer, we compute the representation of a token i by combining information about i from the previous layer with infor- mation about the neighboring tokens to produce a contextualized representation for each word at each position. Attention is the mechanism in the transformer that weighs and combines the representations from appropriate other tokens in the context from layer k−1 to build the representation for tokens in layer k. ehT ehT nekcihc nekcihc t’ndid t’ndid ssorc ssorc eht eht daor daor esuaceb esuaceb ti ti saw saw oot oot derit derit columns corresponding to input tokens Layer k+1 self-attention distribution Layer k Figure 9.2 The self-attention weight distribution α that is part of the computation of the representation for the word it at layer k+1. In computing the representation for it, we attend differently to the various words at layer l, with darker shades indicating higher self-attention values. Note that the transformer is attending highly to the columns corresponding to the tokenschickenandroad,asensibleresult,sinceatthepointwhereitoccurs,itcouldplausibly corefers with the chicken or the road, and hence we’d like the representation for it to draw on the representation for these earlier words. Figure adapted from Uszkoreit (2017). Fig. 9.2 shows a schematic example simplified from a transformer (Uszkoreit, 2017). The figure describes the situation when the current token is it and we need tocomputeacontextualrepresentationforthistokenatlayerk+1ofthetransformer, drawing on the representations (from layer k) of every prior token. The figure uses color to represent the attention distribution over the contextual words: the tokens chicken and road both have a high attention weight, meaning that as we are com- puting the representation for it, we will draw most heavily on the representation for chicken and road. This will be useful in building the final representation for it, since it will end up coreferring with either chicken or road. Let’s now turn to how this attention distribution is represented and computed.4 CHAPTER 9 • THE TRANSFORMER 9.1.1 Attention more formally As we’ve said, the attention computation is a way to compute a vector representation for a token at a particular layer of a transformer, by selectively attending to and integrating information from prior tokens at the previous layer. Attention takes an input representation x corresponding to the input token at position i, and a context i window of prior inputs x ..x , and produces an output a . 1 i−1 i In causal, left-to-right language models, the context is any of the prior words. That is, when processing x , the model has access to x as well as the representations i i of all the prior tokens in the context window (context windows consist of thousands of tokens) but no tokens after i. (By contrast, in Chapter 11 we’ll generalize attention so it can also look ahead to future words.) Fig. 9.3 illustrates this flow of information in an entire causal self-attention layer, in which this same attention computation happens in parallel at each token position i. Thus a self-attention layer maps input sequences (x ,...,x ) to output sequences 1 n of the same length (a ,...,a ). 1 n a1 a2 a3 a4 a5 Self-Attention attention attention attention attention attention Layer x1 x2 x3 x4 x5 Figure 9.3 Information flow in causal self-attention. When processing each input x i, the model attends to all the inputs up to, and including x i. Simplified version of attention At its heart, attention is really just a weighted sum of context vectors, with a lot of complications added to how the weights are computed and what gets summed. For pedagogical purposes let’s first describe a simplified intuition of attention, in which the attention output a at token position i i is simply the weighted sum of all the representations x , for all j ≤ i; we’ll use α j ij to mean how much x should contribute to a : i j (cid:88) Simplified version: a i = α ijx j (9.6) j≤i Each α is a scalar used for weighing the value of input x when summing up ij j the inputs to compute a . How shall we compute this α weighting? In attention we i weight each prior embedding proportionally to how similar it is to the current token i. So the output of attention is a sum of the embeddings of prior tokens weighted by their similarity with the current token embedding. We compute similarity scores via dot product, which maps two vectors into a scalar value ranging from −∞ to ∞. The larger the score, the more similar the vectors that are being compared. We’ll normalize these scores with a softmax to create the vector of weights α , j ≤ i. ij Simplified Version: score(x i,x j) = x i·x j (9.7) α ij = softmax(score(x i,x j)) ∀j ≤ i (9.8) Thus in Fig. 9.3 we compute a by computing three scores: x ·x , x ·x and x ·x , 3 3 1 3 2 3 3 normalizing them by a softmax, and using the resulting probabilities as weights indicating each of their proportional relevance to the current position i. Of course,9.1 • ATTENTION 5 the softmax weight will likely be highest for x , since x is very similar to itself, i i resulting in a high dot product. But other context words may also be similar to i, and the softmax will also assign some weight to those words. Then we use these weights as the α values in Eq. 9.6 to compute the weighted sum that is our a . 3 The simplified attention in equations 9.6 – 9.8 demonstrates the attention-based approach to computing a : compare the x to prior vectors, normalize those scores i i into a probability distribution used to weight the sum of the prior vector. But now we’re ready to remove the simplifications. A single attention head using query, key, and value matrices Now that we’ve attentionhead seen a simple intuition of attention, let’s introduce the actual attention head, the head version of attention that’s used in transformers. (The word head is often used in transformers to refer to specific structured layers). The attention head allows us to distinctly represent three different roles that each input embedding plays during the course of the attention process: • As the current element being compared to the preceding inputs. We’ll refer to query this role as a query. • In its role as a preceding input that is being compared to the current element key to determine a similarity weight. We’ll refer to this role as a key. value • And finally, as a value of a preceding element that gets weighted and summed up to compute the output for the current element. To capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will project each input vector x into a represen- i tation of its role as a key, query, or value: q i = x iWQ; k i = x iWK; v i = x iWV (9.9) Given these projections, when we are computing the similarity of the current ele- ment x with some prior element x , we’ll use the dot product between the current i j element’s query vector q and the preceding element’s key vector k . Furthermore, i j the result of a dot product can be an arbitrarily large (positive or negative) value, and exponentiating large values can lead to numerical issues and loss of gradients during training. To avoid this, we scale the dot product by a factor related to the size of the embeddings, via diving by the square root of the dimensionality of the query and key vectors (d ). We thus replace the simplified Eq. 9.7 with Eq. 9.11. The ensuing k softmax calculation resulting in α remains the same, but the output calculation for ij a is now based on a weighted sum over the value vectors v (Eq. 9.13). i Here’s a final set of equations for computing self-attention for a single self- attention output vector a from a single input vector x . This version of attention i i computes a by summing the values of the prior elements, each weighted by the i similarity of its key to the query from the current element: q i = x iWQ; k j = x jWK; v j = x jWV (9.10) q ·k i j score(x i,x j) = √ (9.11) d k α ij = softmax(score(x i,x j)) ∀j ≤ i (9.12) (cid:88) a i = α ijv j (9.13) j≤i We illustrate this in Fig. 9.4 for the case of calculating the value of the third output a in a sequence. 36 CHAPTER 9 • THE TRANSFORMER a Output of self-attention 3 6. Sum the weighted value vectors 4. Turn into weights via softmax i,j 𝛼 2. Compare x3’s query with the keys for x1, x2, and x3 Wk k k k 1. Generate Wq q q q key, query, value vectors x v x v x Wv v 1 2 3 × × 5. Weigh each value vector 𝛼3,1 𝛼3,2 𝛼3,3 ÷ ÷ ÷ 3. Divide score by √d k √dk √dk √dk Wk Wk Wq Wq Wv Wv Figure 9.4 Calculating the value of a , the third element of a sequence using causal (left- 3 to-right) self-attention. Let’s talk shapes. The input to attention x and the output from attention a both i i have the same dimensionality 1×d (We often call d the model dimensionality, and indeed as we’ll discuss in Section 9.2 the output h of each transformer block, i as well as the intermediate vectors inside the transformer block also have the same dimensionality 1×d.). We’ll have a dimension d for the key and query vectors. The query vector and k the key vector are both dimensionality 1×d , so we can take their dot product q ·k . k i j We’ll have a separate dimension d for the value vectors. The transform matrix WQ v has shape [d ×d ], WK is [d ×d ], and WV is [d ×d ]. In the original transformer k k v work (Vaswani et al., 2017), d was 512, d and d were both 64. k v Multi-head Attention Equations 9.11-9.13 describe a single attention head. But actually, transformers use multiple attention heads. The intuition is that each head might be attending to the context for different purposes: heads might be special- ized to represent different linguistic relationships between context elements and the current token, or to look for particular kinds of patterns in the context. multi-head So in multi-head attention we have h separate attention heads that reside in attention parallel layers at the same depth in a model, each with its own set of parameters that allows the head to model different aspects of the relationships among inputs. Thus each head i in a self-attention layer has its own set of key, query and value matrices: WKi, WQi and WVi. These are used to project the inputs into separate key, value, and query embeddings for each head. When using multiple heads the model dimension d is still used for the input and output, the key and query embeddings have dimensionality d , and the value k embeddings are of dimensionality d (again, in the original transformer paper d = v k d = 64, h = 8, and d = 512). Thus for each head i, we have weight layers WQi of v shape [d×d ], WKi of shape [d×d ], and WVi of shape [d×d ]. k k v Below are the equations for attention augmented with multiple heads; Fig. 9.59.2 • TRANSFORMER BLOCKS 7 shows an intuition. qc i = x iWQc; kc j = x jWKc; vc j = x jWVc; ∀ c 1 ≤ c ≤ h (9.14) qc·kc scorec(x i,x j) = √i j (9.15) d k α ic j = softmax(scorec(x i,x j)) ∀j ≤ i (9.16) (cid:88) headc = αcvc (9.17) i ij j j≤i a i = (head1⊕head2...⊕headh)WO (9.18) MultiHeadAttention(x i,[x 1,··· ,x N]) = a i (9.19) The output of each of the h heads is of shape 1×d , and so the output of the v multi-head layer with h heads consists of h vectors of shape 1×d . These are con- v catenated to produce a single output with dimensionality 1×hd . Then we use yet v another linear projection WO ∈ Rhdv×d to reshape it, resulting in the multi-head attention vector a with the correct output shape [1xd] at each input i. i a i [1 x d] Project down to d WO [hd x d] v … [1 x hd ] v Concatenate Outputs [1 x d v ] [1 x d v ] Each head Head 1 Head 2 … Head 8 attends differently WK WV WQ WK WV WQ WK WV WQ to context 1 1 1 2 2 2 8 8 8 … x x x x i-3 i-2 i-1 i [1 ax d] i Figure 9.5 The multi-head attention computation for input x i, producing output a i. A multi-head attention layer has h heads, each with its own key, query and value weight matrices. The outputs from each of the heads are concatenated and then projected down to d, thus producing an output of the same size as the input. 9.2 Transformer Blocks The self-attention calculation lies at the core of what’s called a transformer block, which, in addition to the self-attention layer, includes three other kinds of layers: (1) a feedforward layer, (2) residual connections, and (3) normalizing layers (colloqui- ally called “layer norm”). Fig. 9.6 illustrates a transformer block, sketching a common way of thinking residualstream about the block that is called the residual stream (Elhage et al., 2021). In the resid- ual stream viewpoint, we consider the processing of an individual token i through the transformer block as a single stream of d-dimensional representations for token position i. This residual stream starts with the original input vector, and the various8 CHAPTER 9 • THE TRANSFORMER h h h i-1 i Residual i+1 Stream + Feedforward Layer Norm … … + MultiHead Attention Layer Norm x x x i-1 i i+1 Figure 9.6 The architecture of a transformer block showing the residual stream. This figureshowstheprenormversionofthearchitecture,inwhichthelayernormshappenbefore the attention and feedforward layers rather than after. components read their input from the residual stream and add their output back into the stream. The input at the bottom of the stream is an embedding for a token, which has dimensionality d. This initial embedding gets passed up (by residual connections), and is progressively added to by the other components of the transformer: the at- tention layer that we have seen, and the feedforward layer that we will introduce. Before the attention and feedforward layer is a computation called the layer norm. Thus the initial vector is passed through a layer norm and attention layer, and the result is added back into the stream, in this case to the original input vector x . And then this summed vector is again passed through another layer norm and a i feedforward layer, and the output of those is added back into the residual, and we’ll use h to refer to the resulting output of the transformer block for token i. (In earlier i descriptions the residual stream was often described using a different metaphor as residual connections that add the input of a component to its output, but the residual stream is a more perspicuous way of visualizing the transformer.) We’ve already seen the attention layer, so let’s now introduce the feedforward and layer norm computations in the context of processing a single input x at token i position i. Feedforward layer The feedforward layer is a fully-connected 2-layer network, i.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights are the same for each token position i , but are different from layer to layer. It is common to make the dimensionality d of the hidden layer of the feedforward ff network be larger than the model dimensionality d. (For example in the original transformer model, d = 512 and d = 2048.) ff FFN(x i) = ReLU(x iW 1+b 1)W 2+b 2 (9.20) Layer Norm At two stages in the transformer block we normalize the vector (Ba layernorm et al., 2016). This process, called layer norm (short for layer normalization), is one9.2 • TRANSFORMER BLOCKS 9 of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. Layer norm is a variation of the z-score from statistics, applied to a single vec- tor in a hidden layer. That is, the term layer norm is a bit confusing; layer norm is not applied to an entire transformer layer, but just to the embedding vector of a single token. Thus the input to layer norm is a single vector of dimensionality d and the output is that vector normalized, again of dimensionality d. The first step in layer normalization is to calculate the mean, µ, and standard deviation, σ, over the elements of the vector to be normalized. Given an embedding vector x of dimen- sionality d, these values are calculated as follows. d 1 (cid:88) µ = x i (9.21) d i=1 (cid:118) (cid:117) d (cid:117)1 (cid:88) σ = (cid:116) (x i−µ)2 (9.22) d i=1 Given these values, the vector components are normalized by subtracting the mean from each and dividing by the standard deviation. The result of this computation is a new vector with zero mean and a standard deviation of one. (x−µ) ˆx = (9.23) σ Finally, in the standard implementation of layer normalization, two learnable param- eters, γ and β, representing gain and offset values, are introduced. (x−µ) LayerNorm(x) = γ +β (9.24) σ Putting it all together The function computed by a transformer block can be ex- pressed by breaking it down with one equation for each component computation, using t (of shape [1×d]) to stand for transformer and superscripts to demarcate each computation inside the block: t1 i = LayerNorm(x i) (9.25) t2 = MultiHeadAttention(t1,(cid:2) x1,··· ,x1 (cid:3) ) (9.26) i i 1 N t3 i = t2 i +x i (9.27) t4 = LayerNorm(t3) (9.28) i i t5 = FFN(t4) (9.29) i i h i = t5 i +t3 i (9.30) Notice that the only component that takes as input information from other tokens (other residual streams) is multi-head attention, which (as we see from (9.27)) looks at all the neighboring tokens in the context. The output from attention, however, is thenaddedintothistoken’sembeddingstream. Infact, Elhageetal.(2021)showthat we can view attention heads as literally moving information from the residual stream of a neighboring token into the current stream. The high-dimensional embedding space at each position thus contains information about the current token and about neighboring tokens, albeit in different subspaces of the vector space. Fig. 9.7 shows a visualization of this movement.10 CHAPTER 9 • THE TRANSFORMER Token A Token B residual residual stream stream Figure 9.7 An attention head can move information from token A’s residual stream into token B’s residual stream. Crucially, the input and output dimensions of transformer blocks are matched so they can be stacked. Each token vector x at the input to the block has dimensionality i d, and the output h also has dimensionality d. Transformers for large language i models stack many of these blocks, from 12 layers (used for the T5 or GPT-3-small language models) to 96 layers (used for GPT-3 large), to even more for more recent models. We’ll come back to this issue of stacking in a bit. Equation (9.27) and following are just the equation for a single transformer block, but the residual stream metaphor goes through all the transformer layers, from the first transformer blocks to the 12th, in a 12-layer transformer. At the ear- lier transformer blocks, the residual stream is representing the current token. At the highest transformer blocks, the residual stream is usually representing the following token, since at the very end it’s being trained to predict the next token. Once we stack many blocks, there is one more requirement: at the very end of the last (highest) transformer block, there is a single extra layer norm that is run on the last h of each token stream (just below the language model head layer that we i will define soon). 3 9.3 Parallelizing computation using a single matrix X This description of multi-head attention and the rest of the transformer block has been from the perspective of computing a single output at a single time step i in a single residual stream. But as we pointed out earlier, the attention computation performed for each token to compute a is independent of the computation for each i other token, and that’s also true for all the computation in the transformer block computing h from the input x . That means we can easily parallelize the entire i i computation, taking advantage of efficient matrix multiplication routines. We do this by packing the input embeddings for the N tokens of the input se- quence into a single matrix X of size [N ×d]. Each row of X is the embedding of one token of the input. Transformers for large language models commonly have an input length N = 1K, 2K, or as many as 32K tokens (or more), so X typically has be- tween 1K and 32K rows, each of the dimensionality of the embedding d (the model 3 Notethatweareusingthemostcommoncurrenttransformerarchitecture,whichiscalledtheprenorm architecture. TheoriginaldefinitionofthetransformerinVaswanietal.(2017)usedanalternativearchi- tecture called the postnorm transformer in which the layer norm happens after the attention and FFN layers; it turns out moving the layer norm beforehand works better, but does require this one extra layer attheend.9.3 • PARALLELIZING COMPUTATION USING A SINGLE MATRIX X 11 dimension). Parallelizing attention Let’s first see this for a single attention head and then turn to multiple heads, and then add in the rest of the components in the transformer block. For one head we multiply X by the key, query, and value matrices WQ of shape [d×d ], WK of shape [d×d ], and WV of shape [d×d ], to produce matrices k k v Q of shape [N ×d k], K ∈ RN×dk, and V ∈ RN×dv, containing all the key, query, and value vectors: Q = XWQ; K = XWK; V = XWV (9.31) Given these matrices we can compute all the requisite query-key comparisons simul- taneously by multiplying Q and K(cid:124) in a single matrix multiplication. The product is of shape N×N, visualized in Fig. 9.8. q1•k1 q1•k2 q1•k3 q1•k4 q2•k1 q2•k2 q2•k3 q2•k4 N q3•k1 q3•k2 q3•k3 q3•k4 q4•k1 q4•k2 q4•k3 q4•k4 N Figure 9.8 The N ×N QK(cid:124) matrix showing how it computes all q i·k j comparisons in a single matrix multiple. Once we have this QK(cid:124) matrix, we can very efficiently scale these scores, take the softmax, and then multiply the result by V resulting in a matrix of shape N ×d: a vector embedding representation for each token in the input. We’ve reduced the entire self-attention step for an entire sequence of N tokens for one head to the following computation: (cid:18) (cid:18) (cid:124)(cid:19)(cid:19) QK A = softmax mask √ V (9.32) d k Maskingoutthefuture Youmayhavenoticedthatweintroducedamaskfunction in Eq. 9.32 above. This is because the self-attention computation as we’ve described (cid:124) it has a problem: the calculation in QK results in a score for each query value to every key value, including those that follow the query. This is inappropriate in the setting of language modeling: guessing the next word is pretty simple if you already know it! To fix this, the elements in the upper-triangular portion of the matrix are zeroed out (set to −∞), thus eliminating any knowledge of words that follow in the sequence. This is done in practice by adding a mask matrix M in which M =−∞∀j >i (i.e. for the upper-triangular portion) and M =0 otherwise. ij ij (cid:124) Fig. 9.9 shows the resulting masked QK matrix. (we’ll see in Chapter 11 how to make use of words in the future for tasks that need it). Fig. 9.10 shows a schematic of all the computations for a single attention head parallelized in matrix form. Fig. 9.8 and Fig. 9.9 also make it clear that attention is quadratic in the length of the input, since at each layer we need to compute dot products between each pair of tokens in the input. This makes it expensive to compute attention over very long documents (like entire novels). Nonetheless modern large language models manage to use quite long contexts of thousands or tens of thousands of tokens.12 CHAPTER 9 • THE TRANSFORMER q1•k1 −∞ −∞ −∞ q2•k1 q2•k2 −∞ −∞ N q3•k1 q3•k2 q3•k3 −∞ q4•k1 q4•k2 q4•k3 q4•k4 N Figure 9.9 The N×N QK(cid:124) matrix showing the q i·k j values, with the upper-triangle por- tion of the comparisons matrix zeroed out (set to −∞, which the softmax will turn to zero). q1 q2 q3 q4 k1 k2 k3 k4 X Q X K X V ToIn kp eu nt 1 WQ TQ oku ee nry 1 ToIn kp eu nt 1 WK ToK kee ny 1 ToIn kp eu nt 1 WV ToV ka elu ne 1 Input Query Input Key Input Value Token 2 x = Token 2 Token 2 x = Token 2 Token 2 x = Token 2 Input Query Input Key Input Value Token 3 Token 3 Token 3 Token 3 Token 3 Token 3 Input Query Input Key Input Value Token 4 d x d Token 4 Token 4 d x d k Token 4 Token 4 d x d v Token 4 k N x d N x d k N x d N x d k N x d N x d v T Q K QKT QKT masked V A x = q1•k1 q1•k2 q1•k3 q1•k4 qqq111•••kkk111 −∞ −∞ −∞ v1 a1 mask q2•k1 q2•k2 q2•k3 q2•k4 = q2•k1 q2•k2 −∞ −∞ x v2 = a2 q3•k1 q3•k2 q3•k3 q3•k4 q3•k1 q3•k2 q3•k3 −∞ v3 a3 d k x N q4•k1 q4•k2 q4•k3 q4•k4 q4•k1 q4•k2 q4•k3 q4•k4 v4 a4 N x d k N x N N x N N x d v N x d v Figure 9.10 Schematicoftheattentioncomputationforasingleattentionheadinparallel. Thefirstrowshows the computation of the Q, K, and V matrices. The second row shows the computation of QKT, the masking (the softmax computation and the normalizing by dimensionality are not shown) and then the weighted sum of the value vectors to get the final attention vectors. Parallelizing multi-head attention In multi-head attention, as with self-attention, the input and output have the model dimension d, the key and query embeddings have dimensionality d , and the value embeddings are of dimensionality d (again, k v in the original transformer paper d = d = 64, h = 8, and d = 512). Thus for each k v head i, we have weight layers WQ i ∈ Rd×dk, WK i ∈ Rd×dk, and WV i ∈ Rd×dv, and these get multiplied by the inputs packed into X to produce Q ∈ RN×dk, K ∈ RN×dk, and V ∈ RN×dv. The output of each of the h heads is of shape N ×d v, and so the outputofthemulti-headlayerwithhheadsconsistsofhmatricesofshapeN×d . To v make use of these matrices in further processing, they are concatenated to produce a single output with dimensionality N ×hd . Finally, we use yet another linear v projection WO ∈ Rhdv×d, that reshape it to the original output dimension for each token. Multiplying the concatenated N ×hd v matrix output by WO ∈ Rhdv×d yields9.4 • THE INPUT: EMBEDDINGS FOR TOKEN AND POSITION 13 the self-attention output A of shape [N×d]. Qi = XWQi ; Ki = XWKi ; Vi = XWVi (9.33) (cid:18) QiKi(cid:124)(cid:19) head i = SelfAttention(Qi,Ki,Vi) = softmax √ Vi (9.34) d k MultiHeadAttention(X) = (head 1⊕head 2...⊕head h)WO (9.35) Putting it all together with the parallel input matrix X The function computed in parallel by an entire layer of N transformer block over the entire N input tokens can be expressed as: O = LayerNorm(X+MultiHeadAttention(X)) (9.36) H = LayerNorm(O+FFN(O)) (9.37) Or we can break it down with one equation for each component computation, using T (of shape [N ×d]) to stand for transformer and superscripts to demarcate each computation inside the block: T1 = MultiHeadAttention(X) (9.38) T2 = X+T1 (9.39) T3 = LayerNorm(T2) (9.40) T4 = FFN(T3) (9.41) T5 = T4+T3 (9.42) H = LayerNorm(T5) (9.43) Here when we use a notation like FFN(T3) we mean that the same FFN is applied in parallel to each of the N embedding vectors in the window. Similarly, each of the N tokens is normed in parallel in the LayerNorm. Crucially, the input and output dimensions of transformer blocks are matched so they can be stacked. Since each token x at the input to the block has dimensionality d, that means the input X and i output H are both of shape [N×d]. 9.4 The input: embeddings for token and position Let’s talk about where the input X comes from. Given a sequence of N tokens (N is embedding the context length in tokens), the matrix X of shape [N ×d] has an embedding for each word in the context. The transformer does this by separately computing two embeddings: an input token embedding, and an input positional embedding. A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di- mension d that will be our initial representation for the input token. (As we pass vectors up through the transformer layers in the residual stream, this embedding representation will change and grow, incorporating context and playing a different role depending on the kind of language model we are building.) The set of initial embeddings are stored in the embedding matrix E, which has a row for each of the |V| tokens in the vocabulary. Thus each word is a row vector of d dimensions, and E has shape [|V|×d]. Given an input token string like Thanks for all the we first convert the tokens into vocabulary indices (these were created when we first tokenized the input using14 CHAPTER 9 • THE TRANSFORMER BPE or SentencePiece). So the representation of thanks for all the might be w = [5,4000,10532,2224]. Next we use indexing to select the corresponding rows from E, (row 5, row 4000, row 10532, row 2224). Another way to think about selecting token embeddings from the embedding matrix is to represent tokens as one-hot vectors of shape [1×|V|], i.e., with one one-hotvector dimension for each word in the vocabulary. Recall that in a one-hot vector all the elements are 0 except one, the element whose dimension is the word’s index in the vocabulary, which hasvalue1. Soifthe word“thanks”has index5in thevocabulary, x = 1, and x = 0 ∀i (cid:54)= 5, as shown here: 5 i [0 0 0 0 1 0 0 ... 0 0 0 0] 1 2 3 4 5 6 7 ... ... |V| Multiplyingbyaone-hotvectorthathasonlyonenon-zeroelementx =1simply i selects out the relevant row vector for word i, resulting in the embedding for word i, as depicted in Fig. 9.11. d 5 5 |V| d 1 0 0 0 0 1 0 0 … 0 0 0 0 ✕ E = 1 |V| Figure 9.11 Selecting the embedding vector for word V by multiplying the embedding 5 matrix E with a one-hot vector with a 1 in index 5. We can extend this idea to represent the entire token sequence as a matrix of one- hot vectors, one for each of the N positions in the transformer’s context window, as shown in Fig. 9.12. d |V| d 0 0 0 0 1 0 0 … 0 0 0 0 0 0 0 0 0 0 0 … 0 0 1 0 ✕ = 1 0 0 0 0 0 0 … 0 0 0 0 E … N N 0 0 0 0 1 0 0 … 0 0 0 0 |V| Figure 9.12 Selecting the embedding matrix for the input sequence of token idsW by mul- tiplying a one-hot matrix corresponding toW by the embedding matrix E. These token embeddings are not position-dependent. To represent the position of each token in the sequence, we combine these token embeddings with positional positional embeddings specific to each position in an input sequence. embeddings Where do we get these positional embeddings? The simplest method, called absolute absolute position, is to start with randomly initialized embeddings corresponding position to each possible input position up to some maximum length. For example, just as we have an embedding for the word fish, we’ll have an embedding for the position 3. As with word embeddings, these positional embeddings are learned along with other parameters during training. We can store them in a matrix Epos of shape [1×N]. To produce an input embedding that captures positional information, we just add the word embedding for each input to its corresponding positional embedding. The individual token and position embeddings are both of size [1×d], so their sum is9.5 • THE LANGUAGE MODELING HEAD 15 Transformer Block X = Composite Embeddings (word + position) Janet 1 will 2 back 3 Janet will back the bill + + + the + 4 bill + 5 Word Embeddings Position Embeddings Figure 9.13 A simple way to model position: add an embedding of the absolute position to the token embedding to produce a new embedding of the same dimensionality. also [1×d], This new embedding serves as the input for further processing. Fig. 9.13 shows the idea. The final representation of the input, the matrix X, is an [N×d] matrix in which each row i is the representation of the ith token in the input, computed by adding E[id(i)]—the embedding of the id of the token that occurred at position i—, to P[i], the positional embedding of position i. A potential problem with the simple absolute position embedding approach is that there will be plenty of training examples for the initial positions in our inputs and correspondingly fewer at the outer length limits. These latter embeddings may be poorly trained and may not generalize well during testing. An alternative ap- proach to absolute positional embeddings is to choose a static function that maps integer inputs to real-valued vectors in a way that captures the inherent relation- ships among the positions. That is, it captures the fact that position 4 in an input is more closely related to position 5 than it is to position 17. A combination of sine and cosine functions with differing frequencies was used in the original transformer work. Even more complex positional embedding methods exist, such as ones that represent relative position instead of absolute position, often implemented in the attention mechanism at each layer rather than being added once at the initial input. 9.5 The Language Modeling Head The last component of the transformer we must introduce is the language modeling language head. Here we are using the word head to mean the additional neural circuitry we modelinghead head add on top of the basic transformer architecture when we apply pretrained trans- former models to various tasks. The language modeling head is the circuitry we need to do language modeling. Recallthatlanguagemodels, fromthesimplen-grammodelsofChapter3through the feedforward and RNN language models of Chapter 7 and Chapter 8, are word predictors. Given a context of words, they assign a probability to each possible next word. For example, if the preceding context is “Thanks for all the” and we want to know how likely the next word is “fish” we would compute: P(fish|Thanks for all the) Language models give us the ability to assign such a conditional probability to every possible next word, giving us a distribution over the entire vocabulary. The n-gram16 CHAPTER 9 • THE TRANSFORMER language models of Chapter 3 compute the probability of a word given counts of its occurrence with the n−1 prior words. The context is thus of size n−1. For transformer language models, the context is the size of the transformer’s context window, which can be quite large: 2K, 4K, even 32K tokens for very large models. The job of the language modeling head is to take the output of the final trans- former layer from the last token N and use it to predict the upcoming word at posi- tion N+1. Fig. 9.14 shows how to accomplish this task, taking the output of the last token at the last layer (the d-dimensional output embedding of shape [1×d]) and producing a probability distribution over words (from which we will choose one to generate). y1 y2 … y|V| Word probabilities 1 x |V| Language Model Head Softmax over vocabulary V L takes h N and outputs a u1 u2 … u|V| Logits 1 x |V| distribution over vocabulary V Unembedding layer Unembedding layer d x |V| U = ET hL 1 hL 2 hL N 1 x d Layer L Transformer Block … w1 w2 w N Figure 9.14 The language modeling head: the circuit at the top of a transformer that maps from the output embedding for token N from the last transformer layer (hL) to a probability distribution over words in the N vocabularyV. The first module in Fig. 9.14 is a linear layer, whose job is to project from the output hL, which represents the output token embedding at position N from the final N logit block L, (hence of shape [1×d]) to the logit vector, or score vector, that will have a single score for each of the |V| possible words in the vocabularyV. The logit vector u is thus of dimensionality 1×|V|. This linear layer can be learned, but more commonly we tie this matrix to (the weighttying transpose of) the embedding matrix E. Recall that in weight tying, we use the same weights for two different matrices in the model. Thus at the input stage of the transformer the embedding matrix (of shape [|V|×d]) is used to map from a one-hot vector over the vocabulary (of shape [1×|V|]) to an embedding (of shape [1×d]). And then in the language model head, ET, the transpose of the embedding matrix (of shape [d×|V|]) is used to map back from an embedding (shape [1×d]) to a vector over the vocabulary (shape [1×|V|]). In the learning process, E will be optimized to be good at doing both of these mappings. We therefore sometimes call the transpose unembedding ET the unembedding layer because it is performing this reverse mapping. A softmax layer turns the logits u into the probabilities y over the vocabulary. u = hL ET (9.44) N y = softmax(u) (9.45) We can use these probabilities to do things like help assign a probability to a given text. But the most important usage to generate text, which we do by sampling9.5 • THE LANGUAGE MODELING HEAD 17 a word from these probabilities y. We might sample the highest probability word (‘greedy’ decoding), or use another of the sampling methods we’ll introduce in Sec- tion ??. In either case, whatever entry y we choose from the probability vector y, k we generate the word that has that index k. … Token probabilities y1 y2 y|V| w i+1 Sample token to Language softmax generate at position i+1 Modeling Head logits u1 u2 … u|V| U hL i feedforward Layer L layer norm attention layer norm hL-1 = xL … i i h2 = x3 i i feedforward layer norm Layer 2 attention layer norm h1 = x2 i i feedforward Layer 1 layer norm attention layer norm x1 i + i Input Encoding E Input token w i Figure 9.15 A transformer language model (decoder-only), stacking transformer blocks and mapping from an input token w i to to a predicted next token w i+1. Fig. 9.15 shows the total stacked architecture for one token i. Note that the input to each transformer layer x is the same as the output from the preceding layer h−. i i Now that we see all these transformer layers spread out on the page, we can point out another useful feature of the unembedding layer: as a tool for interpretability of logitlens the internals of the transformer that we call the logit lens (Nostalgebraist, 2020). We can take a vector from any layer of the transformer and, pretending that it is the prefinal embedding, simply multiply it by the unembedding layer to get logits, and compute a softmax to see the distribution over words that that vector might be representing. This can be a useful window into the internal representations of the model. Since the network wasn’t trained to make the internal representations function in this way, the logit lens doesn’t always work perfectly, but this can still be a useful trick. A terminological note before we conclude: You will sometimes see a trans- former used for this kind of unidirectional causal language model called a decoder- decoder-only only model. This is because this model constitutes roughly half of the encoder- model18 CHAPTER 9 • THE TRANSFORMER decoder model for transformers that we’ll see how to apply to machine translation in Chapter 13. (Confusingly, the original introduction of the transformer had an encoder-decoder architecture, and it was only later that the standard paradigm for causal language model was defined by using only the decoder part of this original architecture). 9.6 Summary This chapter has introduced the transformer and its components for the task of lan- guage modeling. We’ll continue the task of language modeling including issues like training and sampling in the next chapter. Here’s a summary of the main points that we covered: • Transformers are non-recurrent networks based on multi-head attention, a kind of self-attention. A multi-head attention computation takes an input vector x and maps it to an output a by adding in vectors from prior tokens, i i weighted by how relevant they are for the processing of the current word. • A transformer block consists of a residual stream in which the input from the prior layer is passed up to the next layer, with the output of different com- ponents added to it. These components include a multi-head attention layer followed by a feedforward layer, each preceded by layer normalizations. Transformer blocks are stacked to make deeper and more powerful networks. • The input to a transformer is a computing by adding an embedding (computed with an embedding matrix) to a positional encoding that represents the se- quential position of the token in the window. • Language models can be built out of stacks of transformer blocks, with a language model head at the top, which applies an unembedding matrix to the output H of the top layer to generate the logits, which are then passed through a softmax to generate word probabilities. • Transformer-based language models have a wide context window (as wide as 32768 tokens for very large models) allowing them to draw on enormous amounts of context to predict upcoming words. Bibliographical and Historical Notes The transformer (Vaswani et al., 2017) was developed drawing on two lines of prior research: self-attention and memory networks. Encoder-decoder attention, the idea of using a soft weighting over the encodings of input words to inform a generative decoder (see Chapter 13) was developed by Graves (2013) in the context of handwriting generation, and Bahdanau et al. (2015) for MT. This idea was extended to self-attention by dropping the need for separate encoding and decoding sequences and instead seeing attention as a way of weighting the tokens in collecting information passed from lower layers to higher layers (Ling et al., 2015; Cheng et al., 2016; Liu et al., 2016). Other aspects of the transformer, including the terminology of key, query, and value, came from memory networks, a mechanism for adding an external read-BIBLIOGRAPHICAL AND HISTORICAL NOTES 19 write memory to networks, by using an embedding of a query to match keys rep- resenting content in an associative memory (Sukhbaatar et al., 2015; Weston et al., 2015; Graves et al., 2014). MORE HISTORY TBD IN NEXT DRAFT.20 Chapter 9 • The Transformer Ba,J.L.,J.R.Kiros,andG.E.Hinton.2016. Layernormal- ization. NeurIPSworkshop. Bahdanau, D., K. H. Cho, and Y. Bengio. 2015. Neural ma- chinetranslationbyjointlylearningtoalignandtranslate. ICLR2015. Cheng, J., L. Dong, and M. Lapata. 2016. Long short-term memory-networksformachinereading. EMNLP. Elhage, N., N. Nanda, C. Olsson, T. Henighan, N. Joseph, B.Mann,A.Askell,Y.Bai,A.Chen,T.Conerly,N.Das- Sarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Her- nandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCan- dlish, and C. Olah. 2021. A mathematical framework for transformercircuits. Whitepaper. Graves,A.2013.Generatingsequenceswithrecurrentneural networks. ArXiv. Graves, A., G. Wayne, and I. Danihelka. 2014. Neural Tur- ingmachines. ArXiv. Ling, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez, S. Amir, L. Marujo, and T. Lu´ıs. 2015. Finding function inform: Compositionalcharactermodelsforopenvocab- ularywordrepresentation. EMNLP. Liu,Y.,C.Sun,L.Lin,andX.Wang.2016. Learningnatural language inference using bidirectional LSTM model and inner-attention. ArXiv. Nostalgebraist. 2020. Interpreting gpt: the logit lens. White paper. Sukhbaatar, S., A. Szlam, J. Weston, and R. Fergus. 2015. End-to-endmemorynetworks. NeurIPS. Uszkoreit,J.2017. Transformer: Anovelneuralnetworkar- chitecture for language understanding. Google Research blogpost,ThursdayAugust31,2017. Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Atten- tionisallyouneed. NeurIPS. Weston, J., S. Chopra, and A. Bordes. 2015. Memory net- works. ICLR2015.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6490 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: How many words can one input to the X-Transformer? One\n",
      "Question Relevance: 0.7256\n",
      "Answer Relevance: 0.2463\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the T5 model and tokenizer for generating QA pairs\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "# Load a model to compute embeddings for semantic similarity (e.g., BERT or RoBERTa)\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a function to get embeddings from BERT\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer_bert(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Take the mean of the token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Function to generate QA pairs using T5 model\n",
    "def generate_qa(context):\n",
    "    inputs = tokenizer_t5(context, return_tensors=\"pt\")\n",
    "    outputs = model_t5.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def compute_similarity(qa_pair, context):\n",
    "    # Check if the separator token exists and split accordingly\n",
    "    if tokenizer_t5.sep_token in qa_pair:\n",
    "        question, answer = qa_pair.split(tokenizer_t5.sep_token)\n",
    "    else:\n",
    "        # If the separator is not found, assume the whole text is the question (fallback case)\n",
    "        question = qa_pair\n",
    "        answer = \"\"  # Set answer to empty string, can be refined as needed\n",
    "    \n",
    "    # Get embeddings for the context, question, and answer\n",
    "    context_embedding = get_bert_embeddings(context)\n",
    "    question_embedding = get_bert_embeddings(question)\n",
    "    answer_embedding = get_bert_embeddings(answer)\n",
    "    \n",
    "    # Compute cosine similarity between the context and question, context and answer\n",
    "    question_similarity = cosine_similarity(context_embedding, question_embedding)\n",
    "    answer_similarity = cosine_similarity(context_embedding, answer_embedding)\n",
    "    \n",
    "    return question_similarity[0][0], answer_similarity[0][0]\n",
    "\n",
    "\n",
    "# Generate QA pairs and compute their relevance to the context\n",
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    qa_pair = generate_qa(context)\n",
    "    print(\"Generated QA Pair:\", qa_pair)\n",
    "    \n",
    "    # Compute similarity between the question, answer, and context\n",
    "    question_sim, answer_sim = compute_similarity(qa_pair, context)\n",
    "    \n",
    "    print(f\"Question Relevance: {question_sim:.4f}\")\n",
    "    print(f\"Answer Relevance: {answer_sim:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: What is the matrix X made up of? shape\n",
      "Question Relevance (Cosine Similarity): 0.7014\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Relevance (BERTScore F1): {'precision': [0.493060827255249], 'recall': [0.3040444850921631], 'f1': [0.37614208459854126], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What are the two types of embeddings? The matrix X\n",
      "Question Relevance (Cosine Similarity): 0.7307\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Relevance (BERTScore F1): {'precision': [0.5926547646522522], 'recall': [0.39206796884536743], 'f1': [0.47193172574043274], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of position embeddings? learn a position embedding matrix\n",
      "Question Relevance (Cosine Similarity): 0.7823\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Relevance (BERTScore F1): {'precision': [0.6957597136497498], 'recall': [0.4226910471916199], 'f1': [0.5258906483650208], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What does the matrix X have? shape\n",
      "Question Relevance (Cosine Similarity): 0.6578\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Relevance (BERTScore F1): {'precision': [0.542154848575592], 'recall': [0.30750465393066406], 'f1': [0.39242812991142273], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: How are embeddings created? by adding two distinct embedding for each input\n",
      "Question Relevance (Cosine Similarity): 0.7958\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n",
      "Question Relevance (BERTScore F1): {'precision': [0.7363297939300537], 'recall': [0.42741817235946655], 'f1': [0.5408743619918823], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import evaluate  # Import BERTScore library\n",
    "\n",
    "# Load the T5 model and tokenizer for generating QA pairs\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "# Load a model to compute embeddings for semantic similarity (e.g., BERT or RoBERTa)\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Define a function to get embeddings from BERT\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer_bert(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Take the mean of the token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Function to generate QA pairs using T5 model\n",
    "def generate_qa(context):\n",
    "    inputs = tokenizer_t5(context, return_tensors=\"pt\")\n",
    "    outputs = model_t5.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def compute_similarity(qa_pair, context):\n",
    "    # Check if the separator token exists and split accordingly\n",
    "    if tokenizer_t5.sep_token in qa_pair:\n",
    "        question, answer = qa_pair.split(tokenizer_t5.sep_token)\n",
    "    else:\n",
    "        question = qa_pair\n",
    "        answer = \"\"  # Set answer to empty string if no answer part is present\n",
    "    \n",
    "    # Get embeddings for the context, question, and answer\n",
    "    context_embedding = get_bert_embeddings(context)\n",
    "    question_embedding = get_bert_embeddings(question)\n",
    "    answer_embedding = get_bert_embeddings(answer)\n",
    "    \n",
    "    # Compute cosine similarity between the context and question, context and answer\n",
    "    question_similarity = cosine_similarity(context_embedding, question_embedding)\n",
    "    answer_similarity = cosine_similarity(context_embedding, answer_embedding)\n",
    "    \n",
    "    return question_similarity[0][0], answer_similarity[0][0]\n",
    "\n",
    "# Function to compute BERTScore (Precision, Recall, F1 score)\n",
    "def compute_bertscore(qa_pair, context):\n",
    "    # Split the QA pair\n",
    "    if tokenizer_t5.sep_token in qa_pair:\n",
    "        question, answer = qa_pair.split(tokenizer_t5.sep_token)\n",
    "    else:\n",
    "        question = qa_pair\n",
    "        answer = \"\"  # Set answer to empty if no answer is found\n",
    "    \n",
    "    # BERTScore requires lists of sentences (ground truth and generated)\n",
    "    # We will compute BERTScore for both question and answer separately\n",
    "    reference = [context]  # Reference context\n",
    "    question = [question]\n",
    "    answer = [answer]\n",
    "    \n",
    "    # Calculate precision, recall, F1 score for question and answer using BERTScore\n",
    "    bertscore_result_q = bertscore.compute(predictions=question, references=reference, model_type=\"bert-base-uncased\")\n",
    "    bertscore_result_a = bertscore.compute(predictions=answer, references=reference, model_type=\"bert-base-uncased\")\n",
    "    \n",
    "    return bertscore_result_q, bertscore_result_a\n",
    "\n",
    "# Example context (use your own context text here)\n",
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')\n",
    "\n",
    "# Generate QA pairs and compute their relevance to the context\n",
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    qa_pair = generate_qa(context)\n",
    "    print(\"Generated QA Pair:\", qa_pair)\n",
    "    \n",
    "    # Compute cosine similarity between the question, answer, and context\n",
    "    question_sim, answer_sim = compute_similarity(qa_pair, context)\n",
    "    \n",
    "    print(f\"Question Relevance (Cosine Similarity): {question_sim:.4f}\")\n",
    "    print(f\"Answer Relevance (Cosine Similarity): {answer_sim:.4f}\")\n",
    "    \n",
    "    # Compute BERTScore (Precision, Recall, F1 score)\n",
    "    F1q, F1a = compute_bertscore(qa_pair, context)\n",
    "    \n",
    "    print(f\"Question Relevance (BERTScore F1): {F1q}\")\n",
    "    print(f\"Answer Relevance (BERTScore F1): {F1a}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: How is the embedding matrix created? by adding two distinct embedding for each input\n",
      "Question Answer Relevance: 0.8068\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of the embedding matrix? learn a position embedding matrix\n",
      "Question Answer Relevance: 0.7979\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of position embedding? learn a position embedding matrix\n",
      "Question Answer Relevance: 0.7799\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: How are position embeddings learned? along with other parameters during training\n",
      "Question Answer Relevance: 0.7177\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of position embeddings? learn a position embedding matrix\n",
      "Question Answer Relevance: 0.7823\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the T5 model and tokenizer for generating QA pairs\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "# Load a model to compute embeddings for semantic similarity (e.g., BERT or RoBERTa)\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a function to get embeddings from BERT\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer_bert(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Take the mean of the token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Function to generate QA pairs using T5 model\n",
    "def generate_qa(context):\n",
    "    inputs = tokenizer_t5(context, return_tensors=\"pt\")\n",
    "    outputs = model_t5.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def compute_similarity(qa_pair, context):\n",
    "    # Check if the separator token exists and split accordingly\n",
    "    if tokenizer_t5.sep_token in qa_pair:\n",
    "        question, answer = qa_pair.split(tokenizer_t5.sep_token)\n",
    "    else:\n",
    "        # If the separator is not found, assume the whole text is the question (fallback case)\n",
    "        question = qa_pair\n",
    "        answer = \"\"  # Set answer to empty string, can be refined as needed\n",
    "\n",
    "    final_qa = question + answer\n",
    "    \n",
    "    # Get embeddings for the context, question, and answer\n",
    "    context_embedding = get_bert_embeddings(context)\n",
    "    qa_embedding = get_bert_embeddings(final_qa)\n",
    "    \n",
    "    # Compute cosine similarity between the context and question, context and answer\n",
    "    qa_similarity = cosine_similarity(context_embedding, qa_embedding)\n",
    "    \n",
    "    return qa_similarity[0][0]\n",
    "\n",
    "# Example context (use your own context text here)\n",
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')\n",
    "\n",
    "# Generate QA pairs and compute their relevance to the context\n",
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    qa_pair = generate_qa(context)\n",
    "    print(\"Generated QA Pair:\", qa_pair)\n",
    "    \n",
    "    # Compute similarity between the question, answer, and context\n",
    "    qa_sim = compute_similarity(qa_pair, context)\n",
    "    \n",
    "    print(f\"Question Answer Relevance: {qa_sim:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hf://datasets/soufyane/DATA_SCIENCE_QA/data (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context(question, answer):\n",
    "    input_text = f\"Question: {question} Answer: {answer}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_length=200, \n",
    "        min_length=50, \n",
    "        length_penalty=2.0, \n",
    "        num_beams=4, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    context = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"context\"] = df.apply(lambda row: generate_context(row[\"Question\"], row[\"Answer\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
