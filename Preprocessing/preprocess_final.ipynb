{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW_XUbStbeYF"
      },
      "outputs": [],
      "source": [
        "# !pip install PyPDF2\n",
        "# !pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade peft"
      ],
      "metadata": {
        "id": "uvVWZV_FdFu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88N2ujG7beYG"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "from pdfminer.layout import LAParams\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xoZytTrbeYG"
      },
      "outputs": [],
      "source": [
        "summarizer_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "finetuned_model_dir = \"/content/finetuned_model\"\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(finetuned_model_dir)\n",
        "qa_model = AutoModelForSeq2SeqLM.from_pretrained(finetuned_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW_RaKTMbeYH"
      },
      "outputs": [],
      "source": [
        "# pdfplumber\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    laparams = LAParams(line_margin=0.1)  # Adjust line margin to help with word separation\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text(x_tolerance=2, y_tolerance=3, laparams=laparams)  # Fine-tune tolerances\n",
        "    return text\n",
        "\n",
        "# pdf_path = 'D:/dell data/rutgers/nlp/slides/slide 10 - transformers.pdf'\n",
        "# pdf_text = extract_text_from_pdf(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2NcICISbeYH"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'Page \\d+|Header text|Footer text', '', text)\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjLcRBw5beYH"
      },
      "outputs": [],
      "source": [
        "def split_text_with_sentence_overlap(text, chunk_size=512):\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_length = len(sentence.split())\n",
        "\n",
        "        if current_length + sentence_length > chunk_size:\n",
        "            if current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [current_chunk[-1]]\n",
        "                current_length = len(current_chunk[0].split())\n",
        "\n",
        "        current_chunk.append(sentence)\n",
        "        current_length += sentence_length\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4ZMohUpbeYH"
      },
      "outputs": [],
      "source": [
        "def summarize_text(text):\n",
        "    inputs = summarizer_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    summary_ids = summarizer_model.generate(inputs['input_ids'], max_length=250, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C68HPpQbeYH"
      },
      "outputs": [],
      "source": [
        "def generate_qa(context):\n",
        "    inputs = qa_tokenizer(context, return_tensors=\"pt\")\n",
        "    outputs = qa_model.generate(\n",
        "        **inputs,\n",
        "        max_length=100,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return qa_tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbPa3E7obeYH"
      },
      "outputs": [],
      "source": [
        "# def get_bert_embeddings(text):\n",
        "#     inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "#     with torch.no_grad():\n",
        "#         outputs = bert_model(**inputs)\n",
        "#     return outputs.last_hidden_state.mean(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ybz_YUR3beYH"
      },
      "outputs": [],
      "source": [
        "# def compute_similarity(qa_pair, context):\n",
        "#     embeddings = [get_bert_embeddings(text) for text in [context, qa_pair]]\n",
        "#     return cosine_similarity(embeddings[0], embeddings[1])[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X82l0VyPbeYI"
      },
      "outputs": [],
      "source": [
        "pdf_path = '/content/10 (1).pdf'\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "cleaned_text = clean_text(pdf_text)\n",
        "sections = split_text_with_sentence_overlap(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3YiTLPebeYI",
        "outputId": "d51438ea-38be-44a4-bb37-3650479e136b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All rights reserved. Draft of August 20, 2024. CHAPTER 10 Large Language Models “How much do we know at any time? Much more, or so I believe, than we know we know.” Agatha Christie, The Moving Finger Fluent speakers of a language bring an enormous amount of knowledge to bear dur- ing comprehension and production. This knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage. This makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines. Estimates of the size of adult vocabularies vary widely both within and across languages. For example, estimates of the vocabulary size of young adult speakers of American English range from 30,000 to 100,000 depending on the resources used to make the estimate and the definition of what it means to know a word. What is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with caregivers and peers, usually well before the start of formal schooling. This active vocabulary (usually on the order of 2000 words for young speakers) is extremely limited compared to the size of the adult vocabulary, and is quite stable, with very few additional words learned via casual conversation beyond this early stage. Obvi- ously, this leaves a very large number of words to be acquired by other means. A simple consequence of these facts is that children have to learn about 7 to 10 wordsaday, everysingleday, toarriveatobservedvocabularylevelsbythetimethey are 20 years of age. And indeed empirical estimates of vocabulary growth in late el- ementary through high school are consistent with this rate. How do children achieve this rate of vocabulary growth? The bulk of this knowledge acquisition seems to happen as a by-product of reading, as part of the rich processing and reasoning that we perform when we read. Research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate. But the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner! Such facts have motivated the distributional hypothesis of Chapter 6, which sug- gests that aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words they co-occur with (and with the words that those words occur with). The distributional hypothe- sis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial acquisition. Of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful.',\n",
              " 'Of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful. pretraining In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remark-2 CHAPTER 10 • LARGE LANGUAGE MODELS able performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots. We’ll start by seeing how to apply the transformer of Chapter 9 to language modeling, in a setting often called causal or autoregressive language models, in which we iteratively predict words left-to-right from earlier words. We’ll first in- troduce training, seeing how language models are self-trained by iteratively being taught to guess the next word in the text from the prior words. We’ll then talk about the process of text generation. The application of LLMs to generate text has vastly broadened the scope of NLP,. Text generation, code- generation, and image-generation together constitute the important new area of gen- generativeAI erative AI. We’ll introduce specific algorithms for generating text from a language model, like greedy decoding and sampling. And we’ll see that almost any NLP task can be modeled as word prediction in a large language model, if we think about it in the right way. We’ll work through an example of using large language mod- els to solve one classic NLP task of summarization (generating a short text that summarizes some larger document). 10.1 Large Language Models with Transformers The prior chapter introduced most of the components of a transformer in the domain of language modeling: the transformer block including multi-head attention, the language modeling head, and the positional encoding of the input. In the following sections we’ll introduce the remaining aspects of the transformer LLM: sampling and training. Before we do that, we use this section to talk about why and how we apply transformer-based large language models to NLP tasks. conditional The tasks we will describe are all cases of conditional generation. Conditional generation generation is the task of generating text conditioned on an input piece of text. That is, we give the LLM an input piece of text, generally called a prompt, and then have the LLM continue generating text token by token, conditioned on the prompt. The fact that transformers have such long contexts (many thousands of tokens) makes them very powerful for conditional generation, because they can look back so far into the prompting text. Consider the simple task of text completion, illustrated in Fig. 10.1. Here a language model is given a text prefix and is asked to generate a possible completion.',\n",
              " 'Here a language model is given a text prefix and is asked to generate a possible completion. Note that as the generation process proceeds, the model has direct access to the priming context as well as to all of its own subsequently generated outputs (at least as much as fits in the large context window). This ability to incorporate the entirety of the earlier context and generated outputs at each time step is the key to the power of large language models built from transformers. So why should we care about predicting upcoming words or tokens? The in- sight of large language modeling is that many practical NLP tasks can be cast as word prediction, and that a powerful-enough language model can solve them with a high degree of accuracy. For example, we can cast sentiment analysis as language modeling by giving a language model a context like: The sentiment of the sentence ‘‘I like Jackie Chan\" is: and comparing the following conditional probability of the words “positive” and the10.1 • LARGE LANGUAGE MODELS WITH TRANSFORMERS 3 Completion Text all the Language Softmax Modeling Head Unencoder layer logits U U Transformer … … Blocks + i + i + i + i + i + i + i Encoder E E E E E E E So long and thanks for all the Prefix Text Figure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language models. As each token is generated, it gets added onto the context as a prefix for generating the next token. word “negative” to see which is higher: P(positive The sentiment of the sentence ‘‘I like Jackie Chan\" is:) | P(negative The sentiment of the sentence ‘‘I like Jackie Chan\" is:) | If the word “positive” is more probable, we say the sentiment of the sentence is positive, otherwise we say the sentiment is negative. We can also cast more complex tasks as word prediction. Consider question answering, in which the system is given a question (for example a question with a simple factual answer) and must give a textual answer; we introduce this task in detail in Chapter 14. We can cast the task of question answering as word prediction by giving a language model a question and a token like A: suggesting that an answer should come next: Q: Who wrote the book ‘‘The Origin of Species\"? A: If we ask a language model to compute the probability distribution over possible next words given this prefix: P(w Q: Who wrote the book ‘‘The Origin of Species\"? A:) | and look at which words w have high probabilities, we might expect to see that Charles is very likely, and then if we choose Charles and continue and ask P(w Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles) | we might now see that Darwin is the most probable token, and select it. Conditional generation can even be used to accomplish tasks that must generate text longer responses.',\n",
              " 'Conditional generation can even be used to accomplish tasks that must generate text longer responses. Consider the task of text summarization, which is to take a long summarization text, such as a full-length article, and produce an effective shorter summary of it. We can cast summarization as language modeling by giving a large language model a text, and follow the text by a token like tl;dr; this token is short for something like4 CHAPTER 10 • LARGE LANGUAGE MODELS ‘too long; didn’t read’ and in recent years people often use this token, especially in informal work emails, when they are going to give a short summary. Since this token is sufficiently frequent in language model training data, language models have seen many texts in which the token occurs before a summary, and hence will interpret the token as instructions to generate a summary. We can then do conditional generation: give the language model this prefix, and then have it generate the following words, one by one, and take the entire response as a summary. Fig. 10.2 shows an example of a text and a human-produced summary from a widely-used summarization corpus consisting of CNN and Daily Mirror news articles. Original Article The only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff and offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur Kyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough for 10 to 15 snowballs, he says. But not if you live in New England or surrounding states. “We will not ship snow to any states in the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging snow!” His website and social media accounts claim to have filled more than 133 orders for snow – more than 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a record this winter for the snowiest month in its history. Most residents see the huge piles of snow choking their yards and sidewalks as a nuisance, but Waring saw an opportunity. According to Boston.com, it all started a few weeks ago, when Waring and his wife were shov- eling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He joked about shipping the stuff to friends and family in warmer states, and an idea was born. [...] Summary Kyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough for 10 to 15 snowballs, he says. But not if you live in New England or surrounding states. Figure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus (Hermann et al., 2015), (Nallapati et al., 2016). Ifwetakethisfullarticleandappendthetokentl;dr, wecanusethisasthecon- text to prime the generation process to produce a summary as illustrated in Fig. 10.3.',\n",
              " '10.3. Again, what makes transformers able to succeed at this task (as compared, say, to the primitive n-gram language model) is that attention can incorporate information from the large context window, giving the model access to the original article as well as to the newly generated text throughout the process. Which words do we generate at each step? One simple way to generate words is to always generate the most likely word given the context. Generating the most greedy likely word given the context is called greedy decoding. A greedy algorithm is one decoding that make a choice that is locally optimal, whether or not it will turn out to have been the best choice with hindsight. Thus in greedy decoding, at each time step in generation, the output y t is chosen by computing the probability for each possible output (every word in the vocabulary) and then choosing the highest probability word (the argmax): wˆt = argmax w V P(w w <t) (10.1) ∈ | In practice, however, we don’t use greedy decoding with large language models. A major problem with greedy decoding is that because the words it chooses are (by definition) extremely predictable, the resulting text is generic and often quite repeti- tive. Indeed, greedy decoding is so predictable that it is deterministic; if the context10.2 • SAMPLING FOR LLM GENERATION 5 Generated Summary Kyle Waring will … LM Head U U U … E E E E E E E E The only … idea was born. tl;dr Kyle Waring will Original Story Delimiter Figure 10.3 Summarization with large language models using the tl;dr token and context-based autore- gressive generation. is identical, and the probabilistic model is the same, greedy decoding will always re- sult in generating exactly the same string. We’ll see in Chapter 13 that an extension to greedy decoding called beam search works well in tasks like machine translation, which are very constrained in that we are always generating a text in one language conditioned on a very specific text in another language. In most other tasks, how- ever, people prefer text which has been generated by more sophisticated methods, called sampling methods, that introduce a bit more diversity into the generations. We’ll see how to do that in the next few sections. 10.2 Sampling for LLM Generation The core of the generation process for large language models is the task of choosing the single word to generate next based on the context and based on the probabilities that the model assigns to possible words. This task of choosing a word to generate decoding based on the model’s probabilities is called decoding. Decoding from a language model in a left-to-right manner (or right-to-left for languages like Arabic in which we read from right to left), and thus repeatedly choosing the next word conditioned autoregressive on our previous choices is called autoregressive generation or causal LM genera- generation tion.1 (As we’ll see, alternatives like the masked language models of Chapter 11 are non-causal because they can predict words based on both past and future words).',\n",
              " 'Decoding from a language model in a left-to-right manner (or right-to-left for languages like Arabic in which we read from right to left), and thus repeatedly choosing the next word conditioned autoregressive on our previous choices is called autoregressive generation or causal LM genera- generation tion.1 (As we’ll see, alternatives like the masked language models of Chapter 11 are non-causal because they can predict words based on both past and future words). The most common method for decoding in large language models is sampling. sampling Recall from Chapter 3 that sampling from a model’s distribution over words means to choose random words according to their probability assigned by the model. That is, we iteratively choose a word to generate according to its probability in context 1 Technicallyanautoregressivemodelpredictsavalueattimet basedonalinearfunctionofthevalues at timest 1,t 2, and so on. Although language models are not linear (since they have many layers of − − non-linearities), we loosely refer to this generation technique as autoregressive since the word generated ateachtimestepisconditionedonthewordselectedbythenetworkfromthepreviousstep.6 CHAPTER 10 • LARGE LANGUAGE MODELS as defined by the model. Thus we are more likely to generate words that the model thinks have a high probability in the context and less likely to generate words that the model thinks have a low probability. We saw back in Chapter 3 on page ?? how to generate text from a unigram lan- guage model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. To generate text from a trained transformer language model we’ll just generalize this model a bit: at each step we’ll sample words according to their probability condi- tioned on our previous choices, and we’ll use a transformer language model as the probability model that tells us this probability. WecanformalizethisalgorithmforgeneratingasequenceofwordsW =w 1,w 2,...,w N until we hit the end-of-sequence token, using x p(x) to mean ‘choose x by sam- ∼ pling from the distribution p(x): i 1 ← w i p(w) ∼ while w i != EOS i i + 1 ← w i p(w i w <i) ∼ | random The algorithm above is called random sampling, and it turns out random sam- sampling pling doesn’t work well enough. The problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low- probability words in the tail of the distribution, and even though each one is low- probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences. For this reason, instead of random sampling, we usually use sampling methods that avoid generating the very unlikely words. The sampling methods we introduce below each have parameters that enable trading off two important factors in generation: quality and diversity. Methods that emphasize the most probable words tend to produce generations that are rated by people as more accurate, more coherent, and more factual, but also more boring and more repetitive.',\n",
              " 'Methods that emphasize the most probable words tend to produce generations that are rated by people as more accurate, more coherent, and more factual, but also more boring and more repetitive. Methods that give a bit more weight to the middle-probability words tend to be more creative and more diverse, but less factual and more likely to be incoherent or otherwise low-quality. 10.2.1 Top-k sampling top-ksampling Top-k sampling is a simple generalization of greedy decoding. Instead of choosing the single most probable word to generate, we first truncate the distribution to the top k most likely words, renormalize to produce a legitimate probability distribution, andthenrandomlysamplefromwithinthesek wordsaccordingtotheirrenormalized probabilities. More formally: 1. Choose in advance a number of words k 2. For each word in the vocabulary V, use the language model to compute the likelihood of this word given the context p(w t w <t) | 3. Sort the words by their likelihood, and throw away any word that is not one of the top k most probable words. 4. Renormalize the scores of the k words to be a legitimate probability distribu- tion.10.2 • SAMPLING FOR LLM GENERATION 7 5. Randomly sample a word from within these remaining k most-probable words according to its probability. When k = 1, top-k sampling is identical to greedy decoding. Setting k to a larger number than 1 leads us to sometimes select a word which is not necessarily the most probable, but is still probable enough, and whose choice results in generating more diverse but still high-enough-quality text. 10.2.2 Nucleus or top-p sampling One problem with top-k sampling is that k is fixed, but the shape of the probability distribution over words differs in different contexts. If we set k = 10, sometimes the top 10 words will be very likely and include most of the probability mass, but other times the probability distribution will be flatter and the top 10 words will only include a small part of the probability mass. top-psampling An alternative, called top-p sampling or nucleus sampling (Holtzman et al., 2020), is to keep not the top k words, but the top p percent of the probability mass. The goal is the same; to truncate the distribution to remove the very unlikely words. But by measuring probability rather than the number of words, the hope is that the measure will be more robust in very different contexts, dynamically increasing and decreasing the pool of word candidates. Given a distribution P(w t w <t), the top-p vocabulary V(p) is the smallest set of | words such that (cid:88) P(w w <t) p. (10.2) | ≥ w V(p) ∈ 10.2.3 Temperature sampling temperature In temperature sampling, we don’t truncate the distribution, but instead reshape sampling it. The intuition for temperature sampling comes from thermodynamics, where a system at a high temperature is very flexible and can explore many possible states, while a system at a lower temperature is likely to explore a subset of lower energy (better) states.',\n",
              " 'The intuition for temperature sampling comes from thermodynamics, where a system at a high temperature is very flexible and can explore many possible states, while a system at a lower temperature is likely to explore a subset of lower energy (better) states. In low-temperature sampling, we smoothly increase the probability of the most probable words and decrease the probability of the rare words. Weimplementthisintuitionbysimplydividingthelogitbyatemperatureparam- eter τ before we normalize it by passing it through the softmax. In low-temperature sampling, τ (0,1]. Thus instead of computing the probability distribution over the ∈ vocabulary directly from the logit as in the following (repeated from (??)): y = softmax(u) (10.3) we instead first divide the logits by τ, computing the probability vector y as y = softmax(u/τ) (10.4) Why does this work? When τ is close to 1 the distribution doesn’t change much. But the lower τ is, the larger the scores being passed to the softmax (dividing by a smaller fraction τ 1 results in making each score larger). Recall that one of the ≤ useful properties of a softmax is that it tends to push high values toward 1 and low values toward 0. Thus when larger numbers are passed to a softmax the result is a distribution with increased probabilities of the most high-probability words and decreased probabilities of the low probability words, making the distribution more greedy. As τ approaches 0 the probability of the most likely word approaches 1.8 CHAPTER 10 • LARGE LANGUAGE MODELS Note, by the way, that there can be other situations where we may want to do something quite different and flatten the word probability distribution instead of making it greedy. Temperature sampling can help with this situation too, in this case high-temperature sampling, in which case we use τ > 1. 10.3 Pretraining Large Language Models How do we teach a transformer to be a language model? What is the algorithm and what data do we train on? 10.3.1 Self-supervised training algorithm self-supervision To train a transformer as a language model, we use the same self-supervision (or self-training) algorithm we saw in Section ??: we take a corpus of text as training material and at each time step t ask the model to predict the next word. We call such a model self-supervised because we don’t have to add any special gold labels to the data; the natural sequence of words is its own supervision! We simply train the model to minimize the error in predicting the true next word in the training sequence, using cross-entropy as the loss function. Recall that the cross-entropy loss measures the difference between a predicted probability distribution and the correct distribution. (cid:88) L CE = y t[w]logyˆt[w] (10.5) − w V ∈ Inthecaseoflanguagemodeling, thecorrectdistributiony t comesfromknowingthe next word. This is represented as a one-hot vector corresponding to the vocabulary where the entry for the actual next word is 1, and all the other entries are 0.',\n",
              " 'This is represented as a one-hot vector corresponding to the vocabulary where the entry for the actual next word is 1, and all the other entries are 0. Thus, the cross-entropy loss for language modeling is determined by the probability the model assigns to the correct next word (all other words get multiplied by zero). So at time t the CE loss in (10.5) can be simplified as the negative log probability the model assigns to the next word in the training sequence. L CE(yˆt,y t) = logyˆt[w t+1] (10.6) − Thus at each word position t of the input, the model takes as input the correct se- quence of tokens w 1:t, and uses them to compute a probability distribution over possible next words so as to compute the model’s loss for the next token w t+1. Then we move to the next word, we ignore what the model predicted for the next word and instead use the correct sequence of tokens w 1:t+1 to estimate the probability of token w t+2. This idea that we always give the model the correct history sequence to predict the next word (rather than feeding the model its best case from the previous teacherforcing time step) is called teacher forcing. Fig. 10.4 illustrates the general training approach. At each step, given all the preceding words, the final transformer layer produces an output distribution over the entire vocabulary. During training, the probability assigned to the correct word is used to calculate the cross-entropy loss for each item in the sequence. The loss for a training sequence is the average cross-entropy loss over the entire sequence. The weights in the network are adjusted to minimize the average CE loss over the training sequence via gradient descent.10.3 • PRETRAINING LARGE LANGUAGE MODELS 9 Next token long and thanks for all … Loss \\x00<latexit sha1_base64=\"AovqpaL476UmJ1EU1xZPgDZ70tQ=\">AAAB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dzznDvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjTThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8CIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bdbDQfr+utu7KEKpzCGVyADTfQggdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA==</latexit> logy and \\x00<latexit sha1_base64=\"q3ZgXDyG7qtkT7t8hT47RdlwYG4=\">AAAB+XicbVDLSsNAFJ3UV62vWHe6GVsEN5bERXUlBUVcVrAPaEqYTCft0MlMmJkIIQT8AT/CTRE3Cv6Ev+DfmLTdtPXAwOGcM9x7jxcyqrRl/RqFtfWNza3idmlnd2//wDwst5WIJCYtLJiQXQ8pwignLU01I91QEhR4jHS88W3ud56JVFTwJx2HpB+gIac+xUhnkmseXzhMDGHsJk6A9EgGiR4hPlZpWnLNqlWzpoCrxJ6TauP0tXw3qdw0XfPHGQgcBYRrzJBSPdsKdT9BUlPMSFpyIkVChMdoSJLp5ik8y6QB9IXMHtdwqi7kUKBUHHhZMl9PLXu5+J/Xi7R/3U8oDyNNOJ4N8iMGtYB5DXBAJcGaxRlBWNJsQ4hHSCKss7Ly0+3lQ1dJ+7Jm12v1x6yDezBDEZyACjgHNrgCDfAAmqAFMHgBE/AJvozEeDPejY9ZtGDM/xyBBRjff79pldo=</latexit> logy thanks … = Language … Modeling logits logits logits logits logits U U U U U Head Stacked … … … … … … Transformer Blocks … x1 x2 x3 x4 x5 + 1 + 2 + 3 + 4 + 5 Input … Encoding E E E E E Input tokens So long and thanks for Figure 10.4 Training a transformer as a language model. Note the key difference between this figure and the earlier RNN-based version shown in Fig. ??. There the calculation of the outputs and the losses at each step was inherently serial given the recurrence in the calculation of the hidden states. With transformers, each training item can be processed in parallel since the output for each element in the sequence is computed separately. Large models are generally trained by filling the full context window (for exam- ple 4096 tokens for GPT4 or 8192 for Llama 3) with text. If documents are shorter than this, multiple documents are packed into the window with a special end-of-text token between them. The batch size for gradient descent is usually quite large (the largest GPT-3 model uses a batch size of 3.2 million tokens).',\n",
              " 'The batch size for gradient descent is usually quite large (the largest GPT-3 model uses a batch size of 3.2 million tokens). 10.3.2 Training corpora for large language models Large language models are mainly trained on text scraped from the web, augmented by more carefully curated data. Because these training corpora are so large, they are likely to contain many natural examples that can be helpful for NLP tasks, such as question and answer pairs (for example from FAQ lists), translations of sentences between various languages, documents together with their summaries, and so on. Web text is usually taken from corpora of automatically-crawled web pages like commoncrawl the common crawl, a series of snapshots of the entire web produced by the non- profit Common Crawl (https://commoncrawl.org/) that each have billions of webpages. Various versions of common crawl data exist, such as the Colossal Clean Crawled Corpus (C4; Raffel et al. 2020), a corpus of 156 billion tokens of English that is filtered in various ways (deduplicated, removing non-natural language like code, sentences with offensive words from a blocklist). This C4 corpus seems to consist in large part of patent text documents, Wikipedia, and news sites (Dodge et al., 2021). Wikipediaplaysaroleinlotsoflanguagemodeltraining, asdocorporaofbooks. ThePile The Pile (Gao et al., 2020) is an 825 GB English text corpus that is constructed by publicly released code, containing again a large amount of text scraped from the web10 CHAPTER 10 • LARGE LANGUAGE MODELS as well as books and Wikipedia; Fig. 10.5 shows its composition. Dolma is a larger open corpus of English, created with public tools, containing three trillion tokens, which similarly consists of web text, academic papers, code, books, encyclopedic materials, and social media (Soldaini et al., 2024). Figure 10.5 The PileFcigourrpeu1s:,TsreheomwapinogfPtihleecosmizpeonoenftsdbifyfeefrfeecnttivecosimzep. onents, color coded as academic (articles from PubMed and ArXiv, patents from the USPTA; internet (webtext in- cluding a subset of the common crawl as well as Wikipedia), prose (a large corpus of books), troduce a new filtered subset of Common Crawl, 1.1 Contributions dPiailleo-gCuCe,(winitchliumdpinrogvemdoevxitreacstuiobntiqtlueasliatyn.d chat data), and misc.. Figure from Gao et al. (2020). Thecorecontributionsofthispaperare: Through our analyses, we confirm that the Pile is 1. The introduction of a 825.18 GiB english- Fsiilgtneirfiicnangtlyfodrisqtinucatlfirtoymapnudre sCaofmemtyon CPrraewtlrainingldanagtuaagderadwatanseftrfoormlanthgueagweembodieslifingltceormed- fodartab.oAthdd qit uio an la il tly y, oaunrdev sa alu feat ti yon.sQshuoawlitthyatfitlhteers are cblianisnsgifi22erdsivtehrsaetsaosusrcigesn. a score to each existingGPT-2andGPT-3modelsperformpoorly d oo nc mum ane yn ct o. mpQ onu ea nl ti sty ofi ths eo Pf ilec ,o anu drs the ats mub odje elc stive, 2s.oThdeififnetrroednutctqiounaolfit1y4nfielwtelrasngauraegetrmaoidneeld- intradinifefdeorennthtewPailyess,igbnuifitcoanfttleynotuotpevrafolurme bhoitghh-qualitiyngrdeaftearseetns,cwehcicohrpwoereaxpliekcet tWo biekoipfeinddiea-, PII broaowkasn,dafinltderpedarCtiocmumlaornwCreabwslimteosdaelns.dTtoocaovmo-id webpseitnedsenwtiinttherleosttstoorfesPeaIrIch(ePrse.rsonal Iden- tipfileambelentIthnefoperrmfoarmtiaonnc)eeovralaudatuiolntsc,ownetaelnsot.peFr-ilters3.alEsvoalrueatmioonsvedebmooilnesrtrpaltiantge tseigxntifiwcahnitchimi-s formanexploratoryanalysisofthetextwithinthe very frequent on the web. Another kind of quaplirtoyvefimletentrsinacgroisssdmeadnyupdolimcaaitnisobny,GwPhTi-c2h- Pile to provide a detailed picture of the data. We sizedmodelstrainedonthisnewdataset,com- can be done at various levels, so as to remove duplicate documents, duplicate web hopethatourextensivedocumentationofthecon- paredtotrainingonCC-100andrawCommon psatgruecsti,onorandducphlaircaactteeritsetixcst.ofQthueaPliilteywfiillltehreilnpg generally improves language model per- Crawl. forersmeaarcnhceers(mLaokneginpforermeetdadle.c,i2si0on2s4abb;ouLtlpaomtean-Team, 2024). tialdownstreamapplications. 4. The investigation and documentation of this Safety filtering is again a subjective decision, and often includes toxicity detec- dataset,whichwehopewillbetterinformre- tiFoinnalblya,sweedmoankerpuunbnliicnlygavoafifla-bthleet-hsehperlefprtoocxesisc-ity clas ss ei afi rce hr es r.',\n",
              " 'The investigation and documentation of this Safety filtering is again a subjective decision, and often includes toxicity detec- dataset,whichwehopewillbetterinformre- tiFoinnalblya,sweedmoankerpuunbnliicnlygavoafifla-bthleet-hsehperlefprtoocxesisc-ity clas ss ei afi rce hr es r. sT abh oi us thc oa wn th oa uv see im tasix we ed llr ae ss mu olt tis -. OinngecopdreofbolrethmeciosnstthitautenctudrartaesnettstoofxthieciPtiylecanladssifiersvmateisthtaemketnoluyndfleartgakenosinm-itloarxiincvedstaigtaatiiofnsit isthegecondeerafotrecdonbsytruscptiengakaleterrsnaotifvemvienrsoiornitsy2. dInialects loifkteheAirofrwincadnataA. merican English (Xu the interest of reproducibility, we also document et al., 2021). Another problem is that models trained on toxicity-filtered data, while allprocessingperformedoneachdataset(andthe 2 The Pile Datasets soPimleeawsahwatholeles)sintoasxmicu,cahrdeeatalisloaswpoosrssibelea.tFdoretecting toxicity themselves (Longpre et al., ThePileiscomposedof22constituentsub-datasets, 2f0u2rt4hebr)d.etTaihlseasbeouitstshueepsromcesaskinegtohfeeaqchuedasttaisoent, of how to do better safety filtering an im- asshowninTable1. FollowingBrownetal.(2020), psoeretSaencttioonpe2nanpdrAobppleenmdi.xC. we increase the weights of higher quality compo- 2U hts ti pn sg :/l /a gr ig te hud ba .t ca os me /t Es ls ec utra hep re Ad I/from the wenbenttos,twraitihnclearntaginuahgigeh-mquoadliteylsdaptoasseetssseutchhicaasl atnhde-lepiglael questions: Wikipediabeingseenupto3times(“epochs”)for Copyright: Much of the text in these large datasets (like the collections of fic- 2 tion and non-fiction books) is copyrighted. In some countries, like the United States, the fair use doctrine may allow copyrighted content to be used for transformative uses, but it’s not clear if that remains true if the language mod- els are used to generate text that competes with the market for the text they10.3 • PRETRAINING LARGE LANGUAGE MODELS 11 are trained on (Henderson et al., 2023). Data consent: Owners of websites can indicate that they don’t want their sites to be crawled by web crawlers (either via a robots.txt file, or via Terms of Service). Recently there has been a sharp increase in the number of web- sites that have indicated that they don’t want large language model builders crawling their sites for training data (Longpre et al., 2024a). Because it’s not clear what legal status these indications have in different countries, or whether these restrictions are retroactive, what effect this will have on large pretraining datasets is unclear. Privacy: Large web datasets also have privacy issues since they contain private information like phone numbers and IP addresses. While filters are used to try to remove websites likely to contain large amounts of personal information, such filtering isn’t sufficient. 10.3.3 Finetuning Although the enormous pretraining data for a large language model includes text from many domains, it’s often the case that we want to apply it in a new domain or task that might not have appeared sufficiently in the pre-training data. For example, we might want a language model that’s specialized to legal or medical text. Or we might have a multilingual language model that knows many languages but might benefit from some more data in our particular language of interest. Or we want a language model that is specialized to a particular task. In such cases, we can simply continue training the model on relevant data from the new domain or language (Gururangan et al., 2020). This process of taking a fully pretrained model and running additional training passes on some new data is called finetuning finetuning. Fig. 10.6 sketches the paradigm.',\n",
              " '10.6 sketches the paradigm. Fine- Pretraining Data tuning Pretrained LM Data Fine-tuned LM … … … … … … Pretraining Fine-tuning Figure 10.6 Pretraining and finetuning. A pre-trained model can be finetuned to a par- ticular domain, dataset, or task. There are many different ways to finetune, depending on exactly which parameters are updated from the finetuning data: all the parameters, some of the parameters, or only the parameters of specific extra circuitry. We’ll introduce four related kinds of finetuning in this chapter and the two fol- lowing chapters. In all four cases, finetuning means the process of taking a pre- trained model and further adapting some or all of its parameters to some new data. But they differ on exactly which parameters get updated. In the first kind of finetuning we retrain all the parameters of the model on this new data, using the same method (word prediction) and loss function (cross-entropy loss) as for pretraining. In a sense it’s as if the new data were at the tail end of12 CHAPTER 10 • LARGE LANGUAGE MODELS the pretraining data, and so you’ll sometimes see this method called continued pre- continued training. pretraining Retraining all the parameters of the model is very slow and expensive when the freeze language model is huge. So instead we can freeze some of the parameters (i.e., leave them unchanged from their pretrained value) and train only a subset of parameters on the new data. In Section 10.5.3 we’ll describe this second variety of finetun- ing, called parameter-efficient finetuning, or PEFT. because we efficiently select specific parameters to update when finetuning, and leave the rest in their pretrained values. In Chapter 11 we’ll introduce a third kind of finetuning, also parameter-efficient. In this version, the goal is to use a language model as a kind of classifier or labeler for a specific task. For example we might train the model to be a sentiment classifier. We do this by adding extra neural circuitry (an extra head) after the top layer of the model. This classification head takes as input some of the top layer embeddings of the transformer and produces as output a classification. In this method, most com- monlyusedwithmaskedlanguagemodelslikeBERT,wefreezetheentirepretrained model and only train the classification head on some new data, usually labeled with some class that we want to predict. Finally, in Chapter 12 we’ll introduce a fourth kind of finetuning, that is a cru- cial component of the largest language models: supervised finetuning or SFT. SFT is often used for instruction finetuning, in which we want a pretrained language model to learn to follow text instructions, for example to answer questions or follow a command to write something. Here we create a dataset of prompts and desired responses (for example questions and their answers, or commands and their ful- fillments), and we train the language model using the normal cross-entropy loss to predict each token in the instruction prompt iteratively, essentially training it to pro- duce the desired response from the command in the prompt.',\n",
              " 'Here we create a dataset of prompts and desired responses (for example questions and their answers, or commands and their ful- fillments), and we train the language model using the normal cross-entropy loss to predict each token in the instruction prompt iteratively, essentially training it to pro- duce the desired response from the command in the prompt. It’s called supervised because unlike in pretraining, where we just take any data and predict the words in it, we build the special finetuning dataset by hand, creating supervised responses to each command. Ofteneverythingthathappensafterpretrainingislumpedtogetheraspost-training; we’ll discuss the various parts of post-training in Chapter 12. 10.4 Evaluating Large Language Models Perplexity As we first saw in Chapter 3, one way to evaluate language models is to measure how well they predict unseen text. Intuitively, good models are those that assign higher probabilities to unseen data (are less surprised when encountering the new words). perplexity We instantiate this intuition by using perplexity to measure the quality of a language model. Recall from page ?? that the perplexity of a model θ on an unseen test set is the inverse probability that θ assigns to the test set, normalized by the test set length. For a test set of n tokens w 1:n, the perplexity is 1 Perplexity θ(w 1:n) = P θ(w 1:n) −n (cid:115) 1 = n (10.7) P (w ) θ 1:n To visualize how perplexity can be computed as a function of the probabilities the10.4 • EVALUATING LARGE LANGUAGE MODELS 13 LM computes for each new word, we can use the chain rule to expand the computa- tion of probability of the test set: (cid:118) (cid:117) n (cid:117)(cid:89) 1 Perplexity θ(w 1:n) = (cid:116)n P (w w ) (10.8) θ i <i i=1 | Note that because of the inverse in Eq. 10.7, the higher the probability of the word sequence, the lower the perplexity. Thus the the lower the perplexity of a model on the data, the better the model. Minimizing perplexity is equivalent to maximizing the test set probability according to the language model. Onecaveat: becauseperplexitydependsonthelengthofatext, itisverysensitive to differences in the tokenization algorithm. That means that it’s hard to exactly compare perplexities produced by two language models if they have very different tokenizers. For this reason perplexity is best used when comparing language models that use the same tokenizer. Other factors While the predictive accuracy of a language model, as measured by perplexity, is a very useful metric, we also care about different kinds of accuracy, for the downstream tasks we apply our language model to. For each task like machine translation, summarization, question answering, speech recognition, and dialogue, we can measure the accuracy at those tasks. Future chapters will introduce task- specific metrics that allow us to evaluate how accuracy or correct language models are at these downstream tasks. But when evaluating models we also care about factors besides any of these kinds of accuracy (Dodge et al., 2019; Ethayarajh and Jurafsky, 2020).',\n",
              " 'But when evaluating models we also care about factors besides any of these kinds of accuracy (Dodge et al., 2019; Ethayarajh and Jurafsky, 2020). For example, we often care about how a big a model is, and how long it takes to train or do inference. This can matter because we have constraints on time either for training or at inference. Or we may have constraints on memory, since the GPUs we run our models on have fixed memory sizes. Big models also use more energy, and we prefer models that use less energy, both to reduce the environmental impact of the model and to reduce the financial cost of building or deploying it. We can target our evaluation to these factors by measuring performance normalized to a giving compute or memory budget. We can also directly measure the energy usage of our model in kWh or in kilograms of CO emitted (Strubell et al., 2019; Henderson 2 et al., 2020; Liang et al., 2023). Another feature that a language model evaluation can measure is fairness. We know that language models are biased, exhibiting gendered and racial stereotypes, or decreased performance for language from or about certain demographics groups. There are language model evaluation benchmarks that measure the strength of these biases, such as StereoSet (Nadeem et al., 2021), RealToxicityPrompts (Gehman et al., 2020), and BBQ (Parrish et al., 2022) among many others. We also want language models whose performance is equally fair to different groups. For exam- ple, we could chose an evaluation that is fair in a Rawlsian sense by maximizing the welfare of the worst-off group (Rawls, 2001; Hashimoto et al., 2018; Sagawa et al., 2020). Finally, there are many kinds of leaderboards like Dynabench (Kiela et al., 2021) and general evaluation protocols like HELM (Liang et al., 2023); we will return to these in later chapters when we introduce evaluation metrics for specific tasks like question answering and information retrieval.14 CHAPTER 10 • LARGE LANGUAGE MODELS 10.5 Dealing with Scale Large language models are large. For example the Llama 3.1 405B Instruct model fromMetahas405billionparameters(126layers, amodeldimensionalityof16,384, 128 attention heads) and was trained on 15.6 terabytes of text tokens (Llama Team, 2024), using a vocabulary of 128K tokens. So there is a lot of research on un- derstanding how LLMs scale, and especially how to implement them given limited resources. In the next few sections we discuss how to think about scale (the concept of scaling laws), and important techniques for getting language models to work efficiently, such as the KV cache and parameter-efficient fine tuning. 10.5.1 Scaling laws The performance of large language models has shown to be mainly determined by 3 factors: model size (the number of parameters not counting embeddings), dataset size (the amount of training data), and the amount of compute used for training. That is, we can improve a model by adding parameters (adding more layers or having wider contexts or both), by training on more data, or by training for more iterations.',\n",
              " 'That is, we can improve a model by adding parameters (adding more layers or having wider contexts or both), by training on more data, or by training for more iterations. The relationships between these factors and performance are known as scaling scalinglaws laws. Roughly speaking, the performance of a large language model (the loss) scales as a power-law with each of these three properties of model training. For example, Kaplan et al. (2020) found the following three relationships for loss L as a function of the number of non-embedding parameters N, the dataset size D, and the compute budget C, for models training with limited parameters, dataset, or compute budget, if in each case the other two properties are held constant: (cid:18) N (cid:19)αN c L(N) = (10.9) N (cid:18) D (cid:19)αD c L(D) = (10.10) D (cid:18) C (cid:19)αC c L(C) = (10.11) C The number of (non-embedding) parameters N can be roughly computed as fol- lows (ignoring biases, and with d as the input and output dimensionality of the model, d as the self-attention layer size, and d the size of the feedforward layer): attn ff N 2 d n (2 d +d ) layer attn ff ≈ 12 n layer d2 (10.12) ≈ (assuming d = d /4 = d) attn ff Thus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 96 × × 122882 175 billion parameters. ≈ The values of N c, D c, C c, α N, α D, and α C depend on the exact transformer architecture, tokenization, and vocabulary size, so rather than all the precise values, scaling laws focus on the relationship with loss.2 Scaling laws can be useful in deciding how to train a model to a particular per- formance, for example by looking at early in the training curve, or performance with 2 For the initial experiment in Kaplan et al. (2020) the precise values were αN = 0.076, Nc = 8.8 1013 (parameters),αD =0.095,Dc =5.4 1013 (tokens),αC =0.050,Cc =3.1 108 (petaflop-days). × × ×10.5 • DEALING WITH SCALE 15 smaller amounts of data, to predict what the loss would be if we were to add more data or increase model size. Other aspects of scaling laws can also tell us how much data we need to add when scaling up a model. 10.5.2 KV Cache We saw in Fig. ?? and in Eq. ?? (repeated below) how the attention vector can be very efficiently computed in parallel for training, via two matrix multiplications: (cid:18) (cid:124)(cid:19) QK A = softmax V (10.13) √d k Unfortunately we can’t do quite the same efficient computation in inference as in training. That’s because at inference time, we iteratively generate the next tokens one at a time. For a new token that we have just generated, call it x i, we need to compute its query, key, and values by multiplying by WQ, WK, and WV respec- tively.',\n",
              " 'For a new token that we have just generated, call it x i, we need to compute its query, key, and values by multiplying by WQ, WK, and WV respec- tively. But it would be a waste of computation time to recompute the key and value vectors for all the prior tokens x <i; at prior steps we already computed these key and value vectors! So instead of recomputing these, whenever we compute the key KVcache and value vectors we store them in memory in the KV cache, and then we can just grab them from the cache when we need them. Fig. 10.7 modifies Fig. ?? to show the computation that takes place for a single new token, showing which values we can take from the cache rather than recompute. q4 k1 k2 k4 Q QKT V A T K v1 x = x v2 = v3 d x N k q4•k1 q4•k2 q4•k3 q4•k4 v4 a4 1 x d k 1 x N N x d v 1 x d v k3 Figure 10.7 Parts of the attention computation (extracted from Fig. ??) showing, in black, the vectors that can be stored in the cache rather than recomputed when computing the atten- tion score for the 4th token. 10.5.3 Parameter Efficient Fine Tuning As we mentioned above, it’s very common to take a language model and give it more information about a new domain by finetuning it (continuing to train it to predict upcoming words) on some additional data. Fine-tuning can be very difficult with very large language models, because there are enormous numbers of parameters to train; each pass of batch gradient descent has to backpropagate through many many huge layers. This makes finetuning huge language models extremely expensive in processing power, in memory, and in time. For this reason, there are alternative methods that allow a model to be finetuned without changing all the parameters. Such methods are called parameter-efficient parameter- efficientfine finetuningorsometimesPEFT,becauseweefficientlyselectasubsetofparameters tuning PEFT to update when finetuning. For example we freeze some of the parameters (don’t change them), and only update some particular subset of parameters.16 CHAPTER 10 • LARGE LANGUAGE MODELS LoRA Here we describe one such model, called LoRA, for Low-Rank Adaptation. The intuition of LoRA is that transformers have many dense layers which perform matrix multiplication (for example the WQ, WK, WV, WO layers in the attention computa- tion). Instead of updating these layers during finetuning, with LoRA we freeze these layers and instead update a low-rank approximation that has fewer parameters. Consider a matrix W of dimensionality [N d] that needs to be updated during × finetuning via gradient descent. Normally this matrix would get updates ∆W of dimensionality [N d], for updating the N d parameters after gradient descent. In × × LoRA, we freeze W and update instead a low-rank decomposition of W. We create two matrices A and B, where A has size [N r] and B has size [r d], and we choose × × r to be quite small, r << min(d,N).',\n",
              " 'We create two matrices A and B, where A has size [N r] and B has size [r d], and we choose × × r to be quite small, r << min(d,N). During finetuning we update A and B instead of W. That is, we replace W+∆W with W+BA. Fig. 10.8 shows the intuition. For replacing the forward pass h = xW, the new forward pass is instead: h = xW+xAB (10.14) k h 1 k × r B Pretrained Weights d d A W k r x 1 d Figure 10.8 The intuition of LoRA. We freeze W to its pretrained values, and instead fine- tune by training a pair of matrices A and B, updating those instead of W, and just sum W and the updated AB. LoRA has a number of advantages. It dramatically reduces hardware require- ments, since gradients don’t have to be calculated for most parameters. The weight updates can be simply added in to the pretrained weights, since BA is the same size as W). That means it doesn’t add any time during inference. And it also means it’s possible to build LoRA modules for different domains and just swap them in and out by adding them in or subtracting them from W. In its original version LoRA was applied just to the matrices in the attention computation (the WQ, WK, WV, and WO layers). Many variants of LoRA exist.10.6 • POTENTIAL HARMS FROM LANGUAGE MODELS 17 10.6 Potential Harms from Language Models Large pretrained neural language models exhibit many of the potential harms dis- cussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are used for any downstream task, particularly those involving text generation, whether question answering, machine translation, or in assistive technologies like writing aids or web search query completion, or predic- tive typing for email (Olteanu et al., 2020). For example, language models are prone to saying things that are false, a prob- hallucination lem called hallucination. Language models are trained to generate text that is pre- dictable and coherent, but the training algorithms we have seen so far don’t have any way to enforce that the text that is generated is correct or true. This causes enormous problems for any application where the facts matter! We’ll return to this issue in Chapter 14 where we introduce proposed mitigation methods like retrieval augmented generation. toxiclanguage A second source of harm is that language models can generate toxic language. Gehman et al. (2020) show that even completely non-toxic prompts can lead large language models to output hate speech and abuse their users. Language models also generate stereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020; Sheng et al., 2019) about many demographic groups. One source of biases is the training data. Gehman et al. (2020) shows that large language model training datasets include toxic text scraped from banned sites. There are other biases than toxicity: the training data is disproportionately generated by authors from the US and from developed countries.',\n",
              " 'There are other biases than toxicity: the training data is disproportionately generated by authors from the US and from developed countries. Such biased population samples likely skew the resulting generation toward the perspectives or topics of this group alone. Furthermore, language models can amplify demographic and other biases in training data, just as we saw for embedding models in Chapter 6. Datasets can be another source of harms. We already saw in Section 10.3.2 that using pretraining corpora scraped from the web can lead to harms related to copyright and data consent. We also mentioned that pretraining data can tend to have private information like phone numbers and addresses. This is problematic because large language models can leak information from their training data. That is, an adversary can extract training-data text from a language model such as a per- son’s name, phone number, and address (Henderson et al. 2017, Carlini et al. 2021). This becomes even more problematic when large language models are trained on extremely sensitive private datasets such as electronic health records. Language models can also be used by malicious actors for generating text for misinformation, phishing, or other socially harmful activities (Brown et al., 2020). McGuffie and Newhouse (2020) show how large language models generate text that emulates online extremists, with the risk of amplifying extremist movements and their attempt to radicalize and recruit. Finding ways to mitigate all these harms is an important current research area in NLP. At the very least, carefully analyzing the data used to pretrain large language models is important as a way of understanding issues of toxicity, bias, privacy, and fair use, making it extremely important that language models include datasheets (page ??) or model cards (page ??) giving full replicable information on the cor- pora used to train them. Open-source models can specify their exact training data. Requirements that models are transparent in such ways is also in the process of being incorporated into the regulations of various national governments.18 CHAPTER 10 • LARGE LANGUAGE MODELS 10.7 Summary This chapter has introduced the large language model, and how it can be built out of the transformer. Here’s a summary of the main points that we covered: • Many NLP tasks—such as question answering, summarization, sentiment, and machine translation—can be cast as tasks of word prediction and hence addressed with Large language models. • Large language models are generally pretrained on large datasets of 100s of billions of words generally scraped from the web. • These datasets need to be filtered for quality and balanced for domains by upsampling and downsampling. Addressing some problems with pretraining data, like toxicity, are open research problems. • The choice of which word to generate in large language models is generally done by using a sampling algorithm. • Language models are evaluated by perplexity but there are also evaluations of accuracy downstream tasks, and ways to measure other factors like fairness and energy use. • There are various computational tricks for making large language models more efficient, such as the CV cache and parameter-efficient finetuning.',\n",
              " '• There are various computational tricks for making large language models more efficient, such as the CV cache and parameter-efficient finetuning. • Because of their ability to be used in so many ways, language models also have the potential to cause harms. Some harms include hallucinations, bias, stereotypes, misinformation and propaganda, and violations of privacy and copyright. Bibliographical and Historical Notes As we discussed in Chapter 3, the earliest language models were the n-gram lan- guage models developed (roughly simultaneously and independently) by Fred Je- linek and colleagues at the IBM Thomas J. Watson Research Center, and James Baker at CMU. It was the Jelinek and the IBM team who first coined the term lan- guage model to mean a model of the way any kind of linguistic property (grammar, semantics, discourse, speaker characteristics), influenced word sequence probabil- ities (Jelinek et al., 1975). They contrasted the language model with the acoustic model which captured acoustic/phonetic characteristics of phone sequences. N-gramlanguagemodelswereverywidelyusedoverthenext30yearsandmore, across a wide variety of NLP tasks like speech recognition and machine translations, often as one of multiple components of the model. The contexts for these n-gram models grew longer, with 5-gram models used quite commonly by very efficient LM toolkits (Stolcke, 2002; Heafield, 2011). The roots of the neural language model lie in multiple places. One was the application in the 1990s, again in Jelinek’s group at IBM Research, of discrimi- native classifiers to language models. Roni Rosenfeld in his dissertation (Rosen- feld, 1992) first applied logistic regression (under the name maximum entropy or maxent models) to language modeling in that IBM lab, and published a more fully formed version in Rosenfeld (1996). His model integrated various sorts of infor- mation in a logistic regression predictor, including n-gram information along withBIBLIOGRAPHICAL AND HISTORICAL NOTES 19 other features from the context, including distant n-grams and pairs of associated words called trigger pairs. Rosenfeld’s model prefigured modern language models by being a statistical word predictor trained in a self-supervised manner simply by learning to predict upcoming words in a corpus. Anotherwasthefirstuseofpretrainedembeddingstomodelwordmeaninginthe LSA/LSI models (Deerwester et al., 1988). Recall from the history section of Chap- ter 6 that in LSA (latent semantic analysis) a term-document matrix was trained on a corpus and then singular value decomposition was applied and the first 300 dimen- sions were used as a vector embedding to represent words. Landauer et al. (1997) first used the word “embedding”. In addition to their development of the idea of pre- training and of embeddings, the LSA community also developed ways to combine LSA embeddings with n-grams in an integrated language model (Bellegarda, 1997; Coccaro and Jurafsky, 1998). In a very influential series of papers developing the idea of neural language models, (Bengio et al. 2000; Bengio et al. 2003; Bengio et al. 2006), Yoshua Ben- gio and colleagues drew on the central ideas of both these lines of self-supervised language modeling work, (the discriminatively trained word predictor, and the pre- trained embeddings). Like the maxent models of Rosenfeld, Bengio’s model used the next word in running text as its supervision signal.',\n",
              " 'Like the maxent models of Rosenfeld, Bengio’s model used the next word in running text as its supervision signal. Like the LSA models, Ben- gio’s model learned an embedding, but unlike the LSA models did it as part of the process of language modeling. The Bengio et al. (2003) model was a neural lan- guage model: a neural network that learned to predict the next word from prior words, and did so via learning embeddings as part of the prediction process. The neural language model was extended in various ways over the years, perhaps most importantly in the form of the RNN language model of Mikolov et al. (2010) and Mikolov et al. (2011). The RNN language model was perhaps the first neural model that was accurate enough to surpass the performance of a traditional 5-gram language model. Soon afterwards, Mikolov et al. (2013a) and Mikolov et al. (2013b) proposed to simply the hidden layer of these neural net language models to create pretrained word2vec word embeddings. The static embedding models like LSA and word2vec instantiated a particular model of pretraining: a representation was trained on a pretraining dataset, and then the representations could be used in further tasks. The ‘Dai and Le (2015) and (Peters et al., 2018) reframed this idea by proposing models that were pretrained usingalanguagemodelobjective, andthentheidenticalmodelcouldbeeitherfrozen and directly applied for language modeling or further finetuned still using a language model objective. For example ELMo used a biLSTM self-supervised on a large pretrained dataset using a language model objective, then finetuned on a domain- specific dataset, and then froze the weights and added task-specific heads. Transformers were first applied as encoder-decoders (Vaswani et al., 2017) and then to masked language modeling (Devlin et al., 2019) (as we’ll see in Chapter 13 and Chapter 11). Radford et al. (2019) then showed that the transformer-based au- toregressive language model GPT2 could perform zero-shot on many NLP tasks like summarization and question answering. The technology used for transformer-based language models can also be applied to other domains and tasks, like vision, speech, and genetics. the term foundation foundation model is sometimes used as a more general term for this use of large language model model technology across domains and areas, when the elements we are computing over are not necessarily words. Bommasani et al. (2021) is a broad survey that20 CHAPTER 10 • LARGE LANGUAGE MODELS sketches the opportunities and risks of foundation models, with special attention to large language models.Bibliographical and Historical Notes 21 Bellegarda,J.R.1997.Alatentsemanticanalysisframework Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019. forlarge-spanlanguagemodeling. EUROSPEECH. BERT:Pre-trainingofdeepbidirectionaltransformersfor Bengio, Y., R. Ducharme, and P. Vincent. 2000. A neural languageunderstanding. NAACLHLT. probabilisticlanguagemodel. NeurIPS. Dodge, J., S. Gururangan, D. Card, R. Schwartz, and N. A. Bengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003. Smith. 2019. Show your work: Improved reporting of A neural probabilistic language model. JMLR, 3:1137– experimentalresults. EMNLP. 1155. Dodge, J., M. Sap, A. Marasovic´, W. Agnew, G. Ilharco, Bengio, Y., H. Schwenk, J.-S. Sene´cal, F. Morin, and J.-L. D.Groeneveld,M.Mitchell,andM.Gardner.2021. Doc- Gauvain.2006. Neuralprobabilisticlanguagemodels.',\n",
              " 'Neuralprobabilisticlanguagemodels. In umenting large webtext corpora: A case study on the InnovationsinMachineLearning,137–186.Springer. colossalcleancrawledcorpus. EMNLP. Bommasani, R., D. A. Hudson, E. Adeli, R. Altman, Ethayarajh,K.andD.Jurafsky.2020. Utilityisintheeyeof S.Arora,S.vonArx,M.S.Bernstein,J.Bohg,A.Bosse- theuser: AcritiqueofNLPleaderboards. EMNLP. lut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, Gao, L., T. Hoppe, A. Thite, S. Biderman, C. Foster, R. Castellon, N. S. Chatterji, A. S. Chen, K. A. Creel, N.Nabeshima,S.Black,J.Phang,S.Presser,L.Golding, J. Davis, D. Demszky, C. Donahue, M. Doumbouya, H. He, and C. Leahy. 2020. The Pile: An 800GB dataset E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, ofdiversetextforlanguagemodeling. ArXivpreprint. L. Fei-Fei, C. Finn, T. Gale, L. E. Gillespie, K. Goel, Gehman, S., S. Gururangan, M. Sap, Y. Choi, and N. A. N. D. Goodman, S. Grossman, N. Guha, T. Hashimoto, Smith. 2020. RealToxicityPrompts: Evaluating neu- P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, ral toxic degeneration in language models. Findings of J. Huang, T. F. Icard, S. Jain, D. Jurafsky, P. Kalluri, EMNLP. S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. S. Krass, R. Krishna, R. Kuditipudi, A. Ku- Gururangan, S., A. Marasovic´, S. Swayamdipta, K. Lo, mar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Lev- I. Beltagy, D. Downey, and N. A. Smith. 2020. Don’t ent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, stop pretraining: Adapt language models to domains and S. P. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, tasks. ACL. A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Hashimoto, T., M. Srivastava, H. Namkoong, and P. Liang. Niebles, H. Nilforoshan, J. F. Nyarko, G. Ogut, L. Orr, 2018. Fairness without demographics in repeated loss I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, minimization. ICML. C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Heafield, K. 2011. KenLM: Faster and smaller language Y. H. Roohani, C. Ruiz, J. Ryan, C. R’e, D. Sadigh, model queries. Workshop on Statistical Machine Trans- S. Sagawa, K. Santhanam, A. Shih, K. P. Srinivasan, lation. A. Tamkin, R. Taori, A. W. Thomas, F. Trame`r, R. E. Wang,W.Wang,B.Wu,J.Wu,Y.Wu,S.M.Xie,M.Ya- Henderson, P., J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, sunaga, J. You, M. A. Zaharia, M. Zhang, T. Zhang, and J. Pineau. 2020. Towards the systematic reporting X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang. of the energy and carbon footprints of machine learning. 2021. On the opportunities and risks of foundation mod- JournalofMachineLearningResearch,21(248):1–43. els. ArXiv. Henderson,P.,X.Li,D.Jurafsky,T.Hashimoto,M.A.Lem- Brown, T., B. Mann, N. Ryder, M. Subbiah, J. Kaplan, ley, and P. Liang. 2023. Foundation models and fair use. P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, JMLR,24(400):1–79. A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, Henderson, P., K. Sinha, N. Angelard-Gontier, N. R. Ke, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, G. Fried, R. Lowe, and J. Pineau. 2017. Ethical chal- C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, lenges in data-driven dialogue systems. AAAI/ACM AI S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, EthicsandSocietyConference. A.Radford,I.Sutskever,andD.Amodei.2020.Language modelsarefew-shotlearners. NeurIPS,volume33. Hermann, K. M., T. Kocˇisky´, E. Grefenstette, L. Espeholt, W.',\n",
              " 'Espeholt, W. Kay, M. Suleyman, and P. Blunsom. 2015. Teaching Carlini, N., F. Tramer, E. Wallace, M. Jagielski, A. Herbert- machinestoreadandcomprehend. NeurIPS. Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Er- lingsson, et al. 2021. Extracting training data from large Holtzman,A.,J.Buys,L.Du,M.Forbes,andY.Choi.2020. language models. 30th USENIX Security Symposium Thecuriouscaseofneuraltextdegeneration. ICLR. (USENIXSecurity21). Jelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a Cheng, M., E. Durmus, and D. Jurafsky. 2023. Marked per- linguisticstatisticaldecoderfortherecognitionofcontin- sonas: Usingnaturallanguagepromptstomeasurestereo- uous speech. IEEE Transactions on Information Theory, typesinlanguagemodels. ACL. IT-21(3):250–256. Coccaro, N. and D. Jurafsky. 1998. Towards better integra- Kaplan, J., S. McCandlish, T. Henighan, T. B. Brown, tion of semantic predictors in statistical language model- B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and ing. ICSLP. D.Amodei.2020. Scalinglawsforneurallanguagemod- els. ArXivpreprint. Dai, A. M. and Q. V. Le. 2015. Semi-supervised sequence learning. NeurIPS. Kiela,D.,M.Bartolo,Y.Nie,D.Kaushik,A.Geiger,Z.Wu, Deerwester,S.C.,S.T.Dumais,G.W.Furnas,R.A.Harsh- B. Vidgen, G. Prasad, A. Singh, P. Ringshia, et al. 2021. man, T. K. Landauer, K. E. Lochbaum, and L. Streeter. Dynabench: Rethinking benchmarking in nlp. NAACL 1988. Computerinformationretrievalusinglatentseman- HLT. ticstructure: USPatent4,839,853.22 Chapter 10 • Large Language Models Landauer, T. K., D. Laham, B. Rehder, and M. E. Schreiner. Raffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang, 1997. How well can passage meaning be derived with- M.Matena,Y.Zhou,W.Li,andP.J.Liu.2020.Exploring out using word order? A comparison of Latent Semantic the limits of transfer learning with a unified text-to-text Analysisandhumans. COGSCI. transformer. JMLR,21(140):1–67. Liang, P., R. Bommasani, T. Lee, D. Tsipras, D. Soylu, Rawls, J. 2001. Justice as fairness: A restatement. Harvard M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Ku- UniversityPress. mar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cos- Rosenfeld, R. 1992. Adaptive Statistical Language Mod- grove, C. D. Manning, C. Re´, D. Acosta-Navas, D. A. eling: A Maximum Entropy Approach. Ph.D. thesis, Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, CarnegieMellonUniversity. H.Ren,H.Yao,J.Wang,K.Santhanam,L.Orr,L.Zheng, Rosenfeld, R. 1996. A maximum entropy approach to adap- M.Yuksekgonul,M.Suzgun,N.Kim,N.Guha,N.Chat- tive statistical language modeling. Computer Speech and terji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. Language,10:187–228. Xie, S. Santurkar, S. Ganguli, T. Hashimoto, T. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, Sagawa,S.,P.W.Koh,T.B.Hashimoto,andP.Liang.2020. Y. Zhang, and Y. Koreeda. 2023. Holistic evaluation of Distributionally robust neural networks for group shifts: languagemodels. TransactionsonMachineLearningRe- Ontheimportanceofregularizationforworst-casegener- search. alization. ICLR. LlamaTeam.2024. Thellama3herdofmodels. Sheng, E., K.-W. Chang, P. Natarajan, and N. Peng. 2019. Longpre, S., R. Mahari, A. Lee, C. Lund, H. Oderinwale, Thewomanworkedasababysitter: Onbiasesinlanguage W. Brannon, N. Saxena, N. Obeng-Marnu, T. South, generation. EMNLP. C. Hunter, et al. 2024a. Consent in crisis: The rapid de- Soldaini, L., R. Kinney, A. Bhagia, D. Schwenk, D. Atkin- clineoftheaidatacommons. ArXivpreprint. son, R. Authur, B. Bogin, K. Chandu, J. Dumas, Longpre, S., G. Yauney, E. Reif, K. Lee, A. Roberts, Y. Elazar, V. Hofmann, A. H. Jha, S. Kumar, L. Lucy, B. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno, and X. Lyu, N. Lambert, I. Magnusson, J. Morrison, D. Ippolito. 2024b. A pretrainer’s guide to training data: N. Muennighoff, A. Naik, C. Nam, M. E. Peters, Measuringtheeffectsofdataage,domaincoverage,qual- A. Ravichander, K.',\n",
              " 'Ravichander, K. Richardson, Z. Shen, E. Strubell, ity,&toxicity. NAACLHLT. N.Subramani,O.Tafjord,P.Walsh,L.Zettlemoyer,N.A. Smith,H.Hajishirzi,I.Beltagy,D.Groeneveld,J.Dodge, McGuffie, K. and A. Newhouse. 2020. The radicalization andK.Lo.2024. Dolma: Anopencorpusofthreetrillion risks of GPT-3 and advanced neural language models. tokens for language model pretraining research. ArXiv ArXivpreprintarXiv:2009.06807. preprint. Mikolov,T.,K.Chen,G.S.Corrado,andJ.Dean.2013a. Ef- Stolcke,A.2002.SRILM–anextensiblelanguagemodeling ficientestimationofwordrepresentationsinvectorspace. toolkit. ICSLP. ICLR2013. Mikolov, T., M. Karafia´t, L. Burget, J. Cˇernocky`, and Strubell, E., A. Ganesh, and A. McCallum. 2019. Energy andpolicyconsiderationsfordeeplearninginNLP. ACL. S.Khudanpur.2010. Recurrentneuralnetworkbasedlan- guagemodel. INTERSPEECH. Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, Mikolov, T., S. Kombrink, L. Burget, J. H. Cˇernocky`, and A. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Atten- tionisallyouneed. NeurIPS. S. Khudanpur. 2011. Extensions of recurrent neural net- worklanguagemodel. ICASSP. Xu, A., E. Pathak, E. Wallace, S. Gururangan, M. Sap, Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and and D. Klein. 2021. Detoxifying language models risks J.Dean.2013b. Distributedrepresentationsofwordsand marginalizingminorityvoices. NAACLHLT. phrasesandtheircompositionality. NeurIPS. Nadeem, M., A. Bethke, and S. Reddy. 2021. StereoSet: Measuringstereotypicalbiasinpretrainedlanguagemod- els. ACL. Nallapati, R., B. Zhou, C. dos Santos, C¸. Gulc¸ehre, and B. Xiang. 2016. Abstractive text summarization using sequence-to-sequenceRNNsandbeyond. CoNLL. Olteanu, A., F. Diaz, and G. Kazai. 2020. When are search completionsuggestionsproblematic? CSCW. Parrish, A., A. Chen, N. Nangia, V. Padmakumar, J. Phang, J.Thompson,P.M.Htut,andS.Bowman.2022. BBQ:A hand-builtbiasbenchmarkforquestionanswering. Find- ingsofACL2022. Peters, M., M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. 2018. Deep contextualized wordrepresentations. NAACLHLT. Radford, A., J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. 2019. Language models are unsupervised multitasklearners. OpenAItechreport.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPviqbohbeYI"
      },
      "outputs": [],
      "source": [
        "qa_results = []\n",
        "for section in sections:\n",
        "    summarized_section = summarize_text(section)\n",
        "    qa_pair = generate_qa(summarized_section)\n",
        "    # similarity = compute_similarity(qa_pair, summarized_section)\n",
        "    qa_results.append({\"summary\": summarized_section, \"qa_pair\": qa_pair\n",
        "                      #  , \"similarity\": similarity\n",
        "                       })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JolCcCXRbeYI",
        "outputId": "ade6ea35-6de7-4514-e593-9f7f5b396fd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QA Pair: What is the term for the process of producing speech and language? speech and language processing\n",
            "--------------------------------------------------\n",
            "QA Pair: The application of LLMs to generate text has vastly broadened the scope of what? NLP\n",
            "--------------------------------------------------\n",
            "QA Pair: Conditional generation can even be used to accomplish tasks that must generate what? text longer responses\n",
            "--------------------------------------------------\n",
            "QA Pair: What can be used to accomplish tasks that must generate text longer responses? conditional generation\n",
            "--------------------------------------------------\n",
            "QA Pair: What is the core of the generation process for large language models? task of choosing the single word to generate next based on the context\n",
            "--------------------------------------------------\n",
            "QA Pair: What is the most common method for decoding in large language models? sampling\n",
            "--------------------------------------------------\n",
            "QA Pair: Methods that emphasize the most probable words tend to produce generations that are rated by people as more accurate, more coherent, and more what? factual\n",
            "--------------------------------------------------\n",
            "QA Pair: The intuition for temperature sampling comes from what? thermodynamics\n",
            "--------------------------------------------------\n",
            "QA Pair: What is the probability assigned to the correct word used to calculate? cross-entropy loss\n",
            "--------------------------------------------------\n",
            "QA Pair: What is the batch size for gradient descent? large\n",
            "--------------------------------------------------\n",
            "QA Pair: What kind of decision is safety filtering? subjective\n",
            "--------------------------------------------------\n",
            "QA Pair: What is the term for finetuning a model to a specific parameter? finetuning\n",
            "--------------------------------------------------\n",
            "QA Pair: What is the term for pretraining? supervised\n",
            "--------------------------------------------------\n",
            "QA Pair: What do big models use more of? energy\n",
            "--------------------------------------------------\n",
            "QA Pair: The performance of a large language model (the loss) scales as a power-law with each of these three properties of what? model training\n",
            "--------------------------------------------------\n",
            "QA Pair: Where do we store the key and value vectors? the cache\n",
            "--------------------------------------------------\n",
            "QA Pair: Large pretrained neural language models exhibit many of the potential harms dis- cussed in Chapter 4 and chapter 6.\n",
            "--------------------------------------------------\n",
            "QA Pair: Language models can be used by malicious actors for generating text for misinformation, phishing, or other socially harmful activities. language models\n",
            "--------------------------------------------------\n",
            "QA Pair: The earliest language models were the n-gram lan- guage models developed by what group? Fred Je- linek and colleagues\n",
            "--------------------------------------------------\n",
            "QA Pair: The technology used for transformer-based language models can also be applied to other domains and tasks. technology\n",
            "--------------------------------------------------\n",
            "QA Pair: What is the name of the framework that aims to improve language models for different tasks? neuralprobabilisticlanguagemodels\n",
            "--------------------------------------------------\n",
            "QA Pair: What do you call the 'marked per- linguisticstatisticaldecoder'? statisticalaldecoder\n",
            "--------------------------------------------------\n",
            "QA Pair: What kind of risks of GPT-3 and advanced neural language models have been studied? radicalization and toxicity\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for result in qa_results:\n",
        "    # print(f\"Summary: {result['summary']}\")\n",
        "    print(f\"QA Pair: {result['qa_pair']}\")\n",
        "    # print(f\"Relevance Score: {result['similarity']:.4f}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZr7Z00XbeYI",
        "outputId": "ee611819-80a8-4408-d8d3-8575e51074b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "qa_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u3tKumdbeYI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"hf://datasets/soufyane/DATA_SCIENCE_QA/data (1).csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smmjZ7IqbeYI",
        "outputId": "095d0c42-f2f5-43ca-c90d-99c8cd2f9047"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What is under-fitting and overfitting in machi...</td>\n",
              "      <td>Underfitting is when a model is too simple, an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Can you explain what a false positive and a fa...</td>\n",
              "      <td>A false positive incorrectly indicates a condi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Clarify the concept of Phase IV.</td>\n",
              "      <td>Phase IV studies, also known as post-marketing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>What is semi-supervised learning described in ...</td>\n",
              "      <td>Semi-supervised learning integrates both label...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Discuss the parallelization of training in gra...</td>\n",
              "      <td>Parallelizing training of a gradient boosting ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1065</th>\n",
              "      <td>1065</td>\n",
              "      <td>Define the ACID property in SQL and its signif...</td>\n",
              "      <td>ACID principles maintain database integrity by...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1066</th>\n",
              "      <td>1066</td>\n",
              "      <td>What are the different types of data warehouses?</td>\n",
              "      <td>Data warehouses vary by scope and function, wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1067</th>\n",
              "      <td>1067</td>\n",
              "      <td>What are the key stages in a data mining project?</td>\n",
              "      <td>A data mining project starts with understandin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1068</th>\n",
              "      <td>1068</td>\n",
              "      <td>What is information extraction?</td>\n",
              "      <td>Information extraction systematically identifi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1069</th>\n",
              "      <td>1069</td>\n",
              "      <td>Describe kernel support vector machines (KSVMs).</td>\n",
              "      <td>Kernel Support Vector Machines (KSVMs) are a c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1070 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0                                           Question  \\\n",
              "0              0  What is under-fitting and overfitting in machi...   \n",
              "1              1  Can you explain what a false positive and a fa...   \n",
              "2              2                   Clarify the concept of Phase IV.   \n",
              "3              3  What is semi-supervised learning described in ...   \n",
              "4              4  Discuss the parallelization of training in gra...   \n",
              "...          ...                                                ...   \n",
              "1065        1065  Define the ACID property in SQL and its signif...   \n",
              "1066        1066   What are the different types of data warehouses?   \n",
              "1067        1067  What are the key stages in a data mining project?   \n",
              "1068        1068                    What is information extraction?   \n",
              "1069        1069   Describe kernel support vector machines (KSVMs).   \n",
              "\n",
              "                                                 Answer  \n",
              "0     Underfitting is when a model is too simple, an...  \n",
              "1     A false positive incorrectly indicates a condi...  \n",
              "2     Phase IV studies, also known as post-marketing...  \n",
              "3     Semi-supervised learning integrates both label...  \n",
              "4     Parallelizing training of a gradient boosting ...  \n",
              "...                                                 ...  \n",
              "1065  ACID principles maintain database integrity by...  \n",
              "1066  Data warehouses vary by scope and function, wi...  \n",
              "1067  A data mining project starts with understandin...  \n",
              "1068  Information extraction systematically identifi...  \n",
              "1069  Kernel Support Vector Machines (KSVMs) are a c...  \n",
              "\n",
              "[1070 rows x 3 columns]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b9decf816fe1486d85d0fce7d04809a4",
            "cfc2d9dc1e9d435088a59615f320058c",
            "62ac851a53194c9a8cdc177e94061910",
            "9f507e28b2934cc0a46a5269b4e757d6"
          ]
        },
        "id": "xt5m8hJObeYI",
        "outputId": "682e24c9-8c5d-4f01-9935-7401061ba893"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9decf816fe1486d85d0fce7d04809a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfc2d9dc1e9d435088a59615f320058c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62ac851a53194c9a8cdc177e94061910",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f507e28b2934cc0a46a5269b4e757d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "text = \"Scikit-learn supports K-means clustering.\"\n",
        "print(ner_pipeline(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXoAk-PMbeYI",
        "outputId": "dabdf104-4030-4af3-b7f7-fa906bb585c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "text = \"Scikit-learn supports K-means clustering.\"\n",
        "print(ner_pipeline(text))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}