{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PyPDF2\n",
    "# pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback BaseAsyncIOLoop._handle_events(1484, 1)\n",
      "handle: <Handle BaseAsyncIOLoop._handle_events(1484, 1)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\jupyter_client\\session.py\", line 100, in json_packer\n",
      "    ).encode(\"utf8\", errors=\"surrogateescape\")\n",
      "      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode character '\\ud835' in position 12261: surrogates not allowed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Dell\\miniconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 202, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 611, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 640, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 561, in _run_callback\n",
      "    f = callback(*args, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\ipykernel\\iostream.py\", line 170, in _handle_event\n",
      "    event_f()\n",
      "  File \"c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\ipykernel\\iostream.py\", line 649, in _flush\n",
      "    self.session.send(\n",
      "  File \"c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\jupyter_client\\session.py\", line 852, in send\n",
      "    to_send = self.serialize(msg, ident)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\jupyter_client\\session.py\", line 721, in serialize\n",
      "    content = self.pack(content)\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\jupyter_client\\session.py\", line 108, in json_packer\n",
      "    ).encode(\"utf8\", errors=\"surrogateescape\")\n",
      "      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode character '\\ud835' in position 12261: surrogates not allowed\n"
     ]
    }
   ],
   "source": [
    "# PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = 'D:/dell data/rutgers/nlp/slides/slide 10 - transformers.pdf'\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction to Transformers\n",
      "TransformersELMo – Embeddings from Language Models\n",
      "Deep contextualized word representations, Peters et. al. (2018)\n",
      "Word vectors as learned functions of internal states of a deep bidirectional language\n",
      "model (biLM) (pretrained on a large text corpus)\n",
      "Representations can be added to existing models\n",
      "Improve performance across six challenging NLP problems (question answering, textual\n",
      "entailment, sentiment analysis, …)\n",
      "Exposing the deep internals of pre-trained model allows downstream models to mix\n",
      "different types of semi-supervision signals.ELMo\n",
      "2 layer biLSTM\n",
      "ELMo representations are a function of all internal layers of biLM.\n",
      "Linear combination of the vectors stacked above each input word\n",
      "Improves performance over just using the top LSTM layerTransfer Learning\n",
      "Natural Language Processing with Deep Learning CS224N/Ling284, Chris Manning, StanfordULMfit\n",
      "Universal Language Model Fine-tuning for Text\n",
      "Train LM on big general domain corpus (use biLM)\n",
      "Classification (ULMFiT)\n",
      "Tune LM on target task data\n",
      "Fine-tune as classifier on target task\n",
      "Howard and Ruder (2018)\n",
      "29Transfer Learning in ULMFiT\n",
      "IMDB\n",
      "TREC-6 AGA very approximate timeline\n",
      "1990 Static Word Embeddings\n",
      "2003 Neural Language Model\n",
      "2008 Multi-Task Learning\n",
      "2015 Attention\n",
      "2017 Transformer\n",
      "2018 Contextual Word Embeddings and Pretraining\n",
      "2019 PromptingLLMs are built out of transformers\n",
      "Transformer: a specific kind of network architecture, like a\n",
      "fancier feedforward network, but based on attentionAttention\n",
      "TransformersRepresentations for a Word\n",
      "Neural Network for NLP Task (LM,\n",
      "Classifcation)\n",
      "Random word vectors\n",
      "Word Embeddings\n",
      "Word2Vec, Glove, fastText\n",
      "Just one representation for a word type regardless of the context in\n",
      "which a word token occurs\n",
      "◦ Open account in bank vs river bank\n",
      "◦ Star in the sky vs. Movie star\n",
      "Words have different aspects, including semantics, syntactic behavior,\n",
      "connotations\n",
      "◦ E.g arrive vs arrivalHave we handled context\n",
      "for words before ?\n",
      "RNNs, LSTMs Language\n",
      "Models are producing\n",
      "context-specific word\n",
      "representations at each\n",
      "position for the training data.\n",
      "Can we learn the meaning of a\n",
      "word in context in a standard\n",
      "way?Instead of starting with the big Transformer picture\n",
      "Let's consider the embeddings for an individual word from a particular layer\n",
      "Next token long and thanks for all\n",
      "Next token long and thanks for all\n",
      "Language\n",
      "…\n",
      "logits logits logits logits logits\n",
      "Modeling\n",
      "Language\n",
      "U U U U U …\n",
      "MHodeealding logits logits logits logits logits\n",
      "U U U U U\n",
      "Head\n",
      "… … … … …\n",
      "… … … … …\n",
      "Stacked\n",
      "Stacked\n",
      "…\n",
      "Transformer …\n",
      "Transformer\n",
      "BBlloocckkss\n",
      "…\n",
      "x1 x2 x3 x4 x5 …\n",
      "x1 x2 x3 x4 x5\n",
      "+ 1 + 2 + 3 + 4 + 5\n",
      "Input\n",
      "…\n",
      "+ 1 + 2 + 3 + 4 + 5\n",
      "EnIncpoduitng E E E E E …\n",
      "Encoding E E E E E\n",
      "Input tokens So long and thanks for\n",
      "Input tokens So long and thanks forProblem with static embeddings (word2vec)\n",
      "They are static! The embedding for a word doesn't reflect how its\n",
      "meaning changes in context.\n",
      "The chicken didn't cross the road because it was too tired\n",
      "What is the meaning represented in the static embedding for \"it\"?Contextual Embeddings\n",
      "• Intuition: a representation of meaning of a word\n",
      "should be different in different contexts!\n",
      "• Contextual Embedding: each word has a different\n",
      "vector that expresses different meanings\n",
      "depending on the surrounding words\n",
      "• How to compute contextual embeddings?\n",
      "• AttentionContextual Embeddings\n",
      "The chicken didn't cross the road because it\n",
      "What should be the properties of \"it\"?\n",
      "The chicken didn't cross the road because it was too tired\n",
      "The chicken didn't cross the road because it was too wide\n",
      "At this point in the sentence, it's probably referring to either the chicken or the streetIntuition of attention\n",
      "Build up the contextual embedding from a word by\n",
      "selectively integrating information from all the\n",
      "neighboring words\n",
      "We say that a word \"attends to\" some neighboring\n",
      "words more than othersIntuition of attention:\n",
      "testAttention definition\n",
      "A mechanism for helping compute the embedding for\n",
      "a token by selectively attending to and integrating\n",
      "information from surrounding tokens (at the previous\n",
      "layer).\n",
      "More formally: a method for doing a weighted sum of\n",
      "vectors.Attention is left-to-right\n",
      "a a a a a\n",
      "1 2 3 4 5\n",
      "Self-Attention\n",
      "attention attention attention attention attention\n",
      "Layer\n",
      "x x x x x\n",
      "1 2 3 4 510.1 • T T : A S -A N 5\n",
      "HE 10R.A1NS•FOTRMETR ELF :TATESNTI-OAN ETWONRK 5\n",
      "HE RANSFORMER ELF TTENTION ETWORK\n",
      "a a a a a\n",
      "1 2 3 4 5\n",
      "a a a a a\n",
      "1 2 3 4 5\n",
      "Self-Attention\n",
      "Layer\n",
      "Self-Attention\n",
      "Layer\n",
      "x x x x x\n",
      "1 2 3 4 5\n",
      "x x x x x\n",
      "Figure 10.2 Information flow in a causal (or masked) self-attention model. In processing\n",
      "1 2 3 4 5\n",
      "each element of the sequence, the model attends to all the inputs up to, and including, the\n",
      "current one. Unlike RNNs, the computations at each time step are independent of all the\n",
      "Figur e 10.2 Information flow in a causal (or masked) self-attention model. In processing\n",
      "other steps and therefore can be performed in parallel.\n",
      "each element of the sequence, the model attends to all the inputs up to, and including, the\n",
      "current one. Unlike RNNs, the computations at each time step are independent of all the\n",
      "10.1.3 Self-attention mor e for mally\n",
      "other steps and therefore can be performed in parallel.\n",
      "We’ve given the intuition of self-attention (as a way to compute representations of a\n",
      "word at a given layer by integrating information from words at the previous layer)\n",
      "10.1.3 Self-attention mor e for mally\n",
      "and we’ve defined context as all the prior words in the input. Let’s now introduce\n",
      "the self-attention computation itself.\n",
      "We’ve given the intuition of self-attention (as a way to compute representations of a\n",
      "The core intuition of attention is the idea of comparing an item of interest to a\n",
      "word at a given layer by integrating information from words at the previous layer)\n",
      "collection of other items in a way that reveals their relevance in the current context.\n",
      "and we’ve defined context as all the prior words in the input. Let’s now introduce\n",
      "In the case of self-attention for language, the set of comparisons are to other words\n",
      "(or tokens) within a given sequence. The result of these comparisons is then used to\n",
      "the self-attention computation itself.\n",
      "compute an output sequence for the current input sequence. For example, returning\n",
      "The core intuition of attention is the idea of comparing an item of interest to a\n",
      "to Fig. 10.2, the computation of a is based on a set of comparisons between the\n",
      "3\n",
      "collection of other items in a way that reveals their relevance in the current context.\n",
      "input x and its preceding elements x and x , and to x itself.\n",
      "3 1 2 3\n",
      "In the case of self-attention for language, the set of comparisons are to other words\n",
      "How shall we compare words to other words? Since our representations for\n",
      "(or tokens) withinwaogrdivseanresveeqcutoernsc, ew.eT’ llhemarekesuulsteoofftohuerseoldcofmriepnadrithsoendsotisptrhoednucut stehdat twoe used\n",
      "for computing word similarity in Chapter 6, and also played a role in attention in\n",
      "compute an output sequence for the current input sequence. For example, returning\n",
      "Chapter 9. Let’s refer to the result of this comparison between words i and j as a\n",
      "to Fig. 10.2, the computation of a is based on a set of comparisons between the\n",
      "3\n",
      "score (we’ ll be updating this equation to add attention to the computation of this\n",
      "input x and its preceding elements x and x , and to x itself.\n",
      "3 score): 1 2 3\n",
      "How shall we compare words to other words? Since our representations for\n",
      "words are vectors, we’ ll make use of oV uer rso on ld1: friesc no dre t( hx e, x do) t = prx od· ux ct that we used (10.4)\n",
      "i j i j\n",
      "Simplified version of attention: a sum of prior words\n",
      "weighted by their similarity with the current word\n",
      "for computing word similarity in Chapter 6, and also played a role in attention in\n",
      "The result of a dot product is a scalar value ranging from − • to • , the larger\n",
      "Chapter 9. Let’s refer to the resuGlitveonf ath siesqucoenmcpe aorfi stooknenb eetmwbeeedndiwngosr:ds i and j as a\n",
      "the value the more similar the vectors that are being compared. Continuing with our\n",
      "x x x x x x x x\n",
      "score (we’ ll be updating this equation to add attention to the computation of this\n",
      "example, the first step in1 com2 p uti3ng y4 wo5uld 6be to7 comi pute three scores: x · x ,\n",
      "3 3 1\n",
      "score):\n",
      "x · x and x · x .PTrohdenuctoe:m aa k=e ae wffeecigtihvteedu sseuomf tohfe xse tshcororeusg,hw xe’ l(lannodr mx )alize them\n",
      "3 2 3 3\n",
      "i 1 7 i\n",
      "with a softmax toWceriegahtetead bveyc tthoer iro fsiwmeiliagrhittsy, tao x, that indicates the proportional\n",
      "i j\n",
      "i\n",
      "relevance of each input to the input element i that is the current focus of attention.\n",
      "Verson 1: score(x , x ) = x · x\n",
      "(10.4)\n",
      "i j i j\n",
      "a = softmax(score(x , x )) 8 j i (10.5)\n",
      "i j i j\n",
      "The result of a dot product is a scalar value ranging from − • to • , the larger\n",
      "exp(score(x , x ))\n",
      "i j\n",
      "= P 8 j i (10.6)\n",
      "the value the more similar the vectors that are being compared. Continuing with our\n",
      "i\n",
      "exp(score(x , x ))\n",
      "i k\n",
      "k= 1\n",
      "example, the first step in computing y would be to compute three scores: x · x ,\n",
      "3 3 1\n",
      "Of course, thesoftmax weight will likely behighest for thecurrent focuselement\n",
      "x · x and x · x . Then to make effective use of these scores, we’ ll normalize them\n",
      "3 2 3 3\n",
      "i, since vecx is very similar to itself, resulting in a high dot product. But other\n",
      "with a softmax to create a viector of weights, a , that indicates the proportional\n",
      "i j\n",
      "context words may also be similar to i, and the softmax will also assign some weight\n",
      "relevance of each input to the input element i that is the current focus of attention.\n",
      "to those words.\n",
      "Given the proportional scores in a , we generate an output value a by summing\n",
      "i\n",
      "a = softmax(score(x , x )) 8 j i\n",
      "(10.5)\n",
      "i j i j\n",
      "exp(score(x , x ))\n",
      "i j\n",
      "= 8 j i\n",
      "P (10.6)\n",
      "i\n",
      "exp(score(x , x ))\n",
      "i k\n",
      "k= 1\n",
      "Of course, thesoftmax weight will likely behighest for thecurrent focus element\n",
      "i, since vecx is very similar to itself, resulting in a high dot product. But other\n",
      "i\n",
      "context words may also be similar to i, and the softmax will also assign some weight\n",
      "to those words.\n",
      "Given the proportional scores in a , we generate an output value a by summing\n",
      "iIntuition of attention:\n",
      "test\n",
      "x1 x2 x3 x4 x5 x6 x7 xiAn Actual Attention Head: slightly more complicated\n",
      "High-level idea: instead of using vectors (like x and x )\n",
      "i 4\n",
      "directly, we'll represent 3 separate roles each vector x plays:\n",
      "i\n",
      "• query: As the current element being compared to the\n",
      "preceding inputs.\n",
      "• key: as a preceding input that is being compared to the\n",
      "current element to determine a similarity\n",
      "• value: a value of a preceding element that gets weighted\n",
      "and summedAttention intuition\n",
      "query\n",
      "x1 x2 x3 x4 x5 x6 x7 xi\n",
      "valuesIntuition of attention:\n",
      "query\n",
      "x1 x2 x3 x4 x5 x6 x7 xi\n",
      "keys\n",
      "k k k k k k k k\n",
      "v v v v v v v v\n",
      "valuesComputation of\n",
      "Self-Attention - 1\n",
      "1. Create three\n",
      "vectors from the\n",
      "embedding of each\n",
      "word: Query, Key,\n",
      "and Value by\n",
      "multiplying the\n",
      "embedding by three\n",
      "matrices that are\n",
      "learned during the\n",
      "training process.\n",
      "The Illustrated Transformer by Jay AlammarComputation of Self-\n",
      "Attention -2\n",
      "Calculate set of scores for each word. (focus on other parts of\n",
      "the input sentence as a word is encoded at a position). Score =\n",
      "dot product of query vector with key vector of respective\n",
      "scored word.\n",
      "Divide scores by 8 (square root of dimension of the key\n",
      "vectors used in paper).\n",
      "Pass the result through softmax to normalize scores so they’re\n",
      "all positive and add up to 1.\n",
      "Multiply value vector by softmax score. (keep values of\n",
      "word(s) to focus on, drown-out irrelevant words\n",
      "Sum the weighted value vectors. Produces output of self-\n",
      "attention layer at this position (for the first word).\n",
      "The Illustrated Transformer by Jay AlammarSelf-\n",
      "Attention\n",
      "– with\n",
      "Matrices\n",
      "The Illustrated Transformer by Jay AlammarMulti-Headed Self-Attention (Expanding attention to focus on different aspects)\n",
      "Multiple sets of Query/Key/Value weight\n",
      "matrices giving rise to multiple\n",
      "representation subspaces\n",
      "The Illustrated Transformer by Jay AlammarOverall Process\n",
      "The Illustrated Transformer by Jay AlammarCalculating the value of a3\n",
      "a\n",
      "Output of self-attention\n",
      "3\n",
      "6. Sum the weighted\n",
      "value vectors\n",
      "5. Weigh each value vector × ×\n",
      "\n",
      "3,1 3,2 3,3\n",
      "4. Turn into  weights via softmax\n",
      "i,j\n",
      "÷ ÷ ÷\n",
      "3. Divide score by √d\n",
      "k √d √d √d\n",
      "k k k\n",
      "2. Compare x3’s query with\n",
      "the keys for x1, x2, and x3\n",
      "Wk k Wk k Wk k\n",
      "1. Generate\n",
      "Wq q Wq q Wq q\n",
      "key, query, value\n",
      "vectors x Wv v x Wv v x Wv v\n",
      "1 2 3Actual Attention: slightly more complicated\n",
      "• Instead of one attention head, we'll have lots of them!\n",
      "• Intuition: each head might be attending to the context for different purposes\n",
      "• Different linguistic relationships or patterns in the contextSummary\n",
      "Attention is a method for enriching the representation of a token by\n",
      "incorporating contextual information\n",
      "The result: the embedding for each word will be different in different\n",
      "contexts!\n",
      "Contextual embeddings: a representation of word meaning in its\n",
      "context.\n",
      "We'll see in the next lecture that attention can also be viewed as a\n",
      "way to move information from one token to another.Attention\n",
      "TransformersThe Transformer Block\n",
      "TransformersReminder: transformer language model\n",
      "Next token long and thanks for all\n",
      "Language\n",
      "…\n",
      "logits logits logits logits logits\n",
      "Modeling\n",
      "U U U U U\n",
      "Head\n",
      "… … … … …\n",
      "Stacked\n",
      "…\n",
      "Transformer\n",
      "Blocks\n",
      "…\n",
      "x1 x2 x3 x4 x5\n",
      "+ 1 + 2 + 3 + 4 + 5\n",
      "Input\n",
      "…\n",
      "Encoding E E E E E\n",
      "Input tokens So long and thanks forThe residual stream: each token gets passed up and\n",
      "modifiedWe'll need nonlinearities, so a feedforward layerLayer norm: the vector x is normalized twice\n",
      "iLayer Norm\n",
      "Layer norm is a variation of the z-score from statistics, applied to a single vector in a hidden layerPutting together a single transformer blockA transformer is a stack of these blocks\n",
      "so all the vectors are of the same dimensionality d\n",
      "Block 2\n",
      "Block 1Residual streams and attention\n",
      "Notice that all parts of the transformer block apply to 1 residual stream (1\n",
      "token).\n",
      "Except attention, which takes information from other tokens\n",
      "Elhage et al. (2021) show that we can view attention heads as literally moving\n",
      "information from the residual stream of a neighboring token into the current\n",
      "stream .The Transformer Block\n",
      "TransformersParallelizing Attention\n",
      "Computation\n",
      "TransformersParallelizing computation using X\n",
      "For attention/transformer block we've been computing a single\n",
      "output at a single time step i in a single residual stream.\n",
      "But we can pack the N tokens of the input sequence into a single\n",
      "matrix X of size [N × d].\n",
      "Each row of X is the embedding of one token of the input.\n",
      "X can have 1K-32K rows, each of the dimensionality of the\n",
      "embedding d (the model dimension)QKT\n",
      "Now can do a single matrix multiply to combine Q and KTParallelizing attention\n",
      "• Scale the scores, take the softmax, and then\n",
      "multiply the result by V resulting in a matrix of\n",
      "shape N × d\n",
      "• An attention vector for each input tokenMasking out the future\n",
      "• What is this mask function?\n",
      "QKT has a score for each query dot every key,\n",
      "including those that follow the query.\n",
      "• Guessing the next word is pretty simple if you\n",
      "already know it!Masking out the future\n",
      "Add –∞ to cells in upper triangle\n",
      "The softmax will turn it to 0Another point: Attention is quadratic in lengthAttention againParallelizing Multi-head AttentionParallelizing Multi-head Attention\n",
      "orParallelizing Attention\n",
      "Computation\n",
      "TransformersInput and output: Position\n",
      "embeddings and the Language\n",
      "Model Head\n",
      "TransformersToken and Position Embeddings\n",
      "The matrix X (of shape [N × d]) has an embedding for\n",
      "each word in the context.\n",
      "This embedding is created by adding two distinct\n",
      "embedding for each input\n",
      "• token embedding\n",
      "• positional embeddingToken Embeddings\n",
      "Embedding matrix E has shape [|V | × d ].\n",
      "• One row for each of the |V | tokens in the vocabulary.\n",
      "• Each word is a row vector of d dimensions\n",
      "Given: string \"Thanks for all the\"\n",
      "1. Tokenize with BPE and convert into vocab indices\n",
      "w = [5,4000,10532,2224]\n",
      "2. Select the corresponding rows from E, each row an embedding\n",
      "• (row 5, row 4000, row 10532, row 2224).Position Embeddings\n",
      "There are many methods, but we'll just describe the simplest: absolute\n",
      "position.\n",
      "Goal: learn a position embedding matrix\n",
      "Start with randomly initialized embeddings\n",
      "• one for each integer up to some maximum length.\n",
      "• i.e., just as we have an embedding for token fish, we’ll have an\n",
      "embedding for position 3 and position 17.\n",
      "• As with word embeddings, these position embeddings are learned along\n",
      "with other parameters during training.Each x is just the sum of word and position embeddings\n",
      "Transformer Block\n",
      "X = Composite\n",
      "Embeddings\n",
      "(word + position)\n",
      "+ + + + +\n",
      "Word J\n",
      "b\n",
      "a w a t h b\n",
      "n\n",
      "Embeddings e i l l c k e i l l\n",
      "t\n",
      "Position 1 2 3 4 5\n",
      "Embeddings\n",
      "Janet will back the billLanguage modeling head\n",
      "…\n",
      "y1 y2 y|V| Word probabilities 1 x |V|\n",
      "Language Model Head\n",
      "Softmax over vocabulary V\n",
      "L\n",
      "takes h and outputs a …\n",
      "N u1 u2 u|V| Logits 1 x |V|\n",
      "distribution over vocabulary V\n",
      "Unembedding\n",
      "Unembedding layer d x |V|\n",
      "layer = ET\n",
      "hL hL hL\n",
      "1 x d\n",
      "1 2 N\n",
      "Layer L\n",
      "Transformer\n",
      "Block\n",
      "…\n",
      "w1 w2 w\n",
      "NLanguage modeling head\n",
      "L\n",
      "Unembedding layer: linear layer projects from h (shape [1 × d]) to logit vector\n",
      "N\n",
      "…\n",
      "y1 y2 y|V| Word probabilities 1 x |V|\n",
      "Why \"unembedding\"? Tied to ET\n",
      "Language Model Head\n",
      "Softmax over vocabulary V\n",
      "L\n",
      "takes h and outputs a …\n",
      "N u1 u2 u|V| Logits 1 x |V|\n",
      "distribution over vocabulary V\n",
      "Unembedding\n",
      "Unembedding layer d x |V|\n",
      "Weight tying, we use the same weights for\n",
      "layer = ET\n",
      "two different matrices\n",
      "hL hL hL\n",
      "1 x d\n",
      "1 2 N\n",
      "Layer L\n",
      "Transformer\n",
      "Block Unembedding layer maps from an embedding to a\n",
      "…\n",
      "1x|V| vector of logits\n",
      "w1 w2 w\n",
      "N16 C 9 • T T\n",
      "HAPTER HE RA NSFORMER\n",
      "language models of Chapter 3 compute the probability of a word given counts of\n",
      "its occurrence with the n − 1 prior words. The context is thus of size n − 1. For\n",
      "transformer language models, the context is the size of the transformer’ s context\n",
      "window, which can be quite large: 2K, 4K, even 32K tokens for very large models.\n",
      "The job of the language modeling head is to take the output of the final trans-\n",
      "former layer from the last token N and use it to predict the upcoming word at posi-\n",
      "tion N + 1. Fig. 9.14 shows how to accomplish this task, taking the output of the last\n",
      "token at the last layer (the d-dimensional output embedding of shape [1⇥d]) and\n",
      "producing a probability distribution over words (from which we will choose one to\n",
      "generate).\n",
      "…\n",
      "y1 y2 y|V| Word probabilities 1 x |V|\n",
      "Language Model Head\n",
      "Softmax over vocabulary V\n",
      "L\n",
      "takes h and outputs a …\n",
      "N u1 u2 u|V| Logits 1 x |V|\n",
      "distribution over vocabulary V\n",
      "Unembedding layer\n",
      "Unembedding layer d x |V|\n",
      "U = ET\n",
      "hL hL hL\n",
      "1 x d\n",
      "1 2 N\n",
      "Layer L\n",
      "Transformer\n",
      "Block\n",
      "…\n",
      "w1 w2 w\n",
      "N\n",
      "Figure 9.14 The language modeling head: the circuit at the top of a transformer that maps from the output\n",
      "L\n",
      "embedding for token N from the last transformer layer (h ) to a probability distribution over words in the\n",
      "N\n",
      "vocabulary V.\n",
      "The first module in Fig. 9.14 is a linear layer, whose job is to project from the\n",
      "L\n",
      "output h , which represents the output token embedding at position N from the final\n",
      "N\n",
      "block L, (hence of shape [1⇥d]) to the logit vector, or score vector, that will have a\n",
      "logit\n",
      "single score for each of the |V| possible words in the vocabulary V. The logit vector\n",
      "u is thus of dimensionality 1⇥|V|.\n",
      "This linear layer can be learned, but more commonly we tie this matrix to (the\n",
      "transpose of) the embedding matrix E. Recall that in weight tying, we use the\n",
      "weight tying\n",
      "Language modeling head\n",
      "same weights for two different matrices in the model. Thus at the input stage of the\n",
      "transformer the embedding matrix (of shape [|V|⇥d]) is used to map from a one-hot\n",
      "Logits, the score vector u\n",
      "vector over the vocabulary (of shape [1⇥|V|]) to an embedding (of shape [1⇥d]).\n",
      "T\n",
      "And then in the language model head, E , the transpose of the embedding matrix (of\n",
      "One score for each of the |V |\n",
      "possible words in the vocabulary V .\n",
      "shape [d⇥|V|]) is used to map back from an embedding (shape [1⇥d]) to a vector\n",
      "…\n",
      "y1 y2 y|V| Word probabilities 1 x |V|\n",
      "×\n",
      "Shape 1 |V |.\n",
      "over the vocabulary (shape [1⇥|V|]). In the learning process, E will be optimized to\n",
      "Language Model Head\n",
      "Softmax over vocabulary V\n",
      "L\n",
      "be good at doing both of these mappings. We therefore sometimes call the transpose\n",
      "takes h and outputs a …\n",
      "N u1 u2 u|V| Logits 1 x |V|\n",
      "Softmax turns the logits into\n",
      "distribution over vocabulary V\n",
      "ET the uUnnemebmeddibngedding layer because it is performing this reverse mapping.\n",
      "unembedding Unembedding layer d x |V|\n",
      "probabilities over vocabulary.\n",
      "layer = ET\n",
      "A softmax layer turns the logits u into tShheappero 1b ×ab i|lVit i|e. s y over the vocabulary.\n",
      "hL hL hL\n",
      "1 x d\n",
      "1 2 N\n",
      "Layer L\n",
      "Transformer\n",
      "Block\n",
      "L T\n",
      "… u = h E (9.44)\n",
      "N\n",
      "w1 w2 w\n",
      "N y = softmax(u) (9.45)\n",
      "We can use these probabilities to do things like help assign a probability to a\n",
      "given text. But the most important usage to generate text, which we do by sampling…\n",
      "Token probabilities y1 y2 y|V| w\n",
      "The final transformer i+1\n",
      "Sample token to\n",
      "softmax generate at position i+1\n",
      "model Language\n",
      "Modeling\n",
      "logits u1 u2 … u|V|\n",
      "Head\n",
      "U\n",
      "L\n",
      "h\n",
      "i\n",
      "feedforward\n",
      "…\n",
      "layer norm\n",
      "Layer L\n",
      "y1 y2 y|V| attention\n",
      "Token probabilities w\n",
      "layer norm\n",
      "i+1\n",
      "L-1 L\n",
      "h = x\n",
      "… i i\n",
      "2 3\n",
      "h = x Sample token to\n",
      "i i\n",
      "feedforward\n",
      "softmax\n",
      "generate at position i+1\n",
      "Language Layer 2 layer norm\n",
      "attention\n",
      "Modeling layer norm\n",
      "logits u1 u2 … u|V| 1 2\n",
      "h = x\n",
      "Head i i\n",
      "feedforward\n",
      "U Layer 1 layer norm\n",
      "attention\n",
      "layer norm\n",
      "1\n",
      "x\n",
      "i\n",
      "L\n",
      "h\n",
      "i Input\n",
      "+ i\n",
      "Encoding E\n",
      "feedforward\n",
      "Input token w\n",
      "layer norm i\n",
      "Layer L\n",
      "attention\n",
      "layer norm\n",
      "L-1 L\n",
      "h = x\n",
      "i i\n",
      "…\n",
      "2 3\n",
      "h = x\n",
      "i i\n",
      "feedforward\n",
      "layer norm\n",
      "Layer 2\n",
      "attention\n",
      "layer norm\n",
      "1 2\n",
      "h = x\n",
      "i i\n",
      "feedforward\n",
      "layer norm\n",
      "Layer 1\n",
      "attention\n",
      "layer norm\n",
      "1\n",
      "x\n",
      "i\n",
      "+ i\n",
      "Input\n",
      "Encoding\n",
      "E\n",
      "w\n",
      "Input token\n",
      "iInput and output: Position\n",
      "embeddings and the Language\n",
      "Model Head\n",
      "Transformers\n"
     ]
    }
   ],
   "source": [
    "# pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = 'D:/dell data/rutgers/nlp/slides/slide 10 - transformers.pdf'\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "print(pdf_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
