{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PyPDF2\n",
    "# pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from pdfminer.layout import LAParams\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "qa_model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    laparams = LAParams(line_margin=0.1)  # Adjust line margin to help with word separation\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text(x_tolerance=2, y_tolerance=3, laparams=laparams)  # Fine-tune tolerances\n",
    "    return text\n",
    "\n",
    "# pdf_path = 'D:/dell data/rutgers/nlp/slides/slide 10 - transformers.pdf'\n",
    "# pdf_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'Page \\d+|Header text|Footer text', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_with_sentence_overlap(text, chunk_size=512):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())\n",
    "        \n",
    "        if current_length + sentence_length > chunk_size:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [current_chunk[-1]]\n",
    "                current_length = len(current_chunk[0].split())\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_length += sentence_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    inputs = summarizer_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    summary_ids = summarizer_model.generate(inputs['input_ids'], max_length=250, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa(context):\n",
    "    inputs = qa_tokenizer(context, return_tensors=\"pt\")\n",
    "    outputs = qa_model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return qa_tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(qa_pair, context):\n",
    "    embeddings = [get_bert_embeddings(text) for text in [context, qa_pair]]\n",
    "    return cosine_similarity(embeddings[0], embeddings[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'D:/dell data/rutgers/data viz/assignment5/9.pdf'\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "cleaned_text = clean_text(pdf_text)\n",
    "sections = split_text_with_sentence_overlap(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All rights reserved. Draft of August 20, 2024. CHAPTER 9 The Transformer “The true art of memory is the art of attention ” Samuel Johnson, Idler #74, September 1759 In this chapter we introduce the transformer, the standard architecture for build- ing large language models. Transformer-based large language models have com- pletely changed the field of speech and language processing. Indeed, every subse- quent chapter in this textbook will make use of them. We’ll focus for now on left- to-right (sometimes called causal or autoregressive) language modeling, in which we are given a sequence of input tokens and predict output tokens one by one by conditioning on the prior context. The transformer is a neural network with a specific structure that includes a mechanismcalledself-attentionormulti-headattention.1 Attentioncanbethought of as a way to build contextual representations of a token’s meaning by attending to and integrating information from surrounding tokens, helping the model learn how tokens relate to each other over large spans. Next token long and thanks for all Language … Modeling logits logits logits logits logits U U U U U Head … … … … … Stacked Transformer … Blocks … x1 x2 x3 x4 x5 + 1 + 2 + 3 + 4 + 5 Input … Encoding E E E E E Input tokens So long and thanks for Figure 9.1 The architecture of a (left-to-right) transformer, showing how each input token get encoded, passed through a set of stacked transformer blocks, and then a language model head that predicts the next token. Fig. 9.1 sketches the transformer architecture. A transformer has three major components. At the center are columns of transformer blocks. Each block is a multilayer network (a multi-head attention layer, feedforward networks and layer normalization steps) that maps an input vector x in column i (corresponding to input i 1 Althoughmulti-headattentiondevelopedhistoricallyfromtheRNNattentionmechanism(Chapter8), we’lldefineattentionfromscratchhereforreaderswhohaven’tyetreadChapter8.2 CHAPTER 9 • THE TRANSFORMER token i) to an output vector h . The set of n blocks maps an entire context window i of input vectors (x ,...,x ) to a window of output vectors (h ,...,h ) of the same 1 n 1 n length. A column might contain from 12 to 96 or more stacked blocks. The column of blocks is preceded by the input encoding component, which pro- cesses an input token (like the word thanks) into a contextual vector representation, using an embedding matrix E and a mechanism for encoding token position. Each column is followed by a language modeling head, which takes the embedding out- put by the final transformer block, passes it through an unembedding matrix U and a softmax over the vocabulary to generate a single token for that column. Transformer-based language models are complex, and so the details will unfold over the next 5 chapters. In the next sections we’ll introduce multi-head attention, the rest of the transformer block, and the input encoding and language modeling head components. Chapter 10 discusses how language models are pretrained, and how tokens are generated via sampling.',\n",
       " 'Chapter 10 discusses how language models are pretrained, and how tokens are generated via sampling. Chapter 11 introduces masked language modeling and the BERT family of bidirectional transformer encoder models. Chap- ter 12 shows how to prompt LLMs to perform NLP tasks by giving instructions and demonstrations, and how to align the model with human preferences. Chapter 13 will introduce machine translation with the encoder-decoder architecture. 9.1 Attention Recall from Chapter 6 that for word2vec and other static embeddings, the repre- sentation of a word’s meaning is always the same vector irrespective of the context: the word chicken, for example, is always represented by the same fixed vector. So a static vector for the word it might somehow encode that this is a pronoun used for animals and inanimate entities. But in context it has a much richer meaning. Consider it in one of these two sentences: (9.1) The chicken didn’t cross the road because it was too tired. (9.2) The chicken didn’t cross the road because it was too wide. In (9.1) it is the chicken (i.e., the reader knows that the chicken was tired), while in (9.2) it is the road (and the reader knows that the road was wide).2 That is, if we are to compute the meaning of this sentence, we’ll need the meaning of it to be associated with the chicken in the first sentence and associated with the road in the second one, sensitive to the context. Furthermore, consider reading left to right like a causal language model, pro- cessing the sentence up to the word it: (9.3) The chicken didn’t cross the road because it At this point we don’t yet know which thing it is going to end up referring to! So a representation of it at this point might have aspects of both chicken and road as the reader is trying to guess what happens next. This fact that words have rich linguistic relationships with other words that may be far away pervades language. Consider two more examples: (9.4) The keys to the cabinet are on the table. (9.5) I walked along the pond, and noticed one of the trees along the bank. 2 We say that in the first example it corefers with the chicken, and in the second it corefers with the road;we’llreturntothisinChapter23.9.1 • ATTENTION 3 In (9.4), the phrase The keys is the subject of the sentence, and in English and many languages, must agree in grammatical number with the verb are; in this case both are plural. In English we can’t use a singular verb like is with a plural subject like keys (we’ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers to the side of a pond or river and not a financial institution because of the context, including words like pond. (We’ll discuss word senses more in Chapter 11.) The point of all these examples is that these contextual words that help us com- pute the meaning of words in context can be quite far away in the sentence or para- graph.',\n",
       " '(We’ll discuss word senses more in Chapter 11.) The point of all these examples is that these contextual words that help us com- pute the meaning of words in context can be quite far away in the sentence or para- graph. Transformers can build contextual representations of word meaning, contex- contextual tual embeddings, by integrating the meaning of these helpful contextual words. In a embeddings transformer, layer by layer, we build up richer and richer contextualized representa- tions of the meanings of input tokens. At each layer, we compute the representation of a token i by combining information about i from the previous layer with infor- mation about the neighboring tokens to produce a contextualized representation for each word at each position. Attention is the mechanism in the transformer that weighs and combines the representations from appropriate other tokens in the context from layer k−1 to build the representation for tokens in layer k. ehT ehT nekcihc nekcihc t’ndid t’ndid ssorc ssorc eht eht daor daor esuaceb esuaceb ti ti saw saw oot oot derit derit columns corresponding to input tokens Layer k+1 self-attention distribution Layer k Figure 9.2 The self-attention weight distribution α that is part of the computation of the representation for the word it at layer k+1. In computing the representation for it, we attend differently to the various words at layer l, with darker shades indicating higher self-attention values. Note that the transformer is attending highly to the columns corresponding to the tokenschickenandroad,asensibleresult,sinceatthepointwhereitoccurs,itcouldplausibly corefers with the chicken or the road, and hence we’d like the representation for it to draw on the representation for these earlier words. Figure adapted from Uszkoreit (2017). Fig. 9.2 shows a schematic example simplified from a transformer (Uszkoreit, 2017). The figure describes the situation when the current token is it and we need tocomputeacontextualrepresentationforthistokenatlayerk+1ofthetransformer, drawing on the representations (from layer k) of every prior token. The figure uses color to represent the attention distribution over the contextual words: the tokens chicken and road both have a high attention weight, meaning that as we are com- puting the representation for it, we will draw most heavily on the representation for chicken and road. This will be useful in building the final representation for it, since it will end up coreferring with either chicken or road. Let’s now turn to how this attention distribution is represented and computed.4 CHAPTER 9 • THE TRANSFORMER 9.1.1 Attention more formally As we’ve said, the attention computation is a way to compute a vector representation for a token at a particular layer of a transformer, by selectively attending to and integrating information from prior tokens at the previous layer. Attention takes an input representation x corresponding to the input token at position i, and a context i window of prior inputs x ..x , and produces an output a . 1 i−1 i In causal, left-to-right language models, the context is any of the prior words.',\n",
       " '1 i−1 i In causal, left-to-right language models, the context is any of the prior words. That is, when processing x , the model has access to x as well as the representations i i of all the prior tokens in the context window (context windows consist of thousands of tokens) but no tokens after i. (By contrast, in Chapter 11 we’ll generalize attention so it can also look ahead to future words.) Fig. 9.3 illustrates this flow of information in an entire causal self-attention layer, in which this same attention computation happens in parallel at each token position i. Thus a self-attention layer maps input sequences (x ,...,x ) to output sequences 1 n of the same length (a ,...,a ). 1 n a1 a2 a3 a4 a5 Self-Attention attention attention attention attention attention Layer x1 x2 x3 x4 x5 Figure 9.3 Information flow in causal self-attention. When processing each input x i, the model attends to all the inputs up to, and including x i. Simplified version of attention At its heart, attention is really just a weighted sum of context vectors, with a lot of complications added to how the weights are computed and what gets summed. For pedagogical purposes let’s first describe a simplified intuition of attention, in which the attention output a at token position i i is simply the weighted sum of all the representations x , for all j ≤ i; we’ll use α j ij to mean how much x should contribute to a : i j (cid:88) Simplified version: a i = α ijx j (9.6) j≤i Each α is a scalar used for weighing the value of input x when summing up ij j the inputs to compute a . How shall we compute this α weighting? In attention we i weight each prior embedding proportionally to how similar it is to the current token i. So the output of attention is a sum of the embeddings of prior tokens weighted by their similarity with the current token embedding. We compute similarity scores via dot product, which maps two vectors into a scalar value ranging from −∞ to ∞. The larger the score, the more similar the vectors that are being compared. We’ll normalize these scores with a softmax to create the vector of weights α , j ≤ i. ij Simplified Version: score(x i,x j) = x i·x j (9.7) α ij = softmax(score(x i,x j)) ∀j ≤ i (9.8) Thus in Fig. 9.3 we compute a by computing three scores: x ·x , x ·x and x ·x , 3 3 1 3 2 3 3 normalizing them by a softmax, and using the resulting probabilities as weights indicating each of their proportional relevance to the current position i. Of course,9.1 • ATTENTION 5 the softmax weight will likely be highest for x , since x is very similar to itself, i i resulting in a high dot product. But other context words may also be similar to i, and the softmax will also assign some weight to those words.',\n",
       " 'But other context words may also be similar to i, and the softmax will also assign some weight to those words. Then we use these weights as the α values in Eq. 9.6 to compute the weighted sum that is our a . 3 The simplified attention in equations 9.6 – 9.8 demonstrates the attention-based approach to computing a : compare the x to prior vectors, normalize those scores i i into a probability distribution used to weight the sum of the prior vector. But now we’re ready to remove the simplifications. A single attention head using query, key, and value matrices Now that we’ve attentionhead seen a simple intuition of attention, let’s introduce the actual attention head, the head version of attention that’s used in transformers. (The word head is often used in transformers to refer to specific structured layers). The attention head allows us to distinctly represent three different roles that each input embedding plays during the course of the attention process: • As the current element being compared to the preceding inputs. We’ll refer to query this role as a query. • In its role as a preceding input that is being compared to the current element key to determine a similarity weight. We’ll refer to this role as a key. value • And finally, as a value of a preceding element that gets weighted and summed up to compute the output for the current element. To capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will project each input vector x into a represen- i tation of its role as a key, query, or value: q i = x iWQ; k i = x iWK; v i = x iWV (9.9) Given these projections, when we are computing the similarity of the current ele- ment x with some prior element x , we’ll use the dot product between the current i j element’s query vector q and the preceding element’s key vector k . Furthermore, i j the result of a dot product can be an arbitrarily large (positive or negative) value, and exponentiating large values can lead to numerical issues and loss of gradients during training. To avoid this, we scale the dot product by a factor related to the size of the embeddings, via diving by the square root of the dimensionality of the query and key vectors (d ). We thus replace the simplified Eq. 9.7 with Eq. 9.11. The ensuing k softmax calculation resulting in α remains the same, but the output calculation for ij a is now based on a weighted sum over the value vectors v (Eq. 9.13). i Here’s a final set of equations for computing self-attention for a single self- attention output vector a from a single input vector x .',\n",
       " 'i Here’s a final set of equations for computing self-attention for a single self- attention output vector a from a single input vector x . This version of attention i i computes a by summing the values of the prior elements, each weighted by the i similarity of its key to the query from the current element: q i = x iWQ; k j = x jWK; v j = x jWV (9.10) q ·k i j score(x i,x j) = √ (9.11) d k α ij = softmax(score(x i,x j)) ∀j ≤ i (9.12) (cid:88) a i = α ijv j (9.13) j≤i We illustrate this in Fig. 9.4 for the case of calculating the value of the third output a in a sequence. 36 CHAPTER 9 • THE TRANSFORMER a Output of self-attention 3 6. Sum the weighted value vectors 4. Turn into weights via softmax i,j 𝛼 2. Compare x3’s query with the keys for x1, x2, and x3 Wk k k k 1. Generate Wq q q q key, query, value vectors x v x v x Wv v 1 2 3 × × 5. Weigh each value vector 𝛼3,1 𝛼3,2 𝛼3,3 ÷ ÷ ÷ 3. Divide score by √d k √dk √dk √dk Wk Wk Wq Wq Wv Wv Figure 9.4 Calculating the value of a , the third element of a sequence using causal (left- 3 to-right) self-attention. Let’s talk shapes. The input to attention x and the output from attention a both i i have the same dimensionality 1×d (We often call d the model dimensionality, and indeed as we’ll discuss in Section 9.2 the output h of each transformer block, i as well as the intermediate vectors inside the transformer block also have the same dimensionality 1×d.). We’ll have a dimension d for the key and query vectors. The query vector and k the key vector are both dimensionality 1×d , so we can take their dot product q ·k . k i j We’ll have a separate dimension d for the value vectors. The transform matrix WQ v has shape [d ×d ], WK is [d ×d ], and WV is [d ×d ]. In the original transformer k k v work (Vaswani et al., 2017), d was 512, d and d were both 64. k v Multi-head Attention Equations 9.11-9.13 describe a single attention head. But actually, transformers use multiple attention heads. The intuition is that each head might be attending to the context for different purposes: heads might be special- ized to represent different linguistic relationships between context elements and the current token, or to look for particular kinds of patterns in the context. multi-head So in multi-head attention we have h separate attention heads that reside in attention parallel layers at the same depth in a model, each with its own set of parameters that allows the head to model different aspects of the relationships among inputs. Thus each head i in a self-attention layer has its own set of key, query and value matrices: WKi, WQi and WVi.',\n",
       " 'Thus each head i in a self-attention layer has its own set of key, query and value matrices: WKi, WQi and WVi. These are used to project the inputs into separate key, value, and query embeddings for each head. When using multiple heads the model dimension d is still used for the input and output, the key and query embeddings have dimensionality d , and the value k embeddings are of dimensionality d (again, in the original transformer paper d = v k d = 64, h = 8, and d = 512). Thus for each head i, we have weight layers WQi of v shape [d×d ], WKi of shape [d×d ], and WVi of shape [d×d ]. k k v Below are the equations for attention augmented with multiple heads; Fig. 9.59.2 • TRANSFORMER BLOCKS 7 shows an intuition. qc i = x iWQc; kc j = x jWKc; vc j = x jWVc; ∀ c 1 ≤ c ≤ h (9.14) qc·kc scorec(x i,x j) = √i j (9.15) d k α ic j = softmax(scorec(x i,x j)) ∀j ≤ i (9.16) (cid:88) headc = αcvc (9.17) i ij j j≤i a i = (head1⊕head2...⊕headh)WO (9.18) MultiHeadAttention(x i,[x 1,··· ,x N]) = a i (9.19) The output of each of the h heads is of shape 1×d , and so the output of the v multi-head layer with h heads consists of h vectors of shape 1×d . These are con- v catenated to produce a single output with dimensionality 1×hd . Then we use yet v another linear projection WO ∈ Rhdv×d to reshape it, resulting in the multi-head attention vector a with the correct output shape [1xd] at each input i. i a i [1 x d] Project down to d WO [hd x d] v … [1 x hd ] v Concatenate Outputs [1 x d v ] [1 x d v ] Each head Head 1 Head 2 … Head 8 attends differently WK WV WQ WK WV WQ WK WV WQ to context 1 1 1 2 2 2 8 8 8 … x x x x i-3 i-2 i-1 i [1 ax d] i Figure 9.5 The multi-head attention computation for input x i, producing output a i. A multi-head attention layer has h heads, each with its own key, query and value weight matrices. The outputs from each of the heads are concatenated and then projected down to d, thus producing an output of the same size as the input. 9.2 Transformer Blocks The self-attention calculation lies at the core of what’s called a transformer block, which, in addition to the self-attention layer, includes three other kinds of layers: (1) a feedforward layer, (2) residual connections, and (3) normalizing layers (colloqui- ally called “layer norm”). Fig. 9.6 illustrates a transformer block, sketching a common way of thinking residualstream about the block that is called the residual stream (Elhage et al., 2021).',\n",
       " '9.6 illustrates a transformer block, sketching a common way of thinking residualstream about the block that is called the residual stream (Elhage et al., 2021). In the resid- ual stream viewpoint, we consider the processing of an individual token i through the transformer block as a single stream of d-dimensional representations for token position i. This residual stream starts with the original input vector, and the various8 CHAPTER 9 • THE TRANSFORMER h h h i-1 i Residual i+1 Stream + Feedforward Layer Norm … … + MultiHead Attention Layer Norm x x x i-1 i i+1 Figure 9.6 The architecture of a transformer block showing the residual stream. This figureshowstheprenormversionofthearchitecture,inwhichthelayernormshappenbefore the attention and feedforward layers rather than after. components read their input from the residual stream and add their output back into the stream. The input at the bottom of the stream is an embedding for a token, which has dimensionality d. This initial embedding gets passed up (by residual connections), and is progressively added to by the other components of the transformer: the at- tention layer that we have seen, and the feedforward layer that we will introduce. Before the attention and feedforward layer is a computation called the layer norm. Thus the initial vector is passed through a layer norm and attention layer, and the result is added back into the stream, in this case to the original input vector x . And then this summed vector is again passed through another layer norm and a i feedforward layer, and the output of those is added back into the residual, and we’ll use h to refer to the resulting output of the transformer block for token i. (In earlier i descriptions the residual stream was often described using a different metaphor as residual connections that add the input of a component to its output, but the residual stream is a more perspicuous way of visualizing the transformer.) We’ve already seen the attention layer, so let’s now introduce the feedforward and layer norm computations in the context of processing a single input x at token i position i. Feedforward layer The feedforward layer is a fully-connected 2-layer network, i.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights are the same for each token position i , but are different from layer to layer. It is common to make the dimensionality d of the hidden layer of the feedforward ff network be larger than the model dimensionality d. (For example in the original transformer model, d = 512 and d = 2048.) ff FFN(x i) = ReLU(x iW 1+b 1)W 2+b 2 (9.20) Layer Norm At two stages in the transformer block we normalize the vector (Ba layernorm et al., 2016). This process, called layer norm (short for layer normalization), is one9.2 • TRANSFORMER BLOCKS 9 of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training.',\n",
       " 'This process, called layer norm (short for layer normalization), is one9.2 • TRANSFORMER BLOCKS 9 of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. Layer norm is a variation of the z-score from statistics, applied to a single vec- tor in a hidden layer. That is, the term layer norm is a bit confusing; layer norm is not applied to an entire transformer layer, but just to the embedding vector of a single token. Thus the input to layer norm is a single vector of dimensionality d and the output is that vector normalized, again of dimensionality d. The first step in layer normalization is to calculate the mean, µ, and standard deviation, σ, over the elements of the vector to be normalized. Given an embedding vector x of dimen- sionality d, these values are calculated as follows. d 1 (cid:88) µ = x i (9.21) d i=1 (cid:118) (cid:117) d (cid:117)1 (cid:88) σ = (cid:116) (x i−µ)2 (9.22) d i=1 Given these values, the vector components are normalized by subtracting the mean from each and dividing by the standard deviation. The result of this computation is a new vector with zero mean and a standard deviation of one. (x−µ) ˆx = (9.23) σ Finally, in the standard implementation of layer normalization, two learnable param- eters, γ and β, representing gain and offset values, are introduced. (x−µ) LayerNorm(x) = γ +β (9.24) σ Putting it all together The function computed by a transformer block can be ex- pressed by breaking it down with one equation for each component computation, using t (of shape [1×d]) to stand for transformer and superscripts to demarcate each computation inside the block: t1 i = LayerNorm(x i) (9.25) t2 = MultiHeadAttention(t1,(cid:2) x1,··· ,x1 (cid:3) ) (9.26) i i 1 N t3 i = t2 i +x i (9.27) t4 = LayerNorm(t3) (9.28) i i t5 = FFN(t4) (9.29) i i h i = t5 i +t3 i (9.30) Notice that the only component that takes as input information from other tokens (other residual streams) is multi-head attention, which (as we see from (9.27)) looks at all the neighboring tokens in the context. The output from attention, however, is thenaddedintothistoken’sembeddingstream. Infact, Elhageetal.(2021)showthat we can view attention heads as literally moving information from the residual stream of a neighboring token into the current stream. The high-dimensional embedding space at each position thus contains information about the current token and about neighboring tokens, albeit in different subspaces of the vector space. Fig. 9.7 shows a visualization of this movement.10 CHAPTER 9 • THE TRANSFORMER Token A Token B residual residual stream stream Figure 9.7 An attention head can move information from token A’s residual stream into token B’s residual stream. Crucially, the input and output dimensions of transformer blocks are matched so they can be stacked.',\n",
       " 'Crucially, the input and output dimensions of transformer blocks are matched so they can be stacked. Each token vector x at the input to the block has dimensionality i d, and the output h also has dimensionality d. Transformers for large language i models stack many of these blocks, from 12 layers (used for the T5 or GPT-3-small language models) to 96 layers (used for GPT-3 large), to even more for more recent models. We’ll come back to this issue of stacking in a bit. Equation (9.27) and following are just the equation for a single transformer block, but the residual stream metaphor goes through all the transformer layers, from the first transformer blocks to the 12th, in a 12-layer transformer. At the ear- lier transformer blocks, the residual stream is representing the current token. At the highest transformer blocks, the residual stream is usually representing the following token, since at the very end it’s being trained to predict the next token. Once we stack many blocks, there is one more requirement: at the very end of the last (highest) transformer block, there is a single extra layer norm that is run on the last h of each token stream (just below the language model head layer that we i will define soon). 3 9.3 Parallelizing computation using a single matrix X This description of multi-head attention and the rest of the transformer block has been from the perspective of computing a single output at a single time step i in a single residual stream. But as we pointed out earlier, the attention computation performed for each token to compute a is independent of the computation for each i other token, and that’s also true for all the computation in the transformer block computing h from the input x . That means we can easily parallelize the entire i i computation, taking advantage of efficient matrix multiplication routines. We do this by packing the input embeddings for the N tokens of the input se- quence into a single matrix X of size [N ×d]. Each row of X is the embedding of one token of the input. Transformers for large language models commonly have an input length N = 1K, 2K, or as many as 32K tokens (or more), so X typically has be- tween 1K and 32K rows, each of the dimensionality of the embedding d (the model 3 Notethatweareusingthemostcommoncurrenttransformerarchitecture,whichiscalledtheprenorm architecture. TheoriginaldefinitionofthetransformerinVaswanietal.(2017)usedanalternativearchi- tecture called the postnorm transformer in which the layer norm happens after the attention and FFN layers; it turns out moving the layer norm beforehand works better, but does require this one extra layer attheend.9.3 • PARALLELIZING COMPUTATION USING A SINGLE MATRIX X 11 dimension). Parallelizing attention Let’s first see this for a single attention head and then turn to multiple heads, and then add in the rest of the components in the transformer block.',\n",
       " 'Parallelizing attention Let’s first see this for a single attention head and then turn to multiple heads, and then add in the rest of the components in the transformer block. For one head we multiply X by the key, query, and value matrices WQ of shape [d×d ], WK of shape [d×d ], and WV of shape [d×d ], to produce matrices k k v Q of shape [N ×d k], K ∈ RN×dk, and V ∈ RN×dv, containing all the key, query, and value vectors: Q = XWQ; K = XWK; V = XWV (9.31) Given these matrices we can compute all the requisite query-key comparisons simul- taneously by multiplying Q and K(cid:124) in a single matrix multiplication. The product is of shape N×N, visualized in Fig. 9.8. q1•k1 q1•k2 q1•k3 q1•k4 q2•k1 q2•k2 q2•k3 q2•k4 N q3•k1 q3•k2 q3•k3 q3•k4 q4•k1 q4•k2 q4•k3 q4•k4 N Figure 9.8 The N ×N QK(cid:124) matrix showing how it computes all q i·k j comparisons in a single matrix multiple. Once we have this QK(cid:124) matrix, we can very efficiently scale these scores, take the softmax, and then multiply the result by V resulting in a matrix of shape N ×d: a vector embedding representation for each token in the input. We’ve reduced the entire self-attention step for an entire sequence of N tokens for one head to the following computation: (cid:18) (cid:18) (cid:124)(cid:19)(cid:19) QK A = softmax mask √ V (9.32) d k Maskingoutthefuture Youmayhavenoticedthatweintroducedamaskfunction in Eq. 9.32 above. This is because the self-attention computation as we’ve described (cid:124) it has a problem: the calculation in QK results in a score for each query value to every key value, including those that follow the query. This is inappropriate in the setting of language modeling: guessing the next word is pretty simple if you already know it! To fix this, the elements in the upper-triangular portion of the matrix are zeroed out (set to −∞), thus eliminating any knowledge of words that follow in the sequence. This is done in practice by adding a mask matrix M in which M =−∞∀j >i (i.e. for the upper-triangular portion) and M =0 otherwise. ij ij (cid:124) Fig. 9.9 shows the resulting masked QK matrix. (we’ll see in Chapter 11 how to make use of words in the future for tasks that need it). Fig. 9.10 shows a schematic of all the computations for a single attention head parallelized in matrix form. Fig. 9.8 and Fig. 9.9 also make it clear that attention is quadratic in the length of the input, since at each layer we need to compute dot products between each pair of tokens in the input. This makes it expensive to compute attention over very long documents (like entire novels).',\n",
       " 'This makes it expensive to compute attention over very long documents (like entire novels). Nonetheless modern large language models manage to use quite long contexts of thousands or tens of thousands of tokens.12 CHAPTER 9 • THE TRANSFORMER q1•k1 −∞ −∞ −∞ q2•k1 q2•k2 −∞ −∞ N q3•k1 q3•k2 q3•k3 −∞ q4•k1 q4•k2 q4•k3 q4•k4 N Figure 9.9 The N×N QK(cid:124) matrix showing the q i·k j values, with the upper-triangle por- tion of the comparisons matrix zeroed out (set to −∞, which the softmax will turn to zero). q1 q2 q3 q4 k1 k2 k3 k4 X Q X K X V ToIn kp eu nt 1 WQ TQ oku ee nry 1 ToIn kp eu nt 1 WK ToK kee ny 1 ToIn kp eu nt 1 WV ToV ka elu ne 1 Input Query Input Key Input Value Token 2 x = Token 2 Token 2 x = Token 2 Token 2 x = Token 2 Input Query Input Key Input Value Token 3 Token 3 Token 3 Token 3 Token 3 Token 3 Input Query Input Key Input Value Token 4 d x d Token 4 Token 4 d x d k Token 4 Token 4 d x d v Token 4 k N x d N x d k N x d N x d k N x d N x d v T Q K QKT QKT masked V A x = q1•k1 q1•k2 q1•k3 q1•k4 qqq111•••kkk111 −∞ −∞ −∞ v1 a1 mask q2•k1 q2•k2 q2•k3 q2•k4 = q2•k1 q2•k2 −∞ −∞ x v2 = a2 q3•k1 q3•k2 q3•k3 q3•k4 q3•k1 q3•k2 q3•k3 −∞ v3 a3 d k x N q4•k1 q4•k2 q4•k3 q4•k4 q4•k1 q4•k2 q4•k3 q4•k4 v4 a4 N x d k N x N N x N N x d v N x d v Figure 9.10 Schematicoftheattentioncomputationforasingleattentionheadinparallel. Thefirstrowshows the computation of the Q, K, and V matrices. The second row shows the computation of QKT, the masking (the softmax computation and the normalizing by dimensionality are not shown) and then the weighted sum of the value vectors to get the final attention vectors. Parallelizing multi-head attention In multi-head attention, as with self-attention, the input and output have the model dimension d, the key and query embeddings have dimensionality d , and the value embeddings are of dimensionality d (again, k v in the original transformer paper d = d = 64, h = 8, and d = 512). Thus for each k v head i, we have weight layers WQ i ∈ Rd×dk, WK i ∈ Rd×dk, and WV i ∈ Rd×dv, and these get multiplied by the inputs packed into X to produce Q ∈ RN×dk, K ∈ RN×dk, and V ∈ RN×dv. The output of each of the h heads is of shape N ×d v, and so the outputofthemulti-headlayerwithhheadsconsistsofhmatricesofshapeN×d . To v make use of these matrices in further processing, they are concatenated to produce a single output with dimensionality N ×hd .',\n",
       " 'To v make use of these matrices in further processing, they are concatenated to produce a single output with dimensionality N ×hd . Finally, we use yet another linear v projection WO ∈ Rhdv×d, that reshape it to the original output dimension for each token. Multiplying the concatenated N ×hd v matrix output by WO ∈ Rhdv×d yields9.4 • THE INPUT: EMBEDDINGS FOR TOKEN AND POSITION 13 the self-attention output A of shape [N×d]. Qi = XWQi ; Ki = XWKi ; Vi = XWVi (9.33) (cid:18) QiKi(cid:124)(cid:19) head i = SelfAttention(Qi,Ki,Vi) = softmax √ Vi (9.34) d k MultiHeadAttention(X) = (head 1⊕head 2...⊕head h)WO (9.35) Putting it all together with the parallel input matrix X The function computed in parallel by an entire layer of N transformer block over the entire N input tokens can be expressed as: O = LayerNorm(X+MultiHeadAttention(X)) (9.36) H = LayerNorm(O+FFN(O)) (9.37) Or we can break it down with one equation for each component computation, using T (of shape [N ×d]) to stand for transformer and superscripts to demarcate each computation inside the block: T1 = MultiHeadAttention(X) (9.38) T2 = X+T1 (9.39) T3 = LayerNorm(T2) (9.40) T4 = FFN(T3) (9.41) T5 = T4+T3 (9.42) H = LayerNorm(T5) (9.43) Here when we use a notation like FFN(T3) we mean that the same FFN is applied in parallel to each of the N embedding vectors in the window. Similarly, each of the N tokens is normed in parallel in the LayerNorm. Crucially, the input and output dimensions of transformer blocks are matched so they can be stacked. Since each token x at the input to the block has dimensionality d, that means the input X and i output H are both of shape [N×d]. 9.4 The input: embeddings for token and position Let’s talk about where the input X comes from. Given a sequence of N tokens (N is embedding the context length in tokens), the matrix X of shape [N ×d] has an embedding for each word in the context. The transformer does this by separately computing two embeddings: an input token embedding, and an input positional embedding. A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di- mension d that will be our initial representation for the input token. (As we pass vectors up through the transformer layers in the residual stream, this embedding representation will change and grow, incorporating context and playing a different role depending on the kind of language model we are building.) The set of initial embeddings are stored in the embedding matrix E, which has a row for each of the |V| tokens in the vocabulary. Thus each word is a row vector of d dimensions, and E has shape [|V|×d]. Given an input token string like Thanks for all the we first convert the tokens into vocabulary indices (these were created when we first tokenized the input using14 CHAPTER 9 • THE TRANSFORMER BPE or SentencePiece). So the representation of thanks for all the might be w = [5,4000,10532,2224].',\n",
       " 'So the representation of thanks for all the might be w = [5,4000,10532,2224]. Next we use indexing to select the corresponding rows from E, (row 5, row 4000, row 10532, row 2224). Another way to think about selecting token embeddings from the embedding matrix is to represent tokens as one-hot vectors of shape [1×|V|], i.e., with one one-hotvector dimension for each word in the vocabulary. Recall that in a one-hot vector all the elements are 0 except one, the element whose dimension is the word’s index in the vocabulary, which hasvalue1. Soifthe word“thanks”has index5in thevocabulary, x = 1, and x = 0 ∀i (cid:54)= 5, as shown here: 5 i [0 0 0 0 1 0 0 ... 0 0 0 0] 1 2 3 4 5 6 7 ... ... |V| Multiplyingbyaone-hotvectorthathasonlyonenon-zeroelementx =1simply i selects out the relevant row vector for word i, resulting in the embedding for word i, as depicted in Fig. 9.11. d 5 5 |V| d 1 0 0 0 0 1 0 0 … 0 0 0 0 ✕ E = 1 |V| Figure 9.11 Selecting the embedding vector for word V by multiplying the embedding 5 matrix E with a one-hot vector with a 1 in index 5. We can extend this idea to represent the entire token sequence as a matrix of one- hot vectors, one for each of the N positions in the transformer’s context window, as shown in Fig. 9.12. d |V| d 0 0 0 0 1 0 0 … 0 0 0 0 0 0 0 0 0 0 0 … 0 0 1 0 ✕ = 1 0 0 0 0 0 0 … 0 0 0 0 E … N N 0 0 0 0 1 0 0 … 0 0 0 0 |V| Figure 9.12 Selecting the embedding matrix for the input sequence of token idsW by mul- tiplying a one-hot matrix corresponding toW by the embedding matrix E. These token embeddings are not position-dependent. To represent the position of each token in the sequence, we combine these token embeddings with positional positional embeddings specific to each position in an input sequence. embeddings Where do we get these positional embeddings? The simplest method, called absolute absolute position, is to start with randomly initialized embeddings corresponding position to each possible input position up to some maximum length. For example, just as we have an embedding for the word fish, we’ll have an embedding for the position 3. As with word embeddings, these positional embeddings are learned along with other parameters during training. We can store them in a matrix Epos of shape [1×N]. To produce an input embedding that captures positional information, we just add the word embedding for each input to its corresponding positional embedding.',\n",
       " 'To produce an input embedding that captures positional information, we just add the word embedding for each input to its corresponding positional embedding. The individual token and position embeddings are both of size [1×d], so their sum is9.5 • THE LANGUAGE MODELING HEAD 15 Transformer Block X = Composite Embeddings (word + position) Janet 1 will 2 back 3 Janet will back the bill + + + the + 4 bill + 5 Word Embeddings Position Embeddings Figure 9.13 A simple way to model position: add an embedding of the absolute position to the token embedding to produce a new embedding of the same dimensionality. also [1×d], This new embedding serves as the input for further processing. Fig. 9.13 shows the idea. The final representation of the input, the matrix X, is an [N×d] matrix in which each row i is the representation of the ith token in the input, computed by adding E[id(i)]—the embedding of the id of the token that occurred at position i—, to P[i], the positional embedding of position i. A potential problem with the simple absolute position embedding approach is that there will be plenty of training examples for the initial positions in our inputs and correspondingly fewer at the outer length limits. These latter embeddings may be poorly trained and may not generalize well during testing. An alternative ap- proach to absolute positional embeddings is to choose a static function that maps integer inputs to real-valued vectors in a way that captures the inherent relation- ships among the positions. That is, it captures the fact that position 4 in an input is more closely related to position 5 than it is to position 17. A combination of sine and cosine functions with differing frequencies was used in the original transformer work. Even more complex positional embedding methods exist, such as ones that represent relative position instead of absolute position, often implemented in the attention mechanism at each layer rather than being added once at the initial input. 9.5 The Language Modeling Head The last component of the transformer we must introduce is the language modeling language head. Here we are using the word head to mean the additional neural circuitry we modelinghead head add on top of the basic transformer architecture when we apply pretrained trans- former models to various tasks. The language modeling head is the circuitry we need to do language modeling. Recallthatlanguagemodels, fromthesimplen-grammodelsofChapter3through the feedforward and RNN language models of Chapter 7 and Chapter 8, are word predictors. Given a context of words, they assign a probability to each possible next word. For example, if the preceding context is “Thanks for all the” and we want to know how likely the next word is “fish” we would compute: P(fish|Thanks for all the) Language models give us the ability to assign such a conditional probability to every possible next word, giving us a distribution over the entire vocabulary.',\n",
       " 'For example, if the preceding context is “Thanks for all the” and we want to know how likely the next word is “fish” we would compute: P(fish|Thanks for all the) Language models give us the ability to assign such a conditional probability to every possible next word, giving us a distribution over the entire vocabulary. The n-gram16 CHAPTER 9 • THE TRANSFORMER language models of Chapter 3 compute the probability of a word given counts of its occurrence with the n−1 prior words. The context is thus of size n−1. For transformer language models, the context is the size of the transformer’s context window, which can be quite large: 2K, 4K, even 32K tokens for very large models. The job of the language modeling head is to take the output of the final trans- former layer from the last token N and use it to predict the upcoming word at posi- tion N+1. Fig. 9.14 shows how to accomplish this task, taking the output of the last token at the last layer (the d-dimensional output embedding of shape [1×d]) and producing a probability distribution over words (from which we will choose one to generate). y1 y2 … y|V| Word probabilities 1 x |V| Language Model Head Softmax over vocabulary V L takes h N and outputs a u1 u2 … u|V| Logits 1 x |V| distribution over vocabulary V Unembedding layer Unembedding layer d x |V| U = ET hL 1 hL 2 hL N 1 x d Layer L Transformer Block … w1 w2 w N Figure 9.14 The language modeling head: the circuit at the top of a transformer that maps from the output embedding for token N from the last transformer layer (hL) to a probability distribution over words in the N vocabularyV. The first module in Fig. 9.14 is a linear layer, whose job is to project from the output hL, which represents the output token embedding at position N from the final N logit block L, (hence of shape [1×d]) to the logit vector, or score vector, that will have a single score for each of the |V| possible words in the vocabularyV. The logit vector u is thus of dimensionality 1×|V|. This linear layer can be learned, but more commonly we tie this matrix to (the weighttying transpose of) the embedding matrix E. Recall that in weight tying, we use the same weights for two different matrices in the model. Thus at the input stage of the transformer the embedding matrix (of shape [|V|×d]) is used to map from a one-hot vector over the vocabulary (of shape [1×|V|]) to an embedding (of shape [1×d]). And then in the language model head, ET, the transpose of the embedding matrix (of shape [d×|V|]) is used to map back from an embedding (shape [1×d]) to a vector over the vocabulary (shape [1×|V|]). In the learning process, E will be optimized to be good at doing both of these mappings. We therefore sometimes call the transpose unembedding ET the unembedding layer because it is performing this reverse mapping.',\n",
       " 'We therefore sometimes call the transpose unembedding ET the unembedding layer because it is performing this reverse mapping. A softmax layer turns the logits u into the probabilities y over the vocabulary. u = hL ET (9.44) N y = softmax(u) (9.45) We can use these probabilities to do things like help assign a probability to a given text. But the most important usage to generate text, which we do by sampling9.5 • THE LANGUAGE MODELING HEAD 17 a word from these probabilities y. We might sample the highest probability word (‘greedy’ decoding), or use another of the sampling methods we’ll introduce in Sec- tion ??. In either case, whatever entry y we choose from the probability vector y, k we generate the word that has that index k. … Token probabilities y1 y2 y|V| w i+1 Sample token to Language softmax generate at position i+1 Modeling Head logits u1 u2 … u|V| U hL i feedforward Layer L layer norm attention layer norm hL-1 = xL … i i h2 = x3 i i feedforward layer norm Layer 2 attention layer norm h1 = x2 i i feedforward Layer 1 layer norm attention layer norm x1 i + i Input Encoding E Input token w i Figure 9.15 A transformer language model (decoder-only), stacking transformer blocks and mapping from an input token w i to to a predicted next token w i+1. Fig. 9.15 shows the total stacked architecture for one token i. Note that the input to each transformer layer x is the same as the output from the preceding layer h−. i i Now that we see all these transformer layers spread out on the page, we can point out another useful feature of the unembedding layer: as a tool for interpretability of logitlens the internals of the transformer that we call the logit lens (Nostalgebraist, 2020). We can take a vector from any layer of the transformer and, pretending that it is the prefinal embedding, simply multiply it by the unembedding layer to get logits, and compute a softmax to see the distribution over words that that vector might be representing. This can be a useful window into the internal representations of the model. Since the network wasn’t trained to make the internal representations function in this way, the logit lens doesn’t always work perfectly, but this can still be a useful trick. A terminological note before we conclude: You will sometimes see a trans- former used for this kind of unidirectional causal language model called a decoder- decoder-only only model. This is because this model constitutes roughly half of the encoder- model18 CHAPTER 9 • THE TRANSFORMER decoder model for transformers that we’ll see how to apply to machine translation in Chapter 13. (Confusingly, the original introduction of the transformer had an encoder-decoder architecture, and it was only later that the standard paradigm for causal language model was defined by using only the decoder part of this original architecture). 9.6 Summary This chapter has introduced the transformer and its components for the task of lan- guage modeling.',\n",
       " '9.6 Summary This chapter has introduced the transformer and its components for the task of lan- guage modeling. We’ll continue the task of language modeling including issues like training and sampling in the next chapter. Here’s a summary of the main points that we covered: • Transformers are non-recurrent networks based on multi-head attention, a kind of self-attention. A multi-head attention computation takes an input vector x and maps it to an output a by adding in vectors from prior tokens, i i weighted by how relevant they are for the processing of the current word. • A transformer block consists of a residual stream in which the input from the prior layer is passed up to the next layer, with the output of different com- ponents added to it. These components include a multi-head attention layer followed by a feedforward layer, each preceded by layer normalizations. Transformer blocks are stacked to make deeper and more powerful networks. • The input to a transformer is a computing by adding an embedding (computed with an embedding matrix) to a positional encoding that represents the se- quential position of the token in the window. • Language models can be built out of stacks of transformer blocks, with a language model head at the top, which applies an unembedding matrix to the output H of the top layer to generate the logits, which are then passed through a softmax to generate word probabilities. • Transformer-based language models have a wide context window (as wide as 32768 tokens for very large models) allowing them to draw on enormous amounts of context to predict upcoming words. Bibliographical and Historical Notes The transformer (Vaswani et al., 2017) was developed drawing on two lines of prior research: self-attention and memory networks. Encoder-decoder attention, the idea of using a soft weighting over the encodings of input words to inform a generative decoder (see Chapter 13) was developed by Graves (2013) in the context of handwriting generation, and Bahdanau et al. (2015) for MT. This idea was extended to self-attention by dropping the need for separate encoding and decoding sequences and instead seeing attention as a way of weighting the tokens in collecting information passed from lower layers to higher layers (Ling et al., 2015; Cheng et al., 2016; Liu et al., 2016). Other aspects of the transformer, including the terminology of key, query, and value, came from memory networks, a mechanism for adding an external read-BIBLIOGRAPHICAL AND HISTORICAL NOTES 19 write memory to networks, by using an embedding of a query to match keys rep- resenting content in an associative memory (Sukhbaatar et al., 2015; Weston et al., 2015; Graves et al., 2014). MORE HISTORY TBD IN NEXT DRAFT.20 Chapter 9 • The Transformer Ba,J.L.,J.R.Kiros,andG.E.Hinton.2016. Layernormal- ization. NeurIPSworkshop. Bahdanau, D., K. H. Cho, and Y. Bengio. 2015. Neural ma- chinetranslationbyjointlylearningtoalignandtranslate. ICLR2015. Cheng, J., L. Dong, and M. Lapata. 2016. Long short-term memory-networksformachinereading. EMNLP. Elhage, N., N. Nanda, C. Olsson, T. Henighan, N. Joseph, B.Mann,A.Askell,Y.Bai,A.Chen,T.Conerly,N.Das- Sarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Her- nandez, A. Jones, J. Kernion, L.',\n",
       " 'Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCan- dlish, and C. Olah. 2021. A mathematical framework for transformercircuits. Whitepaper. Graves,A.2013.Generatingsequenceswithrecurrentneural networks. ArXiv. Graves, A., G. Wayne, and I. Danihelka. 2014. Neural Tur- ingmachines. ArXiv. Ling, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez, S. Amir, L. Marujo, and T. Lu´ıs. 2015. Finding function inform: Compositionalcharactermodelsforopenvocab- ularywordrepresentation. EMNLP. Liu,Y.,C.Sun,L.Lin,andX.Wang.2016. Learningnatural language inference using bidirectional LSTM model and inner-attention. ArXiv. Nostalgebraist. 2020. Interpreting gpt: the logit lens. White paper. Sukhbaatar, S., A. Szlam, J. Weston, and R. Fergus. 2015. End-to-endmemorynetworks. NeurIPS. Uszkoreit,J.2017. Transformer: Anovelneuralnetworkar- chitecture for language understanding. Google Research blogpost,ThursdayAugust31,2017. Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Atten- tionisallyouneed. NeurIPS. Weston, J., S. Chopra, and A. Bordes. 2015. Memory net- works. ICLR2015.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_results = []\n",
    "for section in sections:\n",
    "    summarized_section = summarize_text(section)\n",
    "    qa_pair = generate_qa(summarized_section)\n",
    "    similarity = compute_similarity(qa_pair, summarized_section)\n",
    "    qa_results.append({\"summary\": summarized_section, \"qa_pair\": qa_pair, \"similarity\": similarity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Draft of August 20, 2024. CHAPTER 9 The Transformer “The true art of memory is the art of attention ” Samuel Johnson, Idler #74, September 1759.\n",
      "QA Pair: What is the true art of memory? the art of attention\n",
      "Relevance Score: 0.7084\n",
      "--------------------------------------------------\n",
      "Summary: In Chapter 23, we look at how language models are pretrained and how tokens are generated via sampling. Chapter 11 introduces masked language modeling and the BERT family of bidirectional transformer encoder models. Chapter 13 will introduce machine translation with the encoder-decoder architecture.\n",
      "QA Pair: In Chapter 23, we look at how language models are pretrained and how tokens are generated via what? sampling\n",
      "Relevance Score: 0.8649\n",
      "--------------------------------------------------\n",
      "Summary: Transformers can build contextual representations of word meaning, contex- contextual tual embeddings, by integrating the meaning of helpful contextual words. At each layer, we compute the representation of a token i by combining information about i from the previous layer with infor- mation about the neighboring tokens to produce a contextualized representation for each word at each position.\n",
      "QA Pair: What do Transformers build? contextual representations of word meaning, contex- contextual tual embeddings\n",
      "Relevance Score: 0.8486\n",
      "--------------------------------------------------\n",
      "Summary: In causal, left-to-right language models, the context is any of the prior words. When processing each input x i, the model attends to all the inputs up to, and including x i. The output of attention is a sum of the embeddings of prior tokens weighted by their similarity with the current token embedding.\n",
      "QA Pair: What model takes all inputs into account when processing each input x i? the model\n",
      "Relevance Score: 0.8556\n",
      "--------------------------------------------------\n",
      "Summary: The simplified attention in equations 9.6 – 9.8 demonstrates the attention-based approach to computing a. But other context words may also be similar to i, and the softmax will also assign some weight to those words. Then we use these weights as the α values in Eq.9.6 to compute the weighted sum that is our a .\n",
      "QA Pair: What is the simplified attention in equation 9.6 – 9.8 demonstrating? attention-based approach to computing a\n",
      "Relevance Score: 0.8752\n",
      "--------------------------------------------------\n",
      "Summary: i Here’s a final set of equations for computing self-attention for a single self- attention output vector a from a single input vector x . This version of attention i i computes a by summing the values of the prior elements, each weighted by the i similarity of its key to the query from the current element. We illustrate this in Fig. 9.4 for the case of calculating the value of the third output a in a sequence.\n",
      "QA Pair: How is attention i computed? by summing the values of the prior elements\n",
      "Relevance Score: 0.8632\n",
      "--------------------------------------------------\n",
      "Summary: Each head i in a self-attention layer has its own set of key, query and value matrices: WKi, WQi and WVi. These are used to project the inputs into separate key, value, and query embeddings for each head. When using multiple heads the model dimension d is still used for the input and output. Below are the equations for attention augmented with multiple heads.\n",
      "QA Pair: What are used to project the inputs into separate key, value, and query embeddings for each head? WKi, WQi and WVi\n",
      "Relevance Score: 0.8886\n",
      "--------------------------------------------------\n",
      "Summary: In the resid- ual stream viewpoint, we consider the processing of an individual token i through the transformer block as a single stream of d-dimensional representations for token position i. The input at the bottom of the stream is an embedding for a token, which has dimensionality d. This initial embedding gets passed up (by residual connections), and is progressively added to by the other components of the transformer.\n",
      "QA Pair: What is the input at the bottom of the stream? an embedding for a token, which has dimensionality d\n",
      "Relevance Score: 0.8742\n",
      "--------------------------------------------------\n",
      "Summary: This process, called layer norm (short for layer normalization), is one of many forms of normalization that can be used to improve training performance in deep neural networks. Layer norm is a variation of the z-score from statistics, applied to a singlevec- tor in a hidden layer.\n",
      "QA Pair: What is a variation of the z-score from statistics? Layer norm\n",
      "Relevance Score: 0.8216\n",
      "--------------------------------------------------\n",
      "Summary: Crucially, the input and output dimensions of transformer blocks are matched so they can be stacked. Transformers for large language i models stack many of these blocks, from 12 layers to 96 layers. At the very end of the last (highest) transformer block, there is a single extra layer norm that is run on the last h of each token stream.\n",
      "QA Pair: How many layers are in a large language i model? 96 layers\n",
      "Relevance Score: 0.7646\n",
      "--------------------------------------------------\n",
      "Summary: For one head we multiply X by the key, query, and value matrices WQ of shape [d×d ] and WV of shape (9.31) Given these matrices we can compute all the requisite query-key comparisons by multiplying Q and K(cid:124) in a single matrix multiplication. The product is of shape N×N, visualized in Fig. 9.8.\n",
      "QA Pair: What shape is the product of? NN\n",
      "Relevance Score: 0.7512\n",
      "--------------------------------------------------\n",
      "Summary: This makes it expensive to compute attention over very long documents (like entire novels) Nonetheless modern large language models manage to use quite long contexts of thousands or tens of thousands of tokens. In multi-head attention, as with self-attention, the input and output have the model dimension d.\n",
      "QA Pair: What model dimension do the input and output of multi-head attention have? the model dimension d\n",
      "Relevance Score: 0.8300\n",
      "--------------------------------------------------\n",
      "Summary: To make use of these matrices in further processing, they are concatenated to produce a single output with dimensionality N ×hd . Finally, we use yet another linear v projection WO ∉ Rhdv×d, that reshape it to the original output dimension for each token.\n",
      "QA Pair: What does WO  Rhdvd do? reshape it to the original output dimension for each token.\n",
      "Relevance Score: 0.8353\n",
      "--------------------------------------------------\n",
      "Summary: We use indexing to select the corresponding rows from E, (row 5, row 4000, row 10532, row 2224) Another way to think about selecting token embeddings is to represent tokens as one-hot vectors of shape [1×|V|]. We can extend this idea to represent the entire token sequence as a matrix of one- hot vectors, one for each of the N positions in the transformer's context window.\n",
      "QA Pair: What is the matrix of one hot vectors? one for each of the N positions in the transformer's context window\n",
      "Relevance Score: 0.8894\n",
      "--------------------------------------------------\n",
      "Summary: To produce an input embedding that captures positional information, we just add the word embedding for each input to its corresponding positional embedding. The language modeling head is the circuitry we need to do language modeling.\n",
      "QA Pair: What is the word embedding for each input to its corresponding positional embedding? positional embedding\n",
      "Relevance Score: 0.8949\n",
      "--------------------------------------------------\n",
      "Summary: Transformer language models compute the probability of a word given counts of its occurrence with the n−1 prior words. The job of the language modeling head is to take the output of the final trans- former layer from the last token N and use it to predict the upcoming word at posi- tion N+1.\n",
      "QA Pair: What word model uses a final trans- former layer to predict the word at posi- tion N+1? Transformer language models\n",
      "Relevance Score: 0.9036\n",
      "--------------------------------------------------\n",
      "Summary: A softmax layer turns the logits u into the probabilities y over the vocabulary. We can use these probabilities to do things like help assign a probability to a given text. But the most important usage to generate text, which we do by sampling, is to generate a word.\n",
      "QA Pair: What does a softmax layer turn the logits u into? probabilities y over the vocabulary\n",
      "Relevance Score: 0.8633\n",
      "--------------------------------------------------\n",
      "Summary: 9.6 Summary This chapter has introduced the transformer and its components for the task of lan- guage modeling. The transformer (Vaswani et al., 2017) was developed drawing on two lines of prior research: self-attention and memory networks. Other aspects of the transformer came from memory networks, a mechanism for adding an external read.\n",
      "QA Pair: What is the purpose of the memory networks? adding an external read\n",
      "Relevance Score: 0.7286\n",
      "--------------------------------------------------\n",
      "Summary: Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Clark, J. Kaplan, S. McCan- dlish, and C. Olah. 2021. A mathematical framework for transformercircuits.\n",
      "QA Pair: What is the name of the paper that was published in 2021? A mathematical framework for transformercircuits.\n",
      "Relevance Score: 0.7920\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for result in qa_results:\n",
    "    print(f\"Summary: {result['summary']}\")\n",
    "    print(f\"QA Pair: {result['qa_pair']}\")\n",
    "    print(f\"Relevance Score: {result['similarity']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary': 'ELMo representations are a function of all internal layers of biLM. Linear combination of the vectors stacked above each input word Improves performance over just using the top LSTM layerTransfer Learning Natural Language Processing with Deep Learning CS224N/Ling284, Chris Manning, StanfordULMfit Universal Language Model.'},\n",
       " {'summary': 'Intuition: a representation of meaning of a word should be different in different contexts. Each word has a different vector that expresses different meanings depending on the surrounding words. We say that a word \"attends to\" some neighboring words more than others.'},\n",
       " {'summary': 'We’ve given the intuition of self-attention (as a way to compute representations of a) The core intuition of attention is the idea of comparing an item of interest to a word at a given layer by integrating information from words at the previous layer. For example, returning Chapter 9.2, the computation of a is based on a set of comparisons between the 3 collection of other items in a way that reveals their relevance in the current context.'},\n",
       " {'summary': 'Let’s refer to the resuGlitveonf ath siesqucoenmcpe aorfi stooknenb eetmwbeeedndiwngosr:ds i and j as a the value the more similar the vectors that are being compared. The result of a dot product is a scalar value ranging from − • to • , the larger exp(score(x), x) is. Given the proportional scores in a , we generate an output value a by summing iIntuition of attention: test x1 x2 x3 x4 x5 x6 x7 xi'},\n",
       " {'summary': \"Intuition of attention: test x1 x2 x3 x4 x5 x6 x7 xiAn Actual Attention Head: slightly more complicated. Instead of using vectors (like x and x ) i 4 directly, we'll represent 3 separate roles each vector x plays: i.\"},\n",
       " {'summary': \"Attention can also be viewed as a way to move information from one token to another. We'll see in the next lecture that attention is quadratic in length. We can pack the N tokens of the input sequence into a single matrix X of size [N × d].\"},\n",
       " {'summary': 'Each word is a row vector of d dimensions Given: string \"Thanks for all the\" Tokenize with BPE and convert into vocab indices w = [5,4000,10532,2224] Each row is an embedding (row 5, row 4000, row 10532, row 2224) Position embeddings are learned along with other parameters during training.'},\n",
       " {'summary': '9.14 shows how to accomplish this task, taking the output of the last token at the last layer and producing a probability distribution over words. The first module in Fig. 9.14 is a linear layer, whose job is to project from the L output h to the score vector u.'},\n",
       " {'summary': 'We can use these probabilities to do things like help assign a probability to a given text. But the most important usage to generate text, which we do by sampling, is to generate a text.'}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"hf://datasets/soufyane/DATA_SCIENCE_QA/data (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What is under-fitting and overfitting in machi...</td>\n",
       "      <td>Underfitting is when a model is too simple, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Can you explain what a false positive and a fa...</td>\n",
       "      <td>A false positive incorrectly indicates a condi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Clarify the concept of Phase IV.</td>\n",
       "      <td>Phase IV studies, also known as post-marketing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What is semi-supervised learning described in ...</td>\n",
       "      <td>Semi-supervised learning integrates both label...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Discuss the parallelization of training in gra...</td>\n",
       "      <td>Parallelizing training of a gradient boosting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>1065</td>\n",
       "      <td>Define the ACID property in SQL and its signif...</td>\n",
       "      <td>ACID principles maintain database integrity by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>1066</td>\n",
       "      <td>What are the different types of data warehouses?</td>\n",
       "      <td>Data warehouses vary by scope and function, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>1067</td>\n",
       "      <td>What are the key stages in a data mining project?</td>\n",
       "      <td>A data mining project starts with understandin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>1068</td>\n",
       "      <td>What is information extraction?</td>\n",
       "      <td>Information extraction systematically identifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>1069</td>\n",
       "      <td>Describe kernel support vector machines (KSVMs).</td>\n",
       "      <td>Kernel Support Vector Machines (KSVMs) are a c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1070 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           Question  \\\n",
       "0              0  What is under-fitting and overfitting in machi...   \n",
       "1              1  Can you explain what a false positive and a fa...   \n",
       "2              2                   Clarify the concept of Phase IV.   \n",
       "3              3  What is semi-supervised learning described in ...   \n",
       "4              4  Discuss the parallelization of training in gra...   \n",
       "...          ...                                                ...   \n",
       "1065        1065  Define the ACID property in SQL and its signif...   \n",
       "1066        1066   What are the different types of data warehouses?   \n",
       "1067        1067  What are the key stages in a data mining project?   \n",
       "1068        1068                    What is information extraction?   \n",
       "1069        1069   Describe kernel support vector machines (KSVMs).   \n",
       "\n",
       "                                                 Answer  \n",
       "0     Underfitting is when a model is too simple, an...  \n",
       "1     A false positive incorrectly indicates a condi...  \n",
       "2     Phase IV studies, also known as post-marketing...  \n",
       "3     Semi-supervised learning integrates both label...  \n",
       "4     Parallelizing training of a gradient boosting ...  \n",
       "...                                                 ...  \n",
       "1065  ACID principles maintain database integrity by...  \n",
       "1066  Data warehouses vary by scope and function, wi...  \n",
       "1067  A data mining project starts with understandin...  \n",
       "1068  Information extraction systematically identifi...  \n",
       "1069  Kernel Support Vector Machines (KSVMs) are a c...  \n",
       "\n",
       "[1070 rows x 3 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9decf816fe1486d85d0fce7d04809a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc2d9dc1e9d435088a59615f320058c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ac851a53194c9a8cdc177e94061910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f507e28b2934cc0a46a5269b4e757d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "text = \"Scikit-learn supports K-means clustering.\"\n",
    "print(ner_pipeline(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "text = \"Scikit-learn supports K-means clustering.\"\n",
    "print(ner_pipeline(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
