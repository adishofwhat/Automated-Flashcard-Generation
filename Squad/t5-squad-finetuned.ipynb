{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# pip install peft","metadata":{"id":"jhu64giPyZk3","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T01:50:17.401563Z","iopub.execute_input":"2024-12-07T01:50:17.401891Z","iopub.status.idle":"2024-12-07T01:50:27.645445Z","shell.execute_reply.started":"2024-12-07T01:50:17.401864Z","shell.execute_reply":"2024-12-07T01:50:27.644402Z"}},"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.14.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq,  EarlyStoppingCallback\nimport torch\nfrom torch.utils.data import Dataset\nfrom peft import LoraConfig, get_peft_model\nimport pandas as pd","metadata":{"id":"dcuyP5csycoz","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T01:50:33.956862Z","iopub.execute_input":"2024-12-07T01:50:33.957223Z","iopub.status.idle":"2024-12-07T01:50:54.772357Z","shell.execute_reply.started":"2024-12-07T01:50:33.957191Z","shell.execute_reply":"2024-12-07T01:50:54.771496Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model_name = \"potsawee/t5-large-generation-squad-QuestionAnswer\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    r=16,  # Rank of low-rank matrices\n    lora_alpha=32,  # Scaling factor\n    target_modules=[\"q\", \"v\"],  # Fine-tune attention layers\n    lora_dropout=0.1,\n    bias=\"none\"\n)","metadata":{"id":"T4xOLUslydpm","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T01:50:59.147291Z","iopub.execute_input":"2024-12-07T01:50:59.148227Z","iopub.status.idle":"2024-12-07T01:51:19.566504Z","shell.execute_reply.started":"2024-12-07T01:50:59.148170Z","shell.execute_reply":"2024-12-07T01:51:19.565700Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33b36525f9b34d0ab686e13313d25ac1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac099ab477ba4ff0b687178b66cea75e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc014c7e3b7e4229afdfb3d133ea3c15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93bd4c2f14f54d8b9737f25946d5b605"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a14dbaa2d66349b5a9e64aa6091fa3e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a82c2e5426848b0a8a89ab84f89d9b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab22c09afeef4ca2859c1f8a301536a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a9f39ba59f246dbb5b91c239a8843b4"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\n# Check trainable parameters\nmodel.print_trainable_parameters()","metadata":{"id":"8Z9nok4myfUO","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T01:51:39.719552Z","iopub.execute_input":"2024-12-07T01:51:39.720259Z","iopub.status.idle":"2024-12-07T01:51:39.937346Z","shell.execute_reply.started":"2024-12-07T01:51:39.720214Z","shell.execute_reply":"2024-12-07T01:51:39.936416Z"}},"outputs":[{"name":"stdout","text":"trainable params: 4,718,592 || all params: 742,386,688 || trainable%: 0.6356\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"model.config.ignore_pad_token_for_loss = True","metadata":{"id":"-8AU24YZyhKh","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T01:51:42.506463Z","iopub.execute_input":"2024-12-07T01:51:42.506836Z","iopub.status.idle":"2024-12-07T01:51:42.511146Z","shell.execute_reply.started":"2024-12-07T01:51:42.506805Z","shell.execute_reply":"2024-12-07T01:51:42.510226Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load dataset using pandas\nsplits = {\n    'train': 'data/train-00000-of-00001.parquet',\n    'validation': 'data/validation-00000-of-00001.parquet',\n    'test': 'data/test-00000-of-00001.parquet'\n}\ntrain_df = pd.read_parquet(\"hf://datasets/allenai/sciq/\" + splits[\"train\"])\nvalidation_df = pd.read_parquet(\n    \"hf://datasets/allenai/sciq/\" + splits[\"validation\"])","metadata":{"id":"JZm24XrEyjEt","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T01:51:48.224075Z","iopub.execute_input":"2024-12-07T01:51:48.224432Z","iopub.status.idle":"2024-12-07T01:51:50.630575Z","shell.execute_reply.started":"2024-12-07T01:51:48.224400Z","shell.execute_reply":"2024-12-07T01:51:50.629853Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def preprocess_function(df):\n    inputs = df[\"support\"].tolist()\n    targets = [q + \" <sep> \" + a for q,\n               a in zip(df[\"question\"], df[\"correct_answer\"])]\n    model_inputs = tokenizer(inputs, max_length=512,\n                             truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=128,\n                       truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n\ntrain_data = preprocess_function(train_df)\nvalidation_data = preprocess_function(validation_df)","metadata":{"id":"HX3shG9iyjAs","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T01:51:58.389250Z","iopub.execute_input":"2024-12-07T01:51:58.389610Z","iopub.status.idle":"2024-12-07T01:52:01.497046Z","shell.execute_reply.started":"2024-12-07T01:51:58.389579Z","shell.execute_reply":"2024-12-07T01:52:01.496296Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class SciQDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n\ntrain_dataset = SciQDataset(train_data)\nvalidation_dataset = SciQDataset(validation_data)","metadata":{"id":"8VgoU3y4ymhz","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T01:52:03.320272Z","iopub.execute_input":"2024-12-07T01:52:03.321152Z","iopub.status.idle":"2024-12-07T01:52:03.326359Z","shell.execute_reply.started":"2024-12-07T01:52:03.321116Z","shell.execute_reply":"2024-12-07T01:52:03.325233Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./t5_lora_sciq\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-4,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=6,\n    weight_decay=0.01,\n    save_total_limit=2,\n    predict_with_generate=True,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    fp16=True,\n    load_best_model_at_end=True,  # Add this line for early stopping\n    metric_for_best_model=\"eval_loss\",  # Specify which metric to use for selecting the best model\n    greater_is_better=False,\n    lr_scheduler_type=\"linear\",\n    warmup_steps=500,\n    label_names=[\"labels\"],\n)\n\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n    callbacks=[early_stopping_callback]  # Add early stopping callback\n)","metadata":{"id":"JPJ0QHmMyoWc","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T01:52:07.312077Z","iopub.execute_input":"2024-12-07T01:52:07.312827Z","iopub.status.idle":"2024-12-07T01:52:09.623508Z","shell.execute_reply.started":"2024-12-07T01:52:07.312791Z","shell.execute_reply":"2024-12-07T01:52:09.622860Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_23/710440967.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"p9N3NEtIyqe3","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T01:52:20.532555Z","iopub.execute_input":"2024-12-07T01:52:20.532945Z","iopub.status.idle":"2024-12-07T07:52:08.985331Z","shell.execute_reply.started":"2024-12-07T01:52:20.532913Z","shell.execute_reply":"2024-12-07T07:52:08.984595Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241207_015544-n3qu4fkg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/kharchesarvesh-rutgers-university/huggingface/runs/n3qu4fkg' target=\"_blank\">./t5_lora_sciq</a></strong> to <a href='https://wandb.ai/kharchesarvesh-rutgers-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/kharchesarvesh-rutgers-university/huggingface' target=\"_blank\">https://wandb.ai/kharchesarvesh-rutgers-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/kharchesarvesh-rutgers-university/huggingface/runs/n3qu4fkg' target=\"_blank\">https://wandb.ai/kharchesarvesh-rutgers-university/huggingface/runs/n3qu4fkg</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8760' max='8760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8760/8760 5:56:20, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.180600</td>\n      <td>0.174946</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.171400</td>\n      <td>0.171466</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.156100</td>\n      <td>0.169538</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.150400</td>\n      <td>0.169569</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.153200</td>\n      <td>0.168435</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.134400</td>\n      <td>0.169165</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=8760, training_loss=0.36786307994633505, metrics={'train_runtime': 21588.0052, 'train_samples_per_second': 3.246, 'train_steps_per_second': 0.406, 'total_flos': 1.5272928291166618e+17, 'train_loss': 0.36786307994633505, 'epoch': 6.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"model.save_pretrained(\"./t5squad_finetuned_sciq\")\ntokenizer.save_pretrained(\"./t5squad_finetuned_sciq\")","metadata":{"id":"FJvWABBpyt7m","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:52:08.986995Z","iopub.execute_input":"2024-12-07T07:52:08.987871Z","iopub.status.idle":"2024-12-07T07:52:09.244971Z","shell.execute_reply.started":"2024-12-07T07:52:08.987825Z","shell.execute_reply":"2024-12-07T07:52:09.244115Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('./t5squad_finetuned_sciq/tokenizer_config.json',\n './t5squad_finetuned_sciq/special_tokens_map.json',\n './t5squad_finetuned_sciq/spiece.model',\n './t5squad_finetuned_sciq/added_tokens.json',\n './t5squad_finetuned_sciq/tokenizer.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"!zip -r t5squad_finetuned_sciq.zip ./t5squad_finetuned_sciq\nfrom IPython.display import FileLink\nFileLink(r't5squad_finetuned_sciq.zip')","metadata":{"id":"y8tmEeCqyv-3","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:52:09.246051Z","iopub.execute_input":"2024-12-07T07:52:09.246324Z","iopub.status.idle":"2024-12-07T07:52:11.451381Z","shell.execute_reply.started":"2024-12-07T07:52:09.246298Z","shell.execute_reply":"2024-12-07T07:52:11.450619Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: t5squad_finetuned_sciq/ (stored 0%)\n  adding: t5squad_finetuned_sciq/tokenizer.json (deflated 74%)\n  adding: t5squad_finetuned_sciq/added_tokens.json (stored 0%)\n  adding: t5squad_finetuned_sciq/special_tokens_map.json (deflated 86%)\n  adding: t5squad_finetuned_sciq/adapter_model.safetensors (deflated 7%)\n  adding: t5squad_finetuned_sciq/adapter_config.json (deflated 53%)\n  adding: t5squad_finetuned_sciq/tokenizer_config.json (deflated 95%)\n  adding: t5squad_finetuned_sciq/README.md (deflated 66%)\n  adding: t5squad_finetuned_sciq/spiece.model (deflated 48%)\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/t5squad_finetuned_sciq.zip","text/html":"<a href='t5squad_finetuned_sciq.zip' target='_blank'>t5squad_finetuned_sciq.zip</a><br>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}