{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install peft","metadata":{"id":"jhu64giPyZk3","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:33:47.511424Z","iopub.execute_input":"2024-12-09T20:33:47.511852Z","iopub.status.idle":"2024-12-09T20:33:57.534392Z","shell.execute_reply.started":"2024-12-09T20:33:47.511815Z","shell.execute_reply":"2024-12-09T20:33:57.533307Z"}},"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.14.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback\nimport torch\nfrom torch.utils.data import Dataset\nfrom peft import LoraConfig, get_peft_model\nimport pandas as pd\nimport nltk","metadata":{"id":"dcuyP5csycoz","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:33:57.536656Z","iopub.execute_input":"2024-12-09T20:33:57.537327Z","iopub.status.idle":"2024-12-09T20:34:18.822965Z","shell.execute_reply.started":"2024-12-09T20:33:57.537283Z","shell.execute_reply":"2024-12-09T20:34:18.822232Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model_name = \"potsawee/t5-large-generation-squad-QuestionAnswer\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    r=16,  # Rank of low-rank matrices\n    lora_alpha=32,  # Scaling factor\n    target_modules=[\"q\", \"v\"],  # Fine-tune attention layers\n    lora_dropout=0.1,\n    bias=\"none\"\n)","metadata":{"id":"T4xOLUslydpm","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:34:18.823877Z","iopub.execute_input":"2024-12-09T20:34:18.824373Z","iopub.status.idle":"2024-12-09T20:34:35.127391Z","shell.execute_reply.started":"2024-12-09T20:34:18.824346Z","shell.execute_reply":"2024-12-09T20:34:35.126499Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bff2047e8794132ba706ca996fedddb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b098746539a142f185422c99648a7f99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ec2a29133414b2fa49cfc4c03331f31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fb8347935784f90ab59ea87f16bc9fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cce63f96f55476aa5d947a35768cffe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"752b1474d01347b9a29aebb7439fdbd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c8a96cb1577498b9e2762bc1f260ad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9dee1936a89427484cdd37b07035f85"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\n# Check trainable parameters\nmodel.print_trainable_parameters()","metadata":{"id":"8Z9nok4myfUO","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:34:35.129030Z","iopub.execute_input":"2024-12-09T20:34:35.129280Z","iopub.status.idle":"2024-12-09T20:34:35.347987Z","shell.execute_reply.started":"2024-12-09T20:34:35.129255Z","shell.execute_reply":"2024-12-09T20:34:35.347158Z"}},"outputs":[{"name":"stdout","text":"trainable params: 4,718,592 || all params: 742,386,688 || trainable%: 0.6356\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"model.config.ignore_pad_token_for_loss = True","metadata":{"id":"-8AU24YZyhKh","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:34:35.348888Z","iopub.execute_input":"2024-12-09T20:34:35.349116Z","iopub.status.idle":"2024-12-09T20:34:45.719377Z","shell.execute_reply.started":"2024-12-09T20:34:35.349094Z","shell.execute_reply":"2024-12-09T20:34:45.717974Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5913759572944f688efcff0b061ff060"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Load dataset using pandas\nsplits = {\n    'train': 'data/train-00000-of-00001.parquet',\n    'validation': 'data/validation-00000-of-00001.parquet',\n    'test': 'data/test-00000-of-00001.parquet'\n}\ntrain_df = pd.read_parquet(\"hf://datasets/allenai/sciq/\" + splits[\"train\"])\nvalidation_df = pd.read_parquet(\n    \"hf://datasets/allenai/sciq/\" + splits[\"validation\"])","metadata":{"id":"JZm24XrEyjEt","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:34:45.720684Z","iopub.execute_input":"2024-12-09T20:34:45.721148Z","iopub.status.idle":"2024-12-09T20:34:47.862089Z","shell.execute_reply.started":"2024-12-09T20:34:45.721117Z","shell.execute_reply":"2024-12-09T20:34:47.861303Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Download NLTK data\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('maxent_ne_chunker')\nnltk.download('words')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:34:47.863589Z","iopub.execute_input":"2024-12-09T20:34:47.863937Z","iopub.status.idle":"2024-12-09T20:34:48.267932Z","shell.execute_reply.started":"2024-12-09T20:34:47.863898Z","shell.execute_reply":"2024-12-09T20:34:48.266640Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to /usr/share/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def preprocess_function_with_nltk(df):\n    inputs = []\n    count = 1\n    for text in df[\"support\"].tolist():\n        tokens = nltk.word_tokenize(text)\n        pos_tags = nltk.pos_tag(tokens)\n        pos_str = \" \".join([f\"{word}/{pos}\" for word, pos in pos_tags])\n        ner_tree = nltk.ne_chunk(pos_tags)\n        ner_tags = \" \".join([f\"{' '.join(c[0] for c in subtree)}({subtree.label()})\" if isinstance(subtree, nltk.Tree) else f\"{subtree[0]}\" for subtree in ner_tree])\n        enriched_input = f\"{text}\\nNER: {ner_tags}\\nPOS: {pos_str}\"\n        if count<=5:\n            print(enriched_input)\n            print('_'*40)\n            count+=1\n        inputs.append(enriched_input)\n    \n    targets = [q + \" <sep> \" + a for q, a in zip(df[\"question\"], df[\"correct_answer\"])]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:34:48.269276Z","iopub.execute_input":"2024-12-09T20:34:48.269664Z","iopub.status.idle":"2024-12-09T20:34:49.043116Z","shell.execute_reply.started":"2024-12-09T20:34:48.269618Z","shell.execute_reply":"2024-12-09T20:34:49.042353Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_data = preprocess_function_with_nltk(train_df)\nvalidation_data = preprocess_function_with_nltk(validation_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:34:49.044162Z","iopub.execute_input":"2024-12-09T20:34:49.044488Z","iopub.status.idle":"2024-12-09T20:38:00.849518Z","shell.execute_reply.started":"2024-12-09T20:34:49.044457Z","shell.execute_reply":"2024-12-09T20:38:00.848768Z"}},"outputs":[{"name":"stdout","text":"Mesophiles grow best in moderate temperature, typically between 25°C and 40°C (77°F and 104°F). Mesophiles are often found living in or on the bodies of humans or other animals. The optimal growth temperature of many pathogenic mesophiles is 37°C (98°F), the normal human body temperature. Mesophilic organisms have important uses in food preparation, including cheese, yogurt, beer and wine.\nNER: Mesophiles grow best in moderate temperature , typically between 25°C and 40°C ( 77°F and 104°F ) . Mesophiles are often found living in or on the bodies of humans or other animals . The optimal growth temperature of many pathogenic mesophiles is 37°C ( 98°F ) , the normal human body temperature . Mesophilic(ORGANIZATION) organisms have important uses in food preparation , including cheese , yogurt , beer and wine .\nPOS: Mesophiles/NNS grow/VBP best/JJS in/IN moderate/JJ temperature/NN ,/, typically/RB between/IN 25°C/CD and/CC 40°C/CD (/( 77°F/CD and/CC 104°F/CD )/) ./. Mesophiles/NNS are/VBP often/RB found/VBN living/NN in/IN or/CC on/IN the/DT bodies/NNS of/IN humans/NNS or/CC other/JJ animals/NNS ./. The/DT optimal/JJ growth/NN temperature/NN of/IN many/JJ pathogenic/JJ mesophiles/NNS is/VBZ 37°C/CD (/( 98°F/CD )/) ,/, the/DT normal/JJ human/JJ body/NN temperature/NN ./. Mesophilic/NNP organisms/NNS have/VBP important/JJ uses/NNS in/IN food/NN preparation/NN ,/, including/VBG cheese/NN ,/, yogurt/NN ,/, beer/NN and/CC wine/NN ./.\n________________________________________\nWithout Coriolis Effect the global winds would blow north to south or south to north. But Coriolis makes them blow northeast to southwest or the reverse in the Northern Hemisphere. The winds blow northwest to southeast or the reverse in the southern hemisphere.\nNER: Without Coriolis(ORGANIZATION) Effect the global winds would blow north to south or south to north . But Coriolis(ORGANIZATION) makes them blow northeast to southwest or the reverse in the Northern Hemisphere(LOCATION) . The winds blow northwest to southeast or the reverse in the southern hemisphere .\nPOS: Without/IN Coriolis/NNP Effect/NNP the/DT global/JJ winds/NNS would/MD blow/VB north/NN to/TO south/VB or/CC south/VB to/TO north/JJ ./. But/CC Coriolis/NNP makes/VBZ them/PRP blow/VB northeast/NN to/TO southwest/VB or/CC the/DT reverse/NN in/IN the/DT Northern/NNP Hemisphere/NNP ./. The/DT winds/NNS blow/VBP northwest/JJS to/TO southeast/VB or/CC the/DT reverse/NN in/IN the/DT southern/JJ hemisphere/NN ./.\n________________________________________\nSummary Changes of state are examples of phase changes, or phase transitions. All phase changes are accompanied by changes in the energy of a system. Changes from a more-ordered state to a less-ordered state (such as a liquid to a gas) areendothermic. Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always exothermic. The conversion of a solid to a liquid is called fusion (or melting). The energy required to melt 1 mol of a substance is its enthalpy of fusion (ΔHfus). The energy change required to vaporize 1 mol of a substance is the enthalpy of vaporization (ΔHvap). The direct conversion of a solid to a gas is sublimation. The amount of energy needed to sublime 1 mol of a substance is its enthalpy of sublimation (ΔHsub) and is the sum of the enthalpies of fusion and vaporization. Plots of the temperature of a substance versus heat added or versus heating time at a constant rate of heating are calledheating curves. Heating curves relate temperature changes to phase transitions. A superheated liquid, a liquid at a temperature and pressure at which it should be a gas, is not stable. A cooling curve is not exactly the reverse of the heating curve because many liquids do not freeze at the expected temperature. Instead, they form a supercooled liquid, a metastable liquid phase that exists below the normal melting point. Supercooled liquids usually crystallize on standing, or adding a seed crystal of the same or another substance can induce crystallization.\nNER: Summary Changes of state are examples of phase changes , or phase transitions . All phase changes are accompanied by changes in the energy of a system . Changes from a more-ordered state to a less-ordered state ( such as a liquid to a gas ) areendothermic . Changes from a less-ordered state to a more-ordered state ( such as a liquid to a solid ) are always exothermic . The conversion of a solid to a liquid is called fusion ( or melting ) . The energy required to melt 1 mol of a substance is its enthalpy of fusion ( ΔHfus(ORGANIZATION) ) . The energy change required to vaporize 1 mol of a substance is the enthalpy of vaporization ( ΔHvap(ORGANIZATION) ) . The direct conversion of a solid to a gas is sublimation . The amount of energy needed to sublime 1 mol of a substance is its enthalpy of sublimation ( ΔHsub(ORGANIZATION) ) and is the sum of the enthalpies of fusion and vaporization . Plots(PERSON) of the temperature of a substance versus heat added or versus heating time at a constant rate of heating are calledheating curves . Heating curves relate temperature changes to phase transitions . A superheated liquid , a liquid at a temperature and pressure at which it should be a gas , is not stable . A cooling curve is not exactly the reverse of the heating curve because many liquids do not freeze at the expected temperature . Instead , they form a supercooled liquid , a metastable liquid phase that exists below the normal melting point . Supercooled liquids usually crystallize on standing , or adding a seed crystal of the same or another substance can induce crystallization .\nPOS: Summary/JJ Changes/NNS of/IN state/NN are/VBP examples/NNS of/IN phase/NN changes/NNS ,/, or/CC phase/NN transitions/NNS ./. All/DT phase/NN changes/NNS are/VBP accompanied/VBN by/IN changes/NNS in/IN the/DT energy/NN of/IN a/DT system/NN ./. Changes/NNS from/IN a/DT more-ordered/JJ state/NN to/TO a/DT less-ordered/JJ state/NN (/( such/JJ as/IN a/DT liquid/NN to/TO a/DT gas/NN )/) areendothermic/NN ./. Changes/NNS from/IN a/DT less-ordered/JJ state/NN to/TO a/DT more-ordered/JJ state/NN (/( such/JJ as/IN a/DT liquid/NN to/TO a/DT solid/JJ )/) are/VBP always/RB exothermic/JJ ./. The/DT conversion/NN of/IN a/DT solid/JJ to/TO a/DT liquid/NN is/VBZ called/VBN fusion/NN (/( or/CC melting/VBG )/) ./. The/DT energy/NN required/VBN to/TO melt/VB 1/CD mol/NN of/IN a/DT substance/NN is/VBZ its/PRP$ enthalpy/NN of/IN fusion/NN (/( ΔHfus/NNP )/) ./. The/DT energy/NN change/NN required/VBN to/TO vaporize/VB 1/CD mol/NN of/IN a/DT substance/NN is/VBZ the/DT enthalpy/NN of/IN vaporization/NN (/( ΔHvap/NNP )/) ./. The/DT direct/JJ conversion/NN of/IN a/DT solid/JJ to/TO a/DT gas/NN is/VBZ sublimation/NN ./. The/DT amount/NN of/IN energy/NN needed/VBN to/TO sublime/VB 1/CD mol/NN of/IN a/DT substance/NN is/VBZ its/PRP$ enthalpy/NN of/IN sublimation/NN (/( ΔHsub/NNP )/) and/CC is/VBZ the/DT sum/NN of/IN the/DT enthalpies/NNS of/IN fusion/NN and/CC vaporization/NN ./. Plots/NNP of/IN the/DT temperature/NN of/IN a/DT substance/NN versus/NN heat/NN added/VBD or/CC versus/IN heating/JJ time/NN at/IN a/DT constant/JJ rate/NN of/IN heating/NN are/VBP calledheating/VBG curves/NNS ./. Heating/VBG curves/NNS relate/VBP temperature/NN changes/NNS to/TO phase/VB transitions/NNS ./. A/DT superheated/JJ liquid/NN ,/, a/DT liquid/NN at/IN a/DT temperature/NN and/CC pressure/NN at/IN which/WDT it/PRP should/MD be/VB a/DT gas/NN ,/, is/VBZ not/RB stable/JJ ./. A/DT cooling/NN curve/NN is/VBZ not/RB exactly/RB the/DT reverse/NN of/IN the/DT heating/NN curve/NN because/IN many/JJ liquids/NNS do/VBP not/RB freeze/VB at/IN the/DT expected/JJ temperature/NN ./. Instead/RB ,/, they/PRP form/VBP a/DT supercooled/JJ liquid/NN ,/, a/DT metastable/JJ liquid/NN phase/NN that/WDT exists/VBZ below/IN the/DT normal/JJ melting/NN point/NN ./. Supercooled/JJ liquids/NNS usually/RB crystallize/VBP on/IN standing/NN ,/, or/CC adding/VBG a/DT seed/NN crystal/NN of/IN the/DT same/JJ or/CC another/DT substance/NN can/MD induce/VB crystallization/NN ./.\n________________________________________\nAll radioactive decay is dangerous to living things, but alpha decay is the least dangerous.\nNER: All radioactive decay is dangerous to living things , but alpha decay is the least dangerous .\nPOS: All/DT radioactive/JJ decay/NN is/VBZ dangerous/JJ to/TO living/VBG things/NNS ,/, but/CC alpha/JJ decay/NN is/VBZ the/DT least/JJS dangerous/JJ ./.\n________________________________________\nExample 3.5 Calculating Projectile Motion: Hot Rock Projectile Kilauea in Hawaii is the world’s most continuously active volcano. Very active volcanoes characteristically eject red-hot rocks and lava rather than smoke and ash. Suppose a large rock is ejected from the volcano with a speed of 25.0 m/s and at an angle 35.0º above the horizontal, as shown in Figure 3.40. The rock strikes the side of the volcano at an altitude 20.0 m lower than its starting point. (a) Calculate the time it takes the rock to follow this path. (b) What are the magnitude and direction of the rock’s velocity at impact?.\nNER: Example 3.5 Calculating Projectile Motion : Hot Rock(PERSON) Projectile Kilauea in Hawaii(GPE) is the world ’ s most continuously active volcano . Very active volcanoes characteristically eject red-hot rocks and lava rather than smoke and ash . Suppose a large rock is ejected from the volcano with a speed of 25.0 m/s and at an angle 35.0º above the horizontal , as shown in Figure 3.40 . The rock strikes the side of the volcano at an altitude 20.0 m lower than its starting point . ( a ) Calculate the time it takes the rock to follow this path . ( b ) What are the magnitude and direction of the rock ’ s velocity at impact ? .\nPOS: Example/RB 3.5/CD Calculating/VBG Projectile/NNP Motion/NN :/: Hot/NNP Rock/NNP Projectile/NNP Kilauea/NNP in/IN Hawaii/NNP is/VBZ the/DT world/NN ’/NNP s/VBZ most/JJS continuously/RB active/JJ volcano/NN ./. Very/RB active/JJ volcanoes/NNS characteristically/RB eject/VBP red-hot/JJ rocks/NNS and/CC lava/VBZ rather/RB than/IN smoke/NN and/CC ash/NN ./. Suppose/VB a/DT large/JJ rock/NN is/VBZ ejected/VBN from/IN the/DT volcano/NN with/IN a/DT speed/NN of/IN 25.0/CD m/s/NN and/CC at/IN an/DT angle/NN 35.0º/CD above/IN the/DT horizontal/NN ,/, as/IN shown/VBN in/IN Figure/NNP 3.40/CD ./. The/DT rock/NN strikes/VBZ the/DT side/NN of/IN the/DT volcano/NN at/IN an/DT altitude/NN 20.0/CD m/NN lower/JJR than/IN its/PRP$ starting/VBG point/NN ./. (/( a/DT )/) Calculate/VB the/DT time/NN it/PRP takes/VBZ the/DT rock/NN to/TO follow/VB this/DT path/NN ./. (/( b/NN )/) What/WP are/VBP the/DT magnitude/NN and/CC direction/NN of/IN the/DT rock/NN ’/NNP s/VBZ velocity/NN at/IN impact/NN ?/. ./.\n________________________________________\n\nNER: \nPOS: \n________________________________________\n\nNER: \nPOS: \n________________________________________\nA frameshift mutation is a deletion or insertion of one or more nucleotides that changes the reading frame of the base sequence. Deletions remove nucleotides, and insertions add nucleotides. Consider the following sequence of bases in RNA:.\nNER: A frameshift mutation is a deletion or insertion of one or more nucleotides that changes the reading frame of the base sequence . Deletions remove nucleotides , and insertions add nucleotides . Consider the following sequence of bases in RNA : .\nPOS: A/DT frameshift/NN mutation/NN is/VBZ a/DT deletion/NN or/CC insertion/NN of/IN one/CD or/CC more/JJR nucleotides/NNS that/IN changes/VBZ the/DT reading/NN frame/NN of/IN the/DT base/NN sequence/NN ./. Deletions/NNS remove/VB nucleotides/NNS ,/, and/CC insertions/NNS add/VBP nucleotides/NNS ./. Consider/VB the/DT following/JJ sequence/NN of/IN bases/NNS in/IN RNA/NNP :/: ./.\n________________________________________\nA wetland is an area that is wet for all or part of the year. Wetlands are home to certain types of plants.\nNER: A wetland is an area that is wet for all or part of the year . Wetlands are home to certain types of plants .\nPOS: A/DT wetland/NN is/VBZ an/DT area/NN that/WDT is/VBZ wet/JJR for/IN all/DT or/CC part/NN of/IN the/DT year/NN ./. Wetlands/NNS are/VBP home/VBN to/TO certain/JJ types/NNS of/IN plants/NNS ./.\n________________________________________\n\nNER: \nPOS: \n________________________________________\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class SciQDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.encodings[\"input_ids\"][idx]),\n            \"attention_mask\": torch.tensor(self.encodings[\"attention_mask\"][idx]),\n            \"labels\": torch.tensor(self.encodings[\"labels\"][idx]),\n        }\n\ntrain_dataset = SciQDataset(train_data)\nvalidation_dataset = SciQDataset(validation_data)","metadata":{"id":"8VgoU3y4ymhz","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:38:00.851829Z","iopub.execute_input":"2024-12-09T20:38:00.852102Z","iopub.status.idle":"2024-12-09T20:38:00.857340Z","shell.execute_reply.started":"2024-12-09T20:38:00.852077Z","shell.execute_reply":"2024-12-09T20:38:00.856447Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Print a sample from the dataset\nprint(\"Sample from train_dataset:\", train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:38:00.858330Z","iopub.execute_input":"2024-12-09T20:38:00.858616Z","iopub.status.idle":"2024-12-09T20:38:00.881974Z","shell.execute_reply.started":"2024-12-09T20:38:00.858590Z","shell.execute_reply":"2024-12-09T20:38:00.881114Z"}},"outputs":[{"name":"stdout","text":"Sample from train_dataset: {'input_ids': tensor([10162, 21144,    15,     7,  1604,   200,    16,  8107,  2912,     6,\n         3115,   344,   944,  1956,   254,    11,  1283,  1956,   254,    41,\n         4013,  1956,   371,    11,     3, 15442,  1956,   371,   137, 10162,\n        21144,    15,     7,    33,   557,   435,   840,    16,    42,    30,\n            8,  5678,    13,  6917,    42,   119,  3127,     5,    37,  6624,\n         1170,  2912,    13,   186,  2071, 20853,   140,     7, 21144,    15,\n            7,    19,  6862,  1956,   254,    41,  3916,  1956,   371,   201,\n            8,  1389,   936,   643,  2912,     5, 10162, 21144,   447,  9329,\n            7,    43,   359,  2284,    16,   542,  4537,     6,   379,  3285,\n            6, 19168,     6,  6061,    11,  2013,     5,     3, 18206,    10,\n        10162, 21144,    15,     7,  1604,   200,    16,  8107,  2912,     3,\n            6,  3115,   344,   944,  1956,   254,    11,  1283,  1956,   254,\n           41,     3,  4013,  1956,   371,    11,     3, 15442,  1956,   371,\n            3,    61,     3,     5, 10162, 21144,    15,     7,    33,   557,\n          435,   840,    16,    42,    30,     8,  5678,    13,  6917,    42,\n          119,  3127,     3,     5,    37,  6624,  1170,  2912,    13,   186,\n         2071, 20853,   140,     7, 21144,    15,     7,    19,  6862,  1956,\n          254,    41,     3,  3916,  1956,   371,     3,    61,     3,     6,\n            8,  1389,   936,   643,  2912,     3,     5, 10162, 21144,   447,\n          599,  2990,   517, 20629,   956,  8015,    61,  9329,     7,    43,\n          359,  2284,    16,   542,  4537,     3,     6,   379,  3285,     3,\n            6, 19168,     3,     6,  6061,    11,  2013,     3,     5,     3,\n        16034,    10, 10162, 21144,    15,     7,    87, 17235,   134,  1604,\n           87,   553, 11165,   200,    87,   683, 23787,    16,    87,  3162,\n         8107,    87,   683,   683,  2912,    87, 17235,     3,     6,    87,\n            6,  3115,    87, 12108,   344,    87,  3162,   944,  1956,   254,\n           87,  6931,    11,    87,  2823,  1283,  1956,   254,    87,  6931,\n           41,    87,   599,     3,  4013,  1956,   371,    87,  6931,    11,\n           87,  2823,     3, 15442,  1956,   371,    87,  6931,     3,    61,\n           87,    61,     3,     5,    87,     5, 10162, 21144,    15,     7,\n           87, 17235,   134,    33,    87,   553, 11165,   557,    87, 12108,\n          435,    87,   553, 19174,   840,    87, 17235,    16,    87,  3162,\n           42,    87,  2823,    30,    87,  3162,     8,    87, 12111,  5678,\n           87, 17235,   134,    13,    87,  3162,  6917,    87, 17235,   134,\n           42,    87,  2823,   119,    87,   683,   683,  3127,    87, 17235,\n          134,     3,     5,    87,     5,    37,    87, 12111,  6624,    87,\n          683,   683,  1170,    87, 17235,  2912,    87, 17235,    13,    87,\n         3162,   186,    87,   683,   683,  2071, 20853,    87,   683,   683,\n          140,     7, 21144,    15,     7,    87, 17235,   134,    19,    87,\n        22086,   956,  6862,  1956,   254,    87,  6931,    41,    87,   599,\n            3,  3916,  1956,   371,    87,  6931,     3,    61,    87,    61,\n            3,     6,    87,     6,     8,    87, 12111,  1389,    87,   683,\n          683,   936,    87,   683,   683,   643,    87, 17235,  2912,    87,\n        17235,     3,     5,    87,     5, 10162, 21144,   447,    87,   567,\n         9082,  9329,     7,    87, 17235,   134,    43,    87,   553, 11165,\n          359,    87,   683,   683,  2284,    87, 17235,   134,    16,    87,\n         3162,   542,    87, 17235,  4537,    87, 17235,     3,     6,    87,\n            6,   379,    87,   553, 19179,  3285,    87, 17235,     3,     6,\n           87,     6, 19168,    87, 17235,     3,     6,    87,     6,  6061,\n           87, 17235,    11,    87,  2823,  2013,    87, 17235,     3,     5,\n           87,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([  363,   686,    13,  9329,    19,  5871,   261,    16,  4537,    13,\n         4371,   224,    38,  3285,    11, 19168,    58, 32100,   140,     7,\n        21144,   447,  9329,     7,     1,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0])}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./t5_lora_sciq\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-4,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=6,\n    weight_decay=0.01,\n    save_total_limit=2,\n    predict_with_generate=True,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    fp16=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    lr_scheduler_type=\"linear\",\n    warmup_steps=500,\n    label_names=[\"labels\"],\n)","metadata":{"id":"JPJ0QHmMyoWc","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:38:00.882917Z","iopub.execute_input":"2024-12-09T20:38:00.883158Z","iopub.status.idle":"2024-12-09T20:38:01.027407Z","shell.execute_reply.started":"2024-12-09T20:38:00.883135Z","shell.execute_reply":"2024-12-09T20:38:01.026557Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Custom data collator to handle labels\nclass CustomDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        if \"labels\" in batch:\n            batch[\"labels\"] = torch.tensor(batch[\"labels\"])\n        return batch\n\ndata_collator = CustomDataCollatorForSeq2Seq(tokenizer, model=model, padding=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:38:01.028451Z","iopub.execute_input":"2024-12-09T20:38:01.028756Z","iopub.status.idle":"2024-12-09T20:38:01.033530Z","shell.execute_reply.started":"2024-12-09T20:38:01.028730Z","shell.execute_reply":"2024-12-09T20:38:01.032787Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[early_stopping_callback],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:38:01.034570Z","iopub.execute_input":"2024-12-09T20:38:01.034811Z","iopub.status.idle":"2024-12-09T20:38:03.230524Z","shell.execute_reply.started":"2024-12-09T20:38:01.034788Z","shell.execute_reply":"2024-12-09T20:38:03.229868Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/3052186335.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Debugging Data Collator\nsample_batch = [train_dataset[i] for i in range(1)]\nprint(\"Sample batch:\", sample_batch)\ncollated_batch = data_collator(sample_batch)\nprint(\"Collated batch:\", collated_batch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:38:03.231543Z","iopub.execute_input":"2024-12-09T20:38:03.231813Z","iopub.status.idle":"2024-12-09T20:38:03.283923Z","shell.execute_reply.started":"2024-12-09T20:38:03.231786Z","shell.execute_reply":"2024-12-09T20:38:03.283115Z"}},"outputs":[{"name":"stdout","text":"Sample batch: [{'input_ids': tensor([10162, 21144,    15,     7,  1604,   200,    16,  8107,  2912,     6,\n         3115,   344,   944,  1956,   254,    11,  1283,  1956,   254,    41,\n         4013,  1956,   371,    11,     3, 15442,  1956,   371,   137, 10162,\n        21144,    15,     7,    33,   557,   435,   840,    16,    42,    30,\n            8,  5678,    13,  6917,    42,   119,  3127,     5,    37,  6624,\n         1170,  2912,    13,   186,  2071, 20853,   140,     7, 21144,    15,\n            7,    19,  6862,  1956,   254,    41,  3916,  1956,   371,   201,\n            8,  1389,   936,   643,  2912,     5, 10162, 21144,   447,  9329,\n            7,    43,   359,  2284,    16,   542,  4537,     6,   379,  3285,\n            6, 19168,     6,  6061,    11,  2013,     5,     3, 18206,    10,\n        10162, 21144,    15,     7,  1604,   200,    16,  8107,  2912,     3,\n            6,  3115,   344,   944,  1956,   254,    11,  1283,  1956,   254,\n           41,     3,  4013,  1956,   371,    11,     3, 15442,  1956,   371,\n            3,    61,     3,     5, 10162, 21144,    15,     7,    33,   557,\n          435,   840,    16,    42,    30,     8,  5678,    13,  6917,    42,\n          119,  3127,     3,     5,    37,  6624,  1170,  2912,    13,   186,\n         2071, 20853,   140,     7, 21144,    15,     7,    19,  6862,  1956,\n          254,    41,     3,  3916,  1956,   371,     3,    61,     3,     6,\n            8,  1389,   936,   643,  2912,     3,     5, 10162, 21144,   447,\n          599,  2990,   517, 20629,   956,  8015,    61,  9329,     7,    43,\n          359,  2284,    16,   542,  4537,     3,     6,   379,  3285,     3,\n            6, 19168,     3,     6,  6061,    11,  2013,     3,     5,     3,\n        16034,    10, 10162, 21144,    15,     7,    87, 17235,   134,  1604,\n           87,   553, 11165,   200,    87,   683, 23787,    16,    87,  3162,\n         8107,    87,   683,   683,  2912,    87, 17235,     3,     6,    87,\n            6,  3115,    87, 12108,   344,    87,  3162,   944,  1956,   254,\n           87,  6931,    11,    87,  2823,  1283,  1956,   254,    87,  6931,\n           41,    87,   599,     3,  4013,  1956,   371,    87,  6931,    11,\n           87,  2823,     3, 15442,  1956,   371,    87,  6931,     3,    61,\n           87,    61,     3,     5,    87,     5, 10162, 21144,    15,     7,\n           87, 17235,   134,    33,    87,   553, 11165,   557,    87, 12108,\n          435,    87,   553, 19174,   840,    87, 17235,    16,    87,  3162,\n           42,    87,  2823,    30,    87,  3162,     8,    87, 12111,  5678,\n           87, 17235,   134,    13,    87,  3162,  6917,    87, 17235,   134,\n           42,    87,  2823,   119,    87,   683,   683,  3127,    87, 17235,\n          134,     3,     5,    87,     5,    37,    87, 12111,  6624,    87,\n          683,   683,  1170,    87, 17235,  2912,    87, 17235,    13,    87,\n         3162,   186,    87,   683,   683,  2071, 20853,    87,   683,   683,\n          140,     7, 21144,    15,     7,    87, 17235,   134,    19,    87,\n        22086,   956,  6862,  1956,   254,    87,  6931,    41,    87,   599,\n            3,  3916,  1956,   371,    87,  6931,     3,    61,    87,    61,\n            3,     6,    87,     6,     8,    87, 12111,  1389,    87,   683,\n          683,   936,    87,   683,   683,   643,    87, 17235,  2912,    87,\n        17235,     3,     5,    87,     5, 10162, 21144,   447,    87,   567,\n         9082,  9329,     7,    87, 17235,   134,    43,    87,   553, 11165,\n          359,    87,   683,   683,  2284,    87, 17235,   134,    16,    87,\n         3162,   542,    87, 17235,  4537,    87, 17235,     3,     6,    87,\n            6,   379,    87,   553, 19179,  3285,    87, 17235,     3,     6,\n           87,     6, 19168,    87, 17235,     3,     6,    87,     6,  6061,\n           87, 17235,    11,    87,  2823,  2013,    87, 17235,     3,     5,\n           87,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([  363,   686,    13,  9329,    19,  5871,   261,    16,  4537,    13,\n         4371,   224,    38,  3285,    11, 19168,    58, 32100,   140,     7,\n        21144,   447,  9329,     7,     1,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0])}]\nCollated batch: {'input_ids': tensor([[10162, 21144,    15,     7,  1604,   200,    16,  8107,  2912,     6,\n          3115,   344,   944,  1956,   254,    11,  1283,  1956,   254,    41,\n          4013,  1956,   371,    11,     3, 15442,  1956,   371,   137, 10162,\n         21144,    15,     7,    33,   557,   435,   840,    16,    42,    30,\n             8,  5678,    13,  6917,    42,   119,  3127,     5,    37,  6624,\n          1170,  2912,    13,   186,  2071, 20853,   140,     7, 21144,    15,\n             7,    19,  6862,  1956,   254,    41,  3916,  1956,   371,   201,\n             8,  1389,   936,   643,  2912,     5, 10162, 21144,   447,  9329,\n             7,    43,   359,  2284,    16,   542,  4537,     6,   379,  3285,\n             6, 19168,     6,  6061,    11,  2013,     5,     3, 18206,    10,\n         10162, 21144,    15,     7,  1604,   200,    16,  8107,  2912,     3,\n             6,  3115,   344,   944,  1956,   254,    11,  1283,  1956,   254,\n            41,     3,  4013,  1956,   371,    11,     3, 15442,  1956,   371,\n             3,    61,     3,     5, 10162, 21144,    15,     7,    33,   557,\n           435,   840,    16,    42,    30,     8,  5678,    13,  6917,    42,\n           119,  3127,     3,     5,    37,  6624,  1170,  2912,    13,   186,\n          2071, 20853,   140,     7, 21144,    15,     7,    19,  6862,  1956,\n           254,    41,     3,  3916,  1956,   371,     3,    61,     3,     6,\n             8,  1389,   936,   643,  2912,     3,     5, 10162, 21144,   447,\n           599,  2990,   517, 20629,   956,  8015,    61,  9329,     7,    43,\n           359,  2284,    16,   542,  4537,     3,     6,   379,  3285,     3,\n             6, 19168,     3,     6,  6061,    11,  2013,     3,     5,     3,\n         16034,    10, 10162, 21144,    15,     7,    87, 17235,   134,  1604,\n            87,   553, 11165,   200,    87,   683, 23787,    16,    87,  3162,\n          8107,    87,   683,   683,  2912,    87, 17235,     3,     6,    87,\n             6,  3115,    87, 12108,   344,    87,  3162,   944,  1956,   254,\n            87,  6931,    11,    87,  2823,  1283,  1956,   254,    87,  6931,\n            41,    87,   599,     3,  4013,  1956,   371,    87,  6931,    11,\n            87,  2823,     3, 15442,  1956,   371,    87,  6931,     3,    61,\n            87,    61,     3,     5,    87,     5, 10162, 21144,    15,     7,\n            87, 17235,   134,    33,    87,   553, 11165,   557,    87, 12108,\n           435,    87,   553, 19174,   840,    87, 17235,    16,    87,  3162,\n            42,    87,  2823,    30,    87,  3162,     8,    87, 12111,  5678,\n            87, 17235,   134,    13,    87,  3162,  6917,    87, 17235,   134,\n            42,    87,  2823,   119,    87,   683,   683,  3127,    87, 17235,\n           134,     3,     5,    87,     5,    37,    87, 12111,  6624,    87,\n           683,   683,  1170,    87, 17235,  2912,    87, 17235,    13,    87,\n          3162,   186,    87,   683,   683,  2071, 20853,    87,   683,   683,\n           140,     7, 21144,    15,     7,    87, 17235,   134,    19,    87,\n         22086,   956,  6862,  1956,   254,    87,  6931,    41,    87,   599,\n             3,  3916,  1956,   371,    87,  6931,     3,    61,    87,    61,\n             3,     6,    87,     6,     8,    87, 12111,  1389,    87,   683,\n           683,   936,    87,   683,   683,   643,    87, 17235,  2912,    87,\n         17235,     3,     5,    87,     5, 10162, 21144,   447,    87,   567,\n          9082,  9329,     7,    87, 17235,   134,    43,    87,   553, 11165,\n           359,    87,   683,   683,  2284,    87, 17235,   134,    16,    87,\n          3162,   542,    87, 17235,  4537,    87, 17235,     3,     6,    87,\n             6,   379,    87,   553, 19179,  3285,    87, 17235,     3,     6,\n            87,     6, 19168,    87, 17235,     3,     6,    87,     6,  6061,\n            87, 17235,    11,    87,  2823,  2013,    87, 17235,     3,     5,\n            87,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[  363,   686,    13,  9329,    19,  5871,   261,    16,  4537,    13,\n          4371,   224,    38,  3285,    11, 19168,    58, 32100,   140,     7,\n         21144,   447,  9329,     7,     1,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0]]), 'decoder_input_ids': tensor([[    0,   363,   686,    13,  9329,    19,  5871,   261,    16,  4537,\n            13,  4371,   224,    38,  3285,    11, 19168,    58, 32100,   140,\n             7, 21144,   447,  9329,     7,     1,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0]])}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/tmp/ipykernel_23/1471272100.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"id":"p9N3NEtIyqe3","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:38:03.284852Z","iopub.execute_input":"2024-12-09T20:38:03.285094Z","iopub.status.idle":"2024-12-10T02:31:35.728961Z","shell.execute_reply.started":"2024-12-09T20:38:03.285070Z","shell.execute_reply":"2024-12-10T02:31:35.728084Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241209_203826-vsmxg19l</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/kharchesarvesh-rutgers-university/huggingface/runs/vsmxg19l' target=\"_blank\">./t5_lora_sciq</a></strong> to <a href='https://wandb.ai/kharchesarvesh-rutgers-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/kharchesarvesh-rutgers-university/huggingface' target=\"_blank\">https://wandb.ai/kharchesarvesh-rutgers-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/kharchesarvesh-rutgers-university/huggingface/runs/vsmxg19l' target=\"_blank\">https://wandb.ai/kharchesarvesh-rutgers-university/huggingface/runs/vsmxg19l</a>"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_23/1471272100.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"])\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8760' max='8760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8760/8760 5:53:04, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.179700</td>\n      <td>0.174001</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.168100</td>\n      <td>0.169950</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.151100</td>\n      <td>0.168197</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.145200</td>\n      <td>0.167968</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.146700</td>\n      <td>0.167461</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.129700</td>\n      <td>0.168105</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_23/1471272100.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"])\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/tmp/ipykernel_23/1471272100.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"])\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/tmp/ipykernel_23/1471272100.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"])\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/tmp/ipykernel_23/1471272100.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"])\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/tmp/ipykernel_23/1471272100.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"])\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/tmp/ipykernel_23/1471272100.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"])\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=8760, training_loss=0.3485445708444674, metrics={'train_runtime': 21211.9745, 'train_samples_per_second': 3.304, 'train_steps_per_second': 0.413, 'total_flos': 1.5272928291166618e+17, 'train_loss': 0.3485445708444674, 'epoch': 6.0})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Save the model and tokenizer\nmodel.save_pretrained(\"./t5squad_ner-pos_finetuned_sciq\")\ntokenizer.save_pretrained(\"./t5squad_ner-pos_finetuned_sciq\")","metadata":{"id":"FJvWABBpyt7m","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T02:31:35.730206Z","iopub.execute_input":"2024-12-10T02:31:35.730929Z","iopub.status.idle":"2024-12-10T02:31:35.950918Z","shell.execute_reply.started":"2024-12-10T02:31:35.730887Z","shell.execute_reply":"2024-12-10T02:31:35.950057Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"('./t5squad_ner-pos_finetuned_sciq/tokenizer_config.json',\n './t5squad_ner-pos_finetuned_sciq/special_tokens_map.json',\n './t5squad_ner-pos_finetuned_sciq/spiece.model',\n './t5squad_ner-pos_finetuned_sciq/added_tokens.json',\n './t5squad_ner-pos_finetuned_sciq/tokenizer.json')"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Zip the saved model\n!zip -r t5squad_ner-pos_finetuned_sciq.zip ./t5squad_ner-pos_finetuned_sciq\nfrom IPython.display import FileLink\nFileLink(r't5squad_ner-pos_finetuned_sciq.zip')","metadata":{"id":"y8tmEeCqyv-3","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T02:31:35.952014Z","iopub.execute_input":"2024-12-10T02:31:35.952293Z","iopub.status.idle":"2024-12-10T02:31:38.210727Z","shell.execute_reply.started":"2024-12-10T02:31:35.952266Z","shell.execute_reply":"2024-12-10T02:31:38.209696Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: t5squad_ner-pos_finetuned_sciq/ (stored 0%)\n  adding: t5squad_ner-pos_finetuned_sciq/README.md (deflated 66%)\n  adding: t5squad_ner-pos_finetuned_sciq/adapter_model.safetensors (deflated 7%)\n  adding: t5squad_ner-pos_finetuned_sciq/tokenizer_config.json (deflated 95%)\n  adding: t5squad_ner-pos_finetuned_sciq/adapter_config.json (deflated 53%)\n  adding: t5squad_ner-pos_finetuned_sciq/special_tokens_map.json (deflated 86%)\n  adding: t5squad_ner-pos_finetuned_sciq/added_tokens.json (stored 0%)\n  adding: t5squad_ner-pos_finetuned_sciq/tokenizer.json (deflated 74%)\n  adding: t5squad_ner-pos_finetuned_sciq/spiece.model (deflated 48%)\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/t5squad_ner-pos_finetuned_sciq.zip","text/html":"<a href='t5squad_ner-pos_finetuned_sciq.zip' target='_blank'>t5squad_ner-pos_finetuned_sciq.zip</a><br>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}