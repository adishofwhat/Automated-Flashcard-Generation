{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Elp-OmLpCjrc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "splits = {'train': 'data/train-00000-of-00001.parquet', 'validation': 'data/validation-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
        "df_train = pd.read_parquet(\"hf://datasets/allenai/sciq/\" + splits[\"train\"])\n",
        "df_val = pd.read_parquet(\"hf://datasets/allenai/sciq/\" + splits[\"validation\"])\n",
        "df_test = pd.read_parquet(\"hf://datasets/allenai/sciq/\" + splits[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Combine question and correct_answer as target\n",
        "df_train['target'] = df_train['question'] + \" \" + df_train['correct_answer']\n",
        "df_val['target'] = df_val['question'] + \" \" + df_val['correct_answer']\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_train['support'].tolist() + df_train['target'].tolist())\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train = tokenizer.texts_to_sequences(df_train['support'].tolist())\n",
        "y_train = tokenizer.texts_to_sequences(df_train['target'].tolist())\n",
        "X_val = tokenizer.texts_to_sequences(df_val['support'].tolist())\n",
        "y_val = tokenizer.texts_to_sequences(df_val['target'].tolist())\n",
        "\n",
        "# Pad sequences\n",
        "max_len = 100\n",
        "X_train = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
        "y_train = pad_sequences(y_train, maxlen=max_len, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=max_len, padding='post')\n",
        "y_val = pad_sequences(y_val, maxlen=max_len, padding='post')"
      ],
      "metadata": {
        "id": "q57br8omCk1w"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
        "model.add(SimpleRNN(128, return_sequences=True))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "X1wXYyY2CmyE",
        "outputId": "c43c9815-c923-449c-a002-b5101c8dfd6e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)             │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)             │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape y_train and y_val to match the output shape\n",
        "y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], 1))\n",
        "y_val = y_val.reshape((y_val.shape[0], y_val.shape[1], 1))\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=6, batch_size=64, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypEfA1ExC3tD",
        "outputId": "5d4d3aca-29fe-49c3-e556-6691ecbdc466"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 90ms/step - accuracy: 0.8013 - loss: 4.5080 - val_accuracy: 0.8537 - val_loss: 1.3902\n",
            "Epoch 2/6\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 76ms/step - accuracy: 0.8559 - loss: 1.3652 - val_accuracy: 0.8580 - val_loss: 1.3139\n",
            "Epoch 3/6\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 75ms/step - accuracy: 0.8618 - loss: 1.2309 - val_accuracy: 0.8619 - val_loss: 1.1239\n",
            "Epoch 4/6\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 75ms/step - accuracy: 0.8658 - loss: 1.0789 - val_accuracy: 0.8627 - val_loss: 1.1167\n",
            "Epoch 5/6\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 75ms/step - accuracy: 0.8655 - loss: 1.0681 - val_accuracy: 0.8631 - val_loss: 1.1015\n",
            "Epoch 6/6\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 77ms/step - accuracy: 0.8663 - loss: 1.0287 - val_accuracy: 0.8634 - val_loss: 1.0599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Validation Loss: {loss}')\n",
        "print(f'Validation Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zHGBm6TDxw-",
        "outputId": "5c9b59c5-6f1b-40dc-ed09-06fc9425c6f8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.8644 - loss: 1.0605\n",
            "Validation Loss: 1.0599321126937866\n",
            "Validation Accuracy: 0.8633899688720703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "\n",
        "def generate_qa(context, model, tokenizer, max_len=100):\n",
        "    # Tokenize the input context\n",
        "    input_seq = tokenizer.texts_to_sequences([context])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Predict the sequence\n",
        "    predicted_seq = model.predict(input_seq)\n",
        "\n",
        "    # Get the predicted tokens\n",
        "    predicted_tokens = np.argmax(predicted_seq, axis=-1)[0]\n",
        "\n",
        "    # Decode the predicted tokens back to text\n",
        "    question_answer = tokenizer.sequences_to_texts([predicted_tokens])[0]\n",
        "\n",
        "    # Split the question and answer\n",
        "    question, answer = question_answer.split(' ', 1)\n",
        "\n",
        "    generated_qa = f\"{question} {answer}\"\n",
        "\n",
        "    return generated_qa\n",
        "\n",
        "def compute_rouge_l(reference, candidate):\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_result = scorer.score(reference, candidate)\n",
        "    return rouge_result['rougeL'].fmeasure\n",
        "\n",
        "def compute_bert_score(reference, candidate):\n",
        "    _, _, f1 = score([candidate], [reference], lang=\"en\", verbose=False)\n",
        "    bert_f1 = f1.mean().item()\n",
        "    return bert_f1\n",
        "\n",
        "def evaluate_context_matching(contexts, model, tokenizer):\n",
        "    rouge_l_scores = []\n",
        "    bert_scores = []\n",
        "\n",
        "    for context in contexts:\n",
        "        generated_qa = generate_qa(context, model, tokenizer)\n",
        "\n",
        "        rouge_l_score = compute_rouge_l(context, generated_qa)\n",
        "        rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "        bert_score = compute_bert_score(context, generated_qa)\n",
        "        bert_scores.append(bert_score)\n",
        "\n",
        "    summary = {\n",
        "        'rouge_l_scores': {\n",
        "            'mean': np.mean(rouge_l_scores),\n",
        "            'std': np.std(rouge_l_scores)\n",
        "        },\n",
        "        'bert_scores': {\n",
        "            'mean': np.mean(bert_scores),\n",
        "            'std': np.std(bert_scores)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Example usage\n",
        "contexts = [\n",
        "    \"A frameshift mutation is a deletion or insertion of one or more nucleotides that changes the reading frame of the base sequence. Deletions remove nucleotides, and insertions add nucleotides. Consider the following sequence of bases in RNA:\"\n",
        "]\n",
        "\n",
        "summary = evaluate_context_matching(contexts, model, tokenizer)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDDDPu2JD2k2",
        "outputId": "8a4a7ccc-1391-424a-cd41-791c284fb504"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge_l_scores': {'mean': 0.22222222222222224, 'std': 0.0}, 'bert_scores': {'mean': 0.7658838629722595, 'std': 0.0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"A frameshift mutation is a deletion or insertion of one or more nucleotides that changes the reading frame of the base sequence. Deletions remove nucleotides, and insertions add nucleotides. Consider the following sequence of bases in RNA:\"\n",
        "\n",
        "generate_qa(context, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "yu1QNXztNQ1b",
        "outputId": "aeaa37da-fcb4-4166-c6ed-d4c874c0df0f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what is the the the of of of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('rnn_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmrJ697ZN3V9",
        "outputId": "1afd3172-116f-4c2b-8b3a-b27f552e35df"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('rnn_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TrJ8rD83W3oc",
        "outputId": "c38c69a6-baec-4587-8b1a-912fd289b4be"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0043ef42-a100-4af0-a0d4-1fc737f1085f\", \"rnn_model.h5\", 66952912)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lj5exycKXA0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}