{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install rouge_score absl-py\n",
    "# pip install bert_score\n",
    "# pip install evaluate\n",
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"Chelsea's mini-revival continued with a third victory in a row as they consigned struggling Leicester City to a fifth consecutive defeat.\n",
    "Buoyed by their Champions League win over Borussia Dortmund, Chelsea started brightly and Ben Chilwell volleyed in from a tight angle against his old club.\n",
    "Chelsea's Joao Felix and Leicester's Kiernan Dewsbury-Hall hit the woodwork in the space of two minutes, then Felix had a goal ruled out by the video assistant referee for offside.\n",
    "Patson Daka rifled home an excellent equaliser after Ricardo Pereira won the ball off the dawdling Felix outside the box.\n",
    "But Kai Havertz pounced six minutes into first-half injury time with an excellent dinked finish from Enzo Fernandez's clever aerial ball.\n",
    "Mykhailo Mudryk thought he had his first goal for the Blues after the break but his effort was disallowed for offside.\n",
    "Mateo Kovacic sealed the win as he volleyed in from Mudryk's header.\n",
    "The sliding Foxes, who ended with 10 men following Wout Faes' late dismissal for a second booking, now just sit one point outside the relegation zone.\n",
    "\"\"\".replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Token and Position Embeddings The matrix X (of shape [N × d]) has an embedding for  each word in the context.  This embedding is created by adding two distinct  embedding for each input • token embedding • positional embedding Token Embeddings Embedding matrix E has shape [|V | × d ].  • One row for each of the |V | tokens in the vocabulary.  • Each word is a row vector of d dimensions Given: string \"Thanks for all the\" 1. Tokenize with BPE and convert into vocab indices w = [5,4000,10532,2224]  2. Select the corresponding rows from E, each row an embedding • (row 5, row 4000, row 10532, row 2224). Position Embeddings There are many methods, but we\\'ll just describe the simplest: absolute  position. Goal: learn a position embedding matrix Start with randomly initialized embeddings • one for each integer up to some maximum length.  • i.e., just as we have an embedding for token fish, we’ll have an  embedding for position 3 and position 17. • As with word embeddings, these position embeddings are learned along  with other parameters during training. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,   # Limit length of generated text\n",
    "    do_sample=True,   # Enable sampling\n",
    "    top_k=50,         # Consider top 50 tokens at each step\n",
    "    top_p=0.95,       # Enable nucleus sampling (tokens with cumulative probability >= 0.95)\n",
    "    temperature=0.7   # Adjust randomness; lower is less random\n",
    ")\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "question_answer = output_text.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "question, answer = question_answer.split(tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the matrix X of shape of?\n",
      "answer:  N\n"
     ]
    }
   ],
   "source": [
    "print(\"question:\", question)\n",
    "print(\"answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: <pad> How many embeddings are needed for each input?<sep> two</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> What's the goal of position embeddings?<sep> learn a position embedding matrix</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> What is the matrix of shape of?<sep> N</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> How are the embeddings created?<sep> by adding two distinct embedding for each input</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> What is the goal of learning a position embedding matrix?<sep> learn a position embedding matrix</s>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    print(\"Generated QA Pair:\", output_text)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f786c5af8a7f4e3fa2196760d4a7b8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub\\models--potsawee--t5-large-generation-race-QuestionAnswer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fccd99677274763a24a0567cf6db95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090fd033874e456998e25e6386e89ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df296e875f19410da76a11ba30e7970b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de82f9c01ed44758e9daca4e93b13c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe52fa192bc4ca0a87ec76fa4dd1ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4117a068543a4b6fb467738289b7744c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0319fe52b2b45038e7881db6631e582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-race-QuestionAnswer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-race-QuestionAnswer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,   # Limit length of generated text\n",
    "    do_sample=True,   # Enable sampling\n",
    "    top_k=50,         # Consider top 50 tokens at each step\n",
    "    top_p=0.95,       # Enable nucleus sampling (tokens with cumulative probability >= 0.95)\n",
    "    temperature=0.7   # Adjust randomness; lower is less random\n",
    ")\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "question_answer = output_text.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "question, answer = question_answer.split(tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  According to the passage, how do you learn an embedding?\n",
      "answer:  By working out the embeddings.\n"
     ]
    }
   ],
   "source": [
    "print(\"question:\", question)\n",
    "print(\"answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: <pad> What is the purpose of the passage?<sep> To tell you how to create embeddings.</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> What can we infer from the passage?<sep> The word \"Thanks\" has two embeddings in the embedding matrix E.</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> The word \"thanks\" is _ .<sep> a token</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> Token Embeddings and positional embeddings are similar in the way that they _ .<sep> are created by adding two distinct embeddings</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> The first step to create an embedding is to _ .<sep> create a matrix X</s>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    print(\"Generated QA Pair:\", output_text)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: {'bleu': 0.0, 'precisions': [0.5, 0.14285714285714285, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.6, 'translation_length': 8, 'reference_length': 5}\n",
      "ROUGE score: {'rouge1': 0.5454545454545454, 'rouge2': 0.2222222222222222, 'rougeL': 0.5454545454545454, 'rougeLsum': 0.5454545454545454}\n",
      "BERTScore: {'precision': [0.7796052694320679], 'recall': [0.8943084478378296], 'f1': [0.8330268859863281], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Example predictions and references\n",
    "references = [[\"What is position embedding?\"]]\n",
    "predictions = [\"What is the goal of position embeddings?\"]\n",
    "\n",
    "# BLEU score\n",
    "bleu_result = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU score:\", bleu_result)\n",
    "\n",
    "# ROUGE score\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "print(\"ROUGE score:\", rouge_result)\n",
    "\n",
    "# BERTScore\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, model_type=\"bert-base-uncased\")\n",
    "print(\"BERTScore:\", bertscore_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdfplumber\n",
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = 'D:/dell data/rutgers/nlp/slides/slide 10 - transformers.pdf'\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "# print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = pdf_text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction to Transformers TransformersELMo – Embeddings from Language Models Deep contextualized word representations, Peters et. al. (2018) Word vectors as learned functions of internal states of a deep bidirectional language model (biLM) (pretrained on a large text corpus) Representations can be added to existing models Improve performance across six challenging NLP problems (question answering, textual entailment, sentiment analysis, …) Exposing the deep internals of pre-trained model allows downstream models to mix different types of semi-supervision signals.ELMo 2 layer biLSTM ELMo representations are a function of all internal layers of biLM. Linear combination of the vectors stacked above each input word Improves performance over just using the top LSTM layerTransfer Learning Natural Language Processing with Deep Learning CS224N/Ling284, Chris Manning, StanfordULMfit Universal Language Model Fine-tuning for Text Train LM on big general domain corpus (use biLM) Classification (ULMFiT) Tune LM on target task data Fine-tune as classifier on target task Howard and Ruder (2018) 29Transfer Learning in ULMFiT IMDB TREC-6 AGA very approximate timeline 1990 Static Word Embeddings 2003 Neural Language Model 2008 Multi-Task Learning 2015 Attention 2017 Transformer 2018 Contextual Word Embeddings and Pretraining 2019 PromptingLLMs are built out of transformers Transformer: a specific kind of network architecture, like a fancier feedforward network, but based on attentionAttention TransformersRepresentations for a Word Neural Network for NLP Task (LM, Classifcation) Random word vectors Word Embeddings Word2Vec, Glove, fastText Just one representation for a word type regardless of the context in which a word token occurs ◦ Open account in bank vs river bank ◦ Star in the sky vs. Movie star Words have different aspects, including semantics, syntactic behavior, connotations ◦ E.g arrive vs arrivalHave we handled context for words before ? RNNs, LSTMs Language Models are producing context-specific word representations at each position for the training data. Can we learn the meaning of a word in context in a standard way?Instead of starting with the big Transformer picture Let\\'s consider the embeddings for an individual word from a particular layer Next token long and thanks for all Next token long and thanks for all Language … logits logits logits logits logits Modeling Language U U U U U … MHodeealding logits logits logits logits logits U U U U U Head … … … … … … … … … … Stacked Stacked … Transformer … Transformer BBlloocckkss … x1 x2 x3 x4 x5 … x1 x2 x3 x4 x5 + 1 + 2 + 3 + 4 + 5 Input … + 1 + 2 + 3 + 4 + 5 EnIncpoduitng E E E E E … Encoding E E E E E Input tokens So long and thanks for Input tokens So long and thanks forProblem with static embeddings (word2vec) They are static! The embedding for a word doesn\\'t reflect how its meaning changes in context. The chicken didn\\'t cross the road because it was too tired What is the meaning represented in the static embedding for \"it\"?Contextual Embeddings • Intuition: a representation of meaning of a word should be different in different contexts! • Contextual Embedding: each word has a different vector that expresses different meanings depending on the surrounding words • How to compute contextual embeddings? • AttentionContextual Embeddings The chicken didn\\'t cross the road because it What should be the properties of \"it\"? The chicken didn\\'t cross the road because it was too tired The chicken didn\\'t cross the road because it was too wide At this point in the sentence, it\\'s probably referring to either the chicken or the streetIntuition of attention Build up the contextual embedding from a word by selectively integrating information from all the neighboring words We say that a word \"attends to\" some neighboring words more than othersIntuition of attention: testAttention definition A mechanism for helping compute the embedding for a token by selectively attending to and integrating information from surrounding tokens (at the previous layer). More formally: a method for doing a weighted sum of vectors.Attention is left-to-right a a a a a 1 2 3 4 5 Self-Attention attention attention attention attention attention Layer x x x x x 1 2 3 4 510.1 • T T : A S -A N 5 HE 10R.A1NS•FOTRMETR ELF :TATESNTI-OAN ETWONRK 5 HE RANSFORMER ELF TTENTION ETWORK a a a a a 1 2 3 4 5 a a a a a 1 2 3 4 5 Self-Attention Layer Self-Attention Layer x x x x x 1 2 3 4 5 x x x x x Figure 10.2 Information flow in a causal (or masked) self-attention model. In processing 1 2 3 4 5 each element of the sequence, the model attends to all the inputs up to, and including, the current one. Unlike RNNs, the computations at each time step are independent of all the Figur e 10.2 Information flow in a causal (or masked) self-attention model. In processing other steps and therefore can be performed in parallel. each element of the sequence, the model attends to all the inputs up to, and including, the current one. Unlike RNNs, the computations at each time step are independent of all the 10.1.3 Self-attention mor e for mally other steps and therefore can be performed in parallel. We’ve given the intuition of self-attention (as a way to compute representations of a word at a given layer by integrating information from words at the previous layer) 10.1.3 Self-attention mor e for mally and we’ve defined context as all the prior words in the input. Let’s now introduce the self-attention computation itself. We’ve given the intuition of self-attention (as a way to compute representations of a The core intuition of attention is the idea of comparing an item of interest to a word at a given layer by integrating information from words at the previous layer) collection of other items in a way that reveals their relevance in the current context. and we’ve defined context as all the prior words in the input. Let’s now introduce In the case of self-attention for language, the set of comparisons are to other words (or tokens) within a given sequence. The result of these comparisons is then used to the self-attention computation itself. compute an output sequence for the current input sequence. For example, returning The core intuition of attention is the idea of comparing an item of interest to a to Fig. 10.2, the computation of a is based on a set of comparisons between the 3 collection of other items in a way that reveals their relevance in the current context. input x and its preceding elements x and x , and to x itself. 3 1 2 3 In the case of self-attention for language, the set of comparisons are to other words How shall we compare words to other words? Since our representations for (or tokens) withinwaogrdivseanresveeqcutoernsc, ew.eT’ llhemarekesuulsteoofftohuerseoldcofmriepnadrithsoendsotisptrhoednucut stehdat twoe used for computing word similarity in Chapter 6, and also played a role in attention in compute an output sequence for the current input sequence. For example, returning Chapter 9. Let’s refer to the result of this comparison between words i and j as a to Fig. 10.2, the computation of a is based on a set of comparisons between the 3 score (we’ ll be updating this equation to add attention to the computation of this input x and its preceding elements x and x , and to x itself. 3 score): 1 2 3 How shall we compare words to other words? Since our representations for words are vectors, we’ ll make use of oV uer rso on ld1: friesc no dre t( hx e, x do) t = prx od· ux ct that we used (10.4) i j i j Simplified version of attention: a sum of prior words weighted by their similarity with the current word for computing word similarity in Chapter 6, and also played a role in attention in The result of a dot product is a scalar value ranging from − • to • , the larger Chapter 9. Let’s refer to the resuGlitveonf ath siesqucoenmcpe aorfi stooknenb eetmwbeeedndiwngosr:ds i and j as a the value the more similar the vectors that are being compared. Continuing with our x x x x x x x x score (we’ ll be updating this equation to add attention to the computation of this example, the first step in1 com2 p uti3ng y4 wo5uld 6be to7 comi pute three scores: x · x , 3 3 1 score): x · x and x · x .PTrohdenuctoe:m aa k=e ae wffeecigtihvteedu sseuomf tohfe xse tshcororeusg,hw xe’ l(lannodr mx )alize them 3 2 3 3 i 1 7 i with a softmax toWceriegahtetead bveyc tthoer iro fsiwmeiliagrhittsy, tao x, that indicates the proportional i j i relevance of each input to the input element i that is the current focus of attention. Verson 1: score(x , x ) = x · x (10.4) i j i j a = softmax(score(x , x )) 8 j i (10.5) i j i j The result of a dot product is a scalar value ranging from − • to • , the larger exp(score(x , x )) i j = P 8 j i (10.6) the value the more similar the vectors that are being compared. Continuing with our i exp(score(x , x )) i k k= 1 example, the first step in computing y would be to compute three scores: x · x , 3 3 1 Of course, thesoftmax weight will likely behighest for thecurrent focuselement x · x and x · x . Then to make effective use of these scores, we’ ll normalize them 3 2 3 3 i, since vecx is very similar to itself, resulting in a high dot product. But other with a softmax to create a viector of weights, a , that indicates the proportional i j context words may also be similar to i, and the softmax will also assign some weight relevance of each input to the input element i that is the current focus of attention. to those words. Given the proportional scores in a , we generate an output value a by summing i a = softmax(score(x , x )) 8 j i (10.5) i j i j exp(score(x , x )) i j = 8 j i P (10.6) i exp(score(x , x )) i k k= 1 Of course, thesoftmax weight will likely behighest for thecurrent focus element i, since vecx is very similar to itself, resulting in a high dot product. But other i context words may also be similar to i, and the softmax will also assign some weight to those words. Given the proportional scores in a , we generate an output value a by summing iIntuition of attention: test x1 x2 x3 x4 x5 x6 x7 xiAn Actual Attention Head: slightly more complicated High-level idea: instead of using vectors (like x and x ) i 4 directly, we\\'ll represent 3 separate roles each vector x plays: i • query: As the current element being compared to the preceding inputs. • key: as a preceding input that is being compared to the current element to determine a similarity • value: a value of a preceding element that gets weighted and summedAttention intuition query x1 x2 x3 x4 x5 x6 x7 xi valuesIntuition of attention: query x1 x2 x3 x4 x5 x6 x7 xi keys k k k k k k k k v v v v v v v v valuesComputation of Self-Attention - 1 1. Create three vectors from the embedding of each word: Query, Key, and Value by multiplying the embedding by three matrices that are learned during the training process. The Illustrated Transformer by Jay AlammarComputation of Self- Attention -2 Calculate set of scores for each word. (focus on other parts of the input sentence as a word is encoded at a position). Score = dot product of query vector with key vector of respective scored word. Divide scores by 8 (square root of dimension of the key vectors used in paper). Pass the result through softmax to normalize scores so they’re all positive and add up to 1. Multiply value vector by softmax score. (keep values of word(s) to focus on, drown-out irrelevant words Sum the weighted value vectors. Produces output of self- attention layer at this position (for the first word). The Illustrated Transformer by Jay AlammarSelf- Attention – with Matrices The Illustrated Transformer by Jay AlammarMulti-Headed Self-Attention (Expanding attention to focus on different aspects) Multiple sets of Query/Key/Value weight matrices giving rise to multiple representation subspaces The Illustrated Transformer by Jay AlammarOverall Process The Illustrated Transformer by Jay AlammarCalculating the value of a3 a Output of self-attention 3 6. Sum the weighted value vectors 5. Weigh each value vector × ×  3,1 3,2 3,3 4. Turn into  weights via softmax i,j ÷ ÷ ÷ 3. Divide score by √d k √d √d √d k k k 2. Compare x3’s query with the keys for x1, x2, and x3 Wk k Wk k Wk k 1. Generate Wq q Wq q Wq q key, query, value vectors x Wv v x Wv v x Wv v 1 2 3Actual Attention: slightly more complicated • Instead of one attention head, we\\'ll have lots of them! • Intuition: each head might be attending to the context for different purposes • Different linguistic relationships or patterns in the contextSummary Attention is a method for enriching the representation of a token by incorporating contextual information The result: the embedding for each word will be different in different contexts! Contextual embeddings: a representation of word meaning in its context. We\\'ll see in the next lecture that attention can also be viewed as a way to move information from one token to another.Attention TransformersThe Transformer Block TransformersReminder: transformer language model Next token long and thanks for all Language … logits logits logits logits logits Modeling U U U U U Head … … … … … Stacked … Transformer Blocks … x1 x2 x3 x4 x5 + 1 + 2 + 3 + 4 + 5 Input … Encoding E E E E E Input tokens So long and thanks forThe residual stream: each token gets passed up and modifiedWe\\'ll need nonlinearities, so a feedforward layerLayer norm: the vector x is normalized twice iLayer Norm Layer norm is a variation of the z-score from statistics, applied to a single vector in a hidden layerPutting together a single transformer blockA transformer is a stack of these blocks so all the vectors are of the same dimensionality d Block 2 Block 1Residual streams and attention Notice that all parts of the transformer block apply to 1 residual stream (1 token). Except attention, which takes information from other tokens Elhage et al. (2021) show that we can view attention heads as literally moving information from the residual stream of a neighboring token into the current stream .The Transformer Block TransformersParallelizing Attention Computation TransformersParallelizing computation using X For attention/transformer block we\\'ve been computing a single output at a single time step i in a single residual stream. But we can pack the N tokens of the input sequence into a single matrix X of size [N × d]. Each row of X is the embedding of one token of the input. X can have 1K-32K rows, each of the dimensionality of the embedding d (the model dimension)QKT Now can do a single matrix multiply to combine Q and KTParallelizing attention • Scale the scores, take the softmax, and then multiply the result by V resulting in a matrix of shape N × d • An attention vector for each input tokenMasking out the future • What is this mask function? QKT has a score for each query dot every key, including those that follow the query. • Guessing the next word is pretty simple if you already know it!Masking out the future Add –∞ to cells in upper triangle The softmax will turn it to 0Another point: Attention is quadratic in lengthAttention againParallelizing Multi-head AttentionParallelizing Multi-head Attention orParallelizing Attention Computation TransformersInput and output: Position embeddings and the Language Model Head TransformersToken and Position Embeddings The matrix X (of shape [N × d]) has an embedding for each word in the context. This embedding is created by adding two distinct embedding for each input • token embedding • positional embeddingToken Embeddings Embedding matrix E has shape [|V | × d ]. • One row for each of the |V | tokens in the vocabulary. • Each word is a row vector of d dimensions Given: string \"Thanks for all the\" 1. Tokenize with BPE and convert into vocab indices w = [5,4000,10532,2224] 2. Select the corresponding rows from E, each row an embedding • (row 5, row 4000, row 10532, row 2224).Position Embeddings There are many methods, but we\\'ll just describe the simplest: absolute position. Goal: learn a position embedding matrix Start with randomly initialized embeddings • one for each integer up to some maximum length. • i.e., just as we have an embedding for token fish, we’ll have an embedding for position 3 and position 17. • As with word embeddings, these position embeddings are learned along with other parameters during training.Each x is just the sum of word and position embeddings Transformer Block X = Composite Embeddings (word + position) + + + + + Word J b a w a t h b n Embeddings e i l l c k e i l l t Position 1 2 3 4 5 Embeddings Janet will back the billLanguage modeling head … y1 y2 y|V| Word probabilities 1 x |V| Language Model Head Softmax over vocabulary V L takes h and outputs a … N u1 u2 u|V| Logits 1 x |V| distribution over vocabulary V Unembedding Unembedding layer d x |V| layer = ET hL hL hL 1 x d 1 2 N Layer L Transformer Block … w1 w2 w NLanguage modeling head L Unembedding layer: linear layer projects from h (shape [1 × d]) to logit vector N … y1 y2 y|V| Word probabilities 1 x |V| Why \"unembedding\"? Tied to ET Language Model Head Softmax over vocabulary V L takes h and outputs a … N u1 u2 u|V| Logits 1 x |V| distribution over vocabulary V Unembedding Unembedding layer d x |V| Weight tying, we use the same weights for layer = ET two different matrices hL hL hL 1 x d 1 2 N Layer L Transformer Block Unembedding layer maps from an embedding to a … 1x|V| vector of logits w1 w2 w N16 C 9 • T T HAPTER HE RA NSFORMER language models of Chapter 3 compute the probability of a word given counts of its occurrence with the n − 1 prior words. The context is thus of size n − 1. For transformer language models, the context is the size of the transformer’ s context window, which can be quite large: 2K, 4K, even 32K tokens for very large models. The job of the language modeling head is to take the output of the final trans- former layer from the last token N and use it to predict the upcoming word at posi- tion N + 1. Fig. 9.14 shows how to accomplish this task, taking the output of the last token at the last layer (the d-dimensional output embedding of shape [1⇥d]) and producing a probability distribution over words (from which we will choose one to generate). … y1 y2 y|V| Word probabilities 1 x |V| Language Model Head Softmax over vocabulary V L takes h and outputs a … N u1 u2 u|V| Logits 1 x |V| distribution over vocabulary V Unembedding layer Unembedding layer d x |V| U = ET hL hL hL 1 x d 1 2 N Layer L Transformer Block … w1 w2 w N Figure 9.14 The language modeling head: the circuit at the top of a transformer that maps from the output L embedding for token N from the last transformer layer (h ) to a probability distribution over words in the N vocabulary V. The first module in Fig. 9.14 is a linear layer, whose job is to project from the L output h , which represents the output token embedding at position N from the final N block L, (hence of shape [1⇥d]) to the logit vector, or score vector, that will have a logit single score for each of the |V| possible words in the vocabulary V. The logit vector u is thus of dimensionality 1⇥|V|. This linear layer can be learned, but more commonly we tie this matrix to (the transpose of) the embedding matrix E. Recall that in weight tying, we use the weight tying Language modeling head same weights for two different matrices in the model. Thus at the input stage of the transformer the embedding matrix (of shape [|V|⇥d]) is used to map from a one-hot Logits, the score vector u vector over the vocabulary (of shape [1⇥|V|]) to an embedding (of shape [1⇥d]). T And then in the language model head, E , the transpose of the embedding matrix (of One score for each of the |V | possible words in the vocabulary V . shape [d⇥|V|]) is used to map back from an embedding (shape [1⇥d]) to a vector … y1 y2 y|V| Word probabilities 1 x |V| × Shape 1 |V |. over the vocabulary (shape [1⇥|V|]). In the learning process, E will be optimized to Language Model Head Softmax over vocabulary V L be good at doing both of these mappings. We therefore sometimes call the transpose takes h and outputs a … N u1 u2 u|V| Logits 1 x |V| Softmax turns the logits into distribution over vocabulary V ET the uUnnemebmeddibngedding layer because it is performing this reverse mapping. unembedding Unembedding layer d x |V| probabilities over vocabulary. layer = ET A softmax layer turns the logits u into tShheappero 1b ×ab i|lVit i|e. s y over the vocabulary. hL hL hL 1 x d 1 2 N Layer L Transformer Block L T … u = h E (9.44) N w1 w2 w N y = softmax(u) (9.45) We can use these probabilities to do things like help assign a probability to a given text. But the most important usage to generate text, which we do by sampling… Token probabilities y1 y2 y|V| w The final transformer i+1 Sample token to softmax generate at position i+1 model Language Modeling logits u1 u2 … u|V| Head U L h i feedforward … layer norm Layer L y1 y2 y|V| attention Token probabilities w layer norm i+1 L-1 L h = x … i i 2 3 h = x Sample token to i i feedforward softmax generate at position i+1 Language Layer 2 layer norm attention Modeling layer norm logits u1 u2 … u|V| 1 2 h = x Head i i feedforward U Layer 1 layer norm attention layer norm 1 x i L h i Input + i Encoding E feedforward Input token w layer norm i Layer L attention layer norm L-1 L h = x i i … 2 3 h = x i i feedforward layer norm Layer 2 attention layer norm 1 2 h = x i i feedforward layer norm Layer 1 attention layer norm 1 x i + i Input Encoding E w Input token iInput and output: Position embeddings and the Language Model Head Transformers'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6490 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: How many words can one input to the X-Transformer? One\n",
      "Question Relevance: 0.7256\n",
      "Answer Relevance: 0.2463\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the T5 model and tokenizer for generating QA pairs\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "# Load a model to compute embeddings for semantic similarity (e.g., BERT or RoBERTa)\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a function to get embeddings from BERT\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer_bert(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Take the mean of the token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Function to generate QA pairs using T5 model\n",
    "def generate_qa(context):\n",
    "    inputs = tokenizer_t5(context, return_tensors=\"pt\")\n",
    "    outputs = model_t5.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def compute_similarity(qa_pair, context):\n",
    "    # Check if the separator token exists and split accordingly\n",
    "    if tokenizer_t5.sep_token in qa_pair:\n",
    "        question, answer = qa_pair.split(tokenizer_t5.sep_token)\n",
    "    else:\n",
    "        # If the separator is not found, assume the whole text is the question (fallback case)\n",
    "        question = qa_pair\n",
    "        answer = \"\"  # Set answer to empty string, can be refined as needed\n",
    "    \n",
    "    # Get embeddings for the context, question, and answer\n",
    "    context_embedding = get_bert_embeddings(context)\n",
    "    question_embedding = get_bert_embeddings(question)\n",
    "    answer_embedding = get_bert_embeddings(answer)\n",
    "    \n",
    "    # Compute cosine similarity between the context and question, context and answer\n",
    "    question_similarity = cosine_similarity(context_embedding, question_embedding)\n",
    "    answer_similarity = cosine_similarity(context_embedding, answer_embedding)\n",
    "    \n",
    "    return question_similarity[0][0], answer_similarity[0][0]\n",
    "\n",
    "\n",
    "# Generate QA pairs and compute their relevance to the context\n",
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    qa_pair = generate_qa(context)\n",
    "    print(\"Generated QA Pair:\", qa_pair)\n",
    "    \n",
    "    # Compute similarity between the question, answer, and context\n",
    "    question_sim, answer_sim = compute_similarity(qa_pair, context)\n",
    "    \n",
    "    print(f\"Question Relevance: {question_sim:.4f}\")\n",
    "    print(f\"Answer Relevance: {answer_sim:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: What is the matrix X made up of? shape\n",
      "Question Relevance (Cosine Similarity): 0.7014\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Relevance (BERTScore F1): {'precision': [0.493060827255249], 'recall': [0.3040444850921631], 'f1': [0.37614208459854126], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What are the two types of embeddings? The matrix X\n",
      "Question Relevance (Cosine Similarity): 0.7307\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Relevance (BERTScore F1): {'precision': [0.5926547646522522], 'recall': [0.39206796884536743], 'f1': [0.47193172574043274], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of position embeddings? learn a position embedding matrix\n",
      "Question Relevance (Cosine Similarity): 0.7823\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Relevance (BERTScore F1): {'precision': [0.6957597136497498], 'recall': [0.4226910471916199], 'f1': [0.5258906483650208], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What does the matrix X have? shape\n",
      "Question Relevance (Cosine Similarity): 0.6578\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Relevance (BERTScore F1): {'precision': [0.542154848575592], 'recall': [0.30750465393066406], 'f1': [0.39242812991142273], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: How are embeddings created? by adding two distinct embedding for each input\n",
      "Question Relevance (Cosine Similarity): 0.7958\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n",
      "Question Relevance (BERTScore F1): {'precision': [0.7363297939300537], 'recall': [0.42741817235946655], 'f1': [0.5408743619918823], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import evaluate  # Import BERTScore library\n",
    "\n",
    "# Load the T5 model and tokenizer for generating QA pairs\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "# Load a model to compute embeddings for semantic similarity (e.g., BERT or RoBERTa)\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Define a function to get embeddings from BERT\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer_bert(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Take the mean of the token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Function to generate QA pairs using T5 model\n",
    "def generate_qa(context):\n",
    "    inputs = tokenizer_t5(context, return_tensors=\"pt\")\n",
    "    outputs = model_t5.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def compute_similarity(qa_pair, context):\n",
    "    # Check if the separator token exists and split accordingly\n",
    "    if tokenizer_t5.sep_token in qa_pair:\n",
    "        question, answer = qa_pair.split(tokenizer_t5.sep_token)\n",
    "    else:\n",
    "        question = qa_pair\n",
    "        answer = \"\"  # Set answer to empty string if no answer part is present\n",
    "    \n",
    "    # Get embeddings for the context, question, and answer\n",
    "    context_embedding = get_bert_embeddings(context)\n",
    "    question_embedding = get_bert_embeddings(question)\n",
    "    answer_embedding = get_bert_embeddings(answer)\n",
    "    \n",
    "    # Compute cosine similarity between the context and question, context and answer\n",
    "    question_similarity = cosine_similarity(context_embedding, question_embedding)\n",
    "    answer_similarity = cosine_similarity(context_embedding, answer_embedding)\n",
    "    \n",
    "    return question_similarity[0][0], answer_similarity[0][0]\n",
    "\n",
    "# Function to compute BERTScore (Precision, Recall, F1 score)\n",
    "def compute_bertscore(qa_pair, context):\n",
    "    # Split the QA pair\n",
    "    if tokenizer_t5.sep_token in qa_pair:\n",
    "        question, answer = qa_pair.split(tokenizer_t5.sep_token)\n",
    "    else:\n",
    "        question = qa_pair\n",
    "        answer = \"\"  # Set answer to empty if no answer is found\n",
    "    \n",
    "    # BERTScore requires lists of sentences (ground truth and generated)\n",
    "    # We will compute BERTScore for both question and answer separately\n",
    "    reference = [context]  # Reference context\n",
    "    question = [question]\n",
    "    answer = [answer]\n",
    "    \n",
    "    # Calculate precision, recall, F1 score for question and answer using BERTScore\n",
    "    bertscore_result_q = bertscore.compute(predictions=question, references=reference, model_type=\"bert-base-uncased\")\n",
    "    bertscore_result_a = bertscore.compute(predictions=answer, references=reference, model_type=\"bert-base-uncased\")\n",
    "    \n",
    "    return bertscore_result_q, bertscore_result_a\n",
    "\n",
    "# Example context (use your own context text here)\n",
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')\n",
    "\n",
    "# Generate QA pairs and compute their relevance to the context\n",
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    qa_pair = generate_qa(context)\n",
    "    print(\"Generated QA Pair:\", qa_pair)\n",
    "    \n",
    "    # Compute cosine similarity between the question, answer, and context\n",
    "    question_sim, answer_sim = compute_similarity(qa_pair, context)\n",
    "    \n",
    "    print(f\"Question Relevance (Cosine Similarity): {question_sim:.4f}\")\n",
    "    print(f\"Answer Relevance (Cosine Similarity): {answer_sim:.4f}\")\n",
    "    \n",
    "    # Compute BERTScore (Precision, Recall, F1 score)\n",
    "    F1q, F1a = compute_bertscore(qa_pair, context)\n",
    "    \n",
    "    print(f\"Question Relevance (BERTScore F1): {F1q}\")\n",
    "    print(f\"Answer Relevance (BERTScore F1): {F1a}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: How is the embedding matrix created? by adding two distinct embedding for each input\n",
      "Question Answer Relevance: 0.8068\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of the embedding matrix? learn a position embedding matrix\n",
      "Question Answer Relevance: 0.7979\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of position embedding? learn a position embedding matrix\n",
      "Question Answer Relevance: 0.7799\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: How are position embeddings learned? along with other parameters during training\n",
      "Question Answer Relevance: 0.7177\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of position embeddings? learn a position embedding matrix\n",
      "Question Answer Relevance: 0.7823\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the T5 model and tokenizer for generating QA pairs\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "# Load a model to compute embeddings for semantic similarity (e.g., BERT or RoBERTa)\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a function to get embeddings from BERT\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer_bert(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Take the mean of the token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Function to generate QA pairs using T5 model\n",
    "def generate_qa(context):\n",
    "    inputs = tokenizer_t5(context, return_tensors=\"pt\")\n",
    "    outputs = model_t5.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def compute_similarity(qa_pair, context):\n",
    "    # Check if the separator token exists and split accordingly\n",
    "    if tokenizer_t5.sep_token in qa_pair:\n",
    "        question, answer = qa_pair.split(tokenizer_t5.sep_token)\n",
    "    else:\n",
    "        # If the separator is not found, assume the whole text is the question (fallback case)\n",
    "        question = qa_pair\n",
    "        answer = \"\"  # Set answer to empty string, can be refined as needed\n",
    "\n",
    "    final_qa = question + answer\n",
    "    \n",
    "    # Get embeddings for the context, question, and answer\n",
    "    context_embedding = get_bert_embeddings(context)\n",
    "    qa_embedding = get_bert_embeddings(final_qa)\n",
    "    \n",
    "    # Compute cosine similarity between the context and question, context and answer\n",
    "    qa_similarity = cosine_similarity(context_embedding, qa_embedding)\n",
    "    \n",
    "    return qa_similarity[0][0]\n",
    "\n",
    "# Example context (use your own context text here)\n",
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')\n",
    "\n",
    "# Generate QA pairs and compute their relevance to the context\n",
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    qa_pair = generate_qa(context)\n",
    "    print(\"Generated QA Pair:\", qa_pair)\n",
    "    \n",
    "    # Compute similarity between the question, answer, and context\n",
    "    qa_sim = compute_similarity(qa_pair, context)\n",
    "    \n",
    "    print(f\"Question Answer Relevance: {qa_sim:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
