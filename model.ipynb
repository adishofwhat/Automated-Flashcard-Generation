{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install rouge_score absl-py\n",
    "# pip install bert_score\n",
    "# pip install evaluate\n",
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"Chelsea's mini-revival continued with a third victory in a row as they consigned struggling Leicester City to a fifth consecutive defeat.\n",
    "Buoyed by their Champions League win over Borussia Dortmund, Chelsea started brightly and Ben Chilwell volleyed in from a tight angle against his old club.\n",
    "Chelsea's Joao Felix and Leicester's Kiernan Dewsbury-Hall hit the woodwork in the space of two minutes, then Felix had a goal ruled out by the video assistant referee for offside.\n",
    "Patson Daka rifled home an excellent equaliser after Ricardo Pereira won the ball off the dawdling Felix outside the box.\n",
    "But Kai Havertz pounced six minutes into first-half injury time with an excellent dinked finish from Enzo Fernandez's clever aerial ball.\n",
    "Mykhailo Mudryk thought he had his first goal for the Blues after the break but his effort was disallowed for offside.\n",
    "Mateo Kovacic sealed the win as he volleyed in from Mudryk's header.\n",
    "The sliding Foxes, who ended with 10 men following Wout Faes' late dismissal for a second booking, now just sit one point outside the relegation zone.\n",
    "\"\"\".replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Token and Position Embeddings The matrix X (of shape [N × d]) has an embedding for  each word in the context.  This embedding is created by adding two distinct  embedding for each input • token embedding • positional embedding Token Embeddings Embedding matrix E has shape [|V | × d ].  • One row for each of the |V | tokens in the vocabulary.  • Each word is a row vector of d dimensions Given: string \"Thanks for all the\" 1. Tokenize with BPE and convert into vocab indices w = [5,4000,10532,2224]  2. Select the corresponding rows from E, each row an embedding • (row 5, row 4000, row 10532, row 2224). Position Embeddings There are many methods, but we\\'ll just describe the simplest: absolute  position. Goal: learn a position embedding matrix Start with randomly initialized embeddings • one for each integer up to some maximum length.  • i.e., just as we have an embedding for token fish, we’ll have an  embedding for position 3 and position 17. • As with word embeddings, these position embeddings are learned along  with other parameters during training. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,   # Limit length of generated text\n",
    "    do_sample=True,   # Enable sampling\n",
    "    top_k=50,         # Consider top 50 tokens at each step\n",
    "    top_p=0.95,       # Enable nucleus sampling (tokens with cumulative probability >= 0.95)\n",
    "    temperature=0.7   # Adjust randomness; lower is less random\n",
    ")\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "question_answer = output_text.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "question, answer = question_answer.split(tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What is the matrix X of shape of?\n",
      "answer:  N\n"
     ]
    }
   ],
   "source": [
    "print(\"question:\", question)\n",
    "print(\"answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: <pad> How many embeddings are needed for each input?<sep> two</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> What's the goal of position embeddings?<sep> learn a position embedding matrix</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> What is the matrix of shape of?<sep> N</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> How are the embeddings created?<sep> by adding two distinct embedding for each input</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> What is the goal of learning a position embedding matrix?<sep> learn a position embedding matrix</s>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    print(\"Generated QA Pair:\", output_text)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f786c5af8a7f4e3fa2196760d4a7b8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub\\models--potsawee--t5-large-generation-race-QuestionAnswer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fccd99677274763a24a0567cf6db95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090fd033874e456998e25e6386e89ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df296e875f19410da76a11ba30e7970b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de82f9c01ed44758e9daca4e93b13c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe52fa192bc4ca0a87ec76fa4dd1ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4117a068543a4b6fb467738289b7744c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0319fe52b2b45038e7881db6631e582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-race-QuestionAnswer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-race-QuestionAnswer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,   # Limit length of generated text\n",
    "    do_sample=True,   # Enable sampling\n",
    "    top_k=50,         # Consider top 50 tokens at each step\n",
    "    top_p=0.95,       # Enable nucleus sampling (tokens with cumulative probability >= 0.95)\n",
    "    temperature=0.7   # Adjust randomness; lower is less random\n",
    ")\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "question_answer = output_text.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "question, answer = question_answer.split(tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  According to the passage, how do you learn an embedding?\n",
      "answer:  By working out the embeddings.\n"
     ]
    }
   ],
   "source": [
    "print(\"question:\", question)\n",
    "print(\"answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: <pad> What is the purpose of the passage?<sep> To tell you how to create embeddings.</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> What can we infer from the passage?<sep> The word \"Thanks\" has two embeddings in the embedding matrix E.</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> The word \"thanks\" is _ .<sep> a token</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> Token Embeddings and positional embeddings are similar in the way that they _ .<sep> are created by adding two distinct embeddings</s>\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: <pad> The first step to create an embedding is to _ .<sep> create a matrix X</s>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    print(\"Generated QA Pair:\", output_text)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: {'bleu': 0.0, 'precisions': [0.5, 0.14285714285714285, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.6, 'translation_length': 8, 'reference_length': 5}\n",
      "ROUGE score: {'rouge1': 0.5454545454545454, 'rouge2': 0.2222222222222222, 'rougeL': 0.5454545454545454, 'rougeLsum': 0.5454545454545454}\n",
      "BERTScore: {'precision': [0.7796052694320679], 'recall': [0.8943084478378296], 'f1': [0.8330268859863281], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Example predictions and references\n",
    "references = [[\"What is position embedding?\"]]\n",
    "predictions = [\"What is the goal of position embeddings?\"]\n",
    "\n",
    "# BLEU score\n",
    "bleu_result = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU score:\", bleu_result)\n",
    "\n",
    "# ROUGE score\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "print(\"ROUGE score:\", rouge_result)\n",
    "\n",
    "# BERTScore\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, model_type=\"bert-base-uncased\")\n",
    "print(\"BERTScore:\", bertscore_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: What is the simplest method to learn a position embedding matrix? absolute position\n",
      "Question Relevance: 0.7698\n",
      "Answer Relevance: 0.2925\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: How is the embedding matrix created? by adding two distinct embedding for each input\n",
      "Question Relevance: 0.8068\n",
      "Answer Relevance: 0.2925\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: How are the embeddings learned? along with other parameters during training\n",
      "Question Relevance: 0.7028\n",
      "Answer Relevance: 0.2925\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of learning a position embedding matrix? learn\n",
      "Question Relevance: 0.7390\n",
      "Answer Relevance: 0.2925\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of the embedding matrix? learn a position embedding matrix\n",
      "Question Relevance: 0.7979\n",
      "Answer Relevance: 0.2925\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the T5 model and tokenizer for generating QA pairs\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "# Load a model to compute embeddings for semantic similarity (e.g., BERT or RoBERTa)\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a function to get embeddings from BERT\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer_bert(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Take the mean of the token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Function to generate QA pairs using T5 model\n",
    "def generate_qa(context):\n",
    "    inputs = tokenizer_t5(context, return_tensors=\"pt\")\n",
    "    outputs = model_t5.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def compute_similarity(qa_pair, context):\n",
    "    # Check if the separator token exists and split accordingly\n",
    "    if tokenizer_t5.sep_token in qa_pair:\n",
    "        question, answer = qa_pair.split(tokenizer_t5.sep_token)\n",
    "    else:\n",
    "        # If the separator is not found, assume the whole text is the question (fallback case)\n",
    "        question = qa_pair\n",
    "        answer = \"\"  # Set answer to empty string, can be refined as needed\n",
    "    \n",
    "    # Get embeddings for the context, question, and answer\n",
    "    context_embedding = get_bert_embeddings(context)\n",
    "    question_embedding = get_bert_embeddings(question)\n",
    "    answer_embedding = get_bert_embeddings(answer)\n",
    "    \n",
    "    # Compute cosine similarity between the context and question, context and answer\n",
    "    question_similarity = cosine_similarity(context_embedding, question_embedding)\n",
    "    answer_similarity = cosine_similarity(context_embedding, answer_embedding)\n",
    "    \n",
    "    return question_similarity[0][0], answer_similarity[0][0]\n",
    "\n",
    "# Example context (use your own context text here)\n",
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')\n",
    "\n",
    "# Generate QA pairs and compute their relevance to the context\n",
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    qa_pair = generate_qa(context)\n",
    "    print(\"Generated QA Pair:\", qa_pair)\n",
    "    \n",
    "    # Compute similarity between the question, answer, and context\n",
    "    question_sim, answer_sim = compute_similarity(qa_pair, context)\n",
    "    \n",
    "    print(f\"Question Relevance: {question_sim:.4f}\")\n",
    "    print(f\"Answer Relevance: {answer_sim:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: What is the matrix X made up of? shape\n",
      "Question Relevance (Cosine Similarity): 0.7014\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Relevance (BERTScore F1): {'precision': [0.493060827255249], 'recall': [0.3040444850921631], 'f1': [0.37614208459854126], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What are the two types of embeddings? The matrix X\n",
      "Question Relevance (Cosine Similarity): 0.7307\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Relevance (BERTScore F1): {'precision': [0.5926547646522522], 'recall': [0.39206796884536743], 'f1': [0.47193172574043274], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of position embeddings? learn a position embedding matrix\n",
      "Question Relevance (Cosine Similarity): 0.7823\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Relevance (BERTScore F1): {'precision': [0.6957597136497498], 'recall': [0.4226910471916199], 'f1': [0.5258906483650208], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What does the matrix X have? shape\n",
      "Question Relevance (Cosine Similarity): 0.6578\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Relevance (BERTScore F1): {'precision': [0.542154848575592], 'recall': [0.30750465393066406], 'f1': [0.39242812991142273], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: How are embeddings created? by adding two distinct embedding for each input\n",
      "Question Relevance (Cosine Similarity): 0.7958\n",
      "Answer Relevance (Cosine Similarity): 0.2925\n",
      "Question Relevance (BERTScore F1): {'precision': [0.7363297939300537], 'recall': [0.42741817235946655], 'f1': [0.5408743619918823], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "Answer Relevance (BERTScore F1): {'precision': [0.0], 'recall': [0.0], 'f1': [0.0], 'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import evaluate  # Import BERTScore library\n",
    "\n",
    "# Load the T5 model and tokenizer for generating QA pairs\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "# Load a model to compute embeddings for semantic similarity (e.g., BERT or RoBERTa)\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Define a function to get embeddings from BERT\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer_bert(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Take the mean of the token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Function to generate QA pairs using T5 model\n",
    "def generate_qa(context):\n",
    "    inputs = tokenizer_t5(context, return_tensors=\"pt\")\n",
    "    outputs = model_t5.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def compute_similarity(qa_pair, context):\n",
    "    # Check if the separator token exists and split accordingly\n",
    "    if tokenizer_t5.sep_token in qa_pair:\n",
    "        question, answer = qa_pair.split(tokenizer_t5.sep_token)\n",
    "    else:\n",
    "        question = qa_pair\n",
    "        answer = \"\"  # Set answer to empty string if no answer part is present\n",
    "    \n",
    "    # Get embeddings for the context, question, and answer\n",
    "    context_embedding = get_bert_embeddings(context)\n",
    "    question_embedding = get_bert_embeddings(question)\n",
    "    answer_embedding = get_bert_embeddings(answer)\n",
    "    \n",
    "    # Compute cosine similarity between the context and question, context and answer\n",
    "    question_similarity = cosine_similarity(context_embedding, question_embedding)\n",
    "    answer_similarity = cosine_similarity(context_embedding, answer_embedding)\n",
    "    \n",
    "    return question_similarity[0][0], answer_similarity[0][0]\n",
    "\n",
    "# Function to compute BERTScore (Precision, Recall, F1 score)\n",
    "def compute_bertscore(qa_pair, context):\n",
    "    # Split the QA pair\n",
    "    if tokenizer_t5.sep_token in qa_pair:\n",
    "        question, answer = qa_pair.split(tokenizer_t5.sep_token)\n",
    "    else:\n",
    "        question = qa_pair\n",
    "        answer = \"\"  # Set answer to empty if no answer is found\n",
    "    \n",
    "    # BERTScore requires lists of sentences (ground truth and generated)\n",
    "    # We will compute BERTScore for both question and answer separately\n",
    "    reference = [context]  # Reference context\n",
    "    question = [question]\n",
    "    answer = [answer]\n",
    "    \n",
    "    # Calculate precision, recall, F1 score for question and answer using BERTScore\n",
    "    bertscore_result_q = bertscore.compute(predictions=question, references=reference, model_type=\"bert-base-uncased\")\n",
    "    bertscore_result_a = bertscore.compute(predictions=answer, references=reference, model_type=\"bert-base-uncased\")\n",
    "    \n",
    "    return bertscore_result_q, bertscore_result_a\n",
    "\n",
    "# Example context (use your own context text here)\n",
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')\n",
    "\n",
    "# Generate QA pairs and compute their relevance to the context\n",
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    qa_pair = generate_qa(context)\n",
    "    print(\"Generated QA Pair:\", qa_pair)\n",
    "    \n",
    "    # Compute cosine similarity between the question, answer, and context\n",
    "    question_sim, answer_sim = compute_similarity(qa_pair, context)\n",
    "    \n",
    "    print(f\"Question Relevance (Cosine Similarity): {question_sim:.4f}\")\n",
    "    print(f\"Answer Relevance (Cosine Similarity): {answer_sim:.4f}\")\n",
    "    \n",
    "    # Compute BERTScore (Precision, Recall, F1 score)\n",
    "    F1q, F1a = compute_bertscore(qa_pair, context)\n",
    "    \n",
    "    print(f\"Question Relevance (BERTScore F1): {F1q}\")\n",
    "    print(f\"Answer Relevance (BERTScore F1): {F1a}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA Pair: How is the embedding matrix created? by adding two distinct embedding for each input\n",
      "Question Answer Relevance: 0.8068\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of the embedding matrix? learn a position embedding matrix\n",
      "Question Answer Relevance: 0.7979\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of position embedding? learn a position embedding matrix\n",
      "Question Answer Relevance: 0.7799\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: How are position embeddings learned? along with other parameters during training\n",
      "Question Answer Relevance: 0.7177\n",
      "--------------------------------------------------\n",
      "Generated QA Pair: What is the goal of position embeddings? learn a position embedding matrix\n",
      "Question Answer Relevance: 0.7823\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the T5 model and tokenizer for generating QA pairs\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "# Load a model to compute embeddings for semantic similarity (e.g., BERT or RoBERTa)\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a function to get embeddings from BERT\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer_bert(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Take the mean of the token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Function to generate QA pairs using T5 model\n",
    "def generate_qa(context):\n",
    "    inputs = tokenizer_t5(context, return_tensors=\"pt\")\n",
    "    outputs = model_t5.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output_text = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def compute_similarity(qa_pair, context):\n",
    "    # Check if the separator token exists and split accordingly\n",
    "    if tokenizer_t5.sep_token in qa_pair:\n",
    "        question, answer = qa_pair.split(tokenizer_t5.sep_token)\n",
    "    else:\n",
    "        # If the separator is not found, assume the whole text is the question (fallback case)\n",
    "        question = qa_pair\n",
    "        answer = \"\"  # Set answer to empty string, can be refined as needed\n",
    "\n",
    "    final_qa = question + answer\n",
    "    \n",
    "    # Get embeddings for the context, question, and answer\n",
    "    context_embedding = get_bert_embeddings(context)\n",
    "    qa_embedding = get_bert_embeddings(final_qa)\n",
    "    \n",
    "    # Compute cosine similarity between the context and question, context and answer\n",
    "    qa_similarity = cosine_similarity(context_embedding, qa_embedding)\n",
    "    \n",
    "    return qa_similarity[0][0]\n",
    "\n",
    "# Example context (use your own context text here)\n",
    "context = r\"\"\"Token and Position Embeddings\n",
    "The matrix X (of shape [N × d]) has an embedding for \n",
    "each word in the context. \n",
    "This embedding is created by adding two distinct \n",
    "embedding for each input\n",
    "• token embedding\n",
    "• positional embedding\n",
    "Token Embeddings\n",
    "Embedding matrix E has shape [|V | × d ]. \n",
    "• One row for each of the |V | tokens in the vocabulary. \n",
    "• Each word is a row vector of d dimensions\n",
    "Given: string \"Thanks for all the\"\n",
    "1. Tokenize with BPE and convert into vocab indices\n",
    "w = [5,4000,10532,2224] \n",
    "2. Select the corresponding rows from E, each row an embedding\n",
    "• (row 5, row 4000, row 10532, row 2224).\n",
    "Position Embeddings\n",
    "There are many methods, but we'll just describe the simplest: absolute \n",
    "position.\n",
    "Goal: learn a position embedding matrix\n",
    "Start with randomly initialized embeddings\n",
    "• one for each integer up to some maximum length. \n",
    "• i.e., just as we have an embedding for token fish, we’ll have an \n",
    "embedding for position 3 and position 17.\n",
    "• As with word embeddings, these position embeddings are learned along \n",
    "with other parameters during training.\n",
    "\"\"\".replace('\\n', ' ')\n",
    "\n",
    "# Generate QA pairs and compute their relevance to the context\n",
    "for _ in range(5):  # Generate 5 different outputs\n",
    "    qa_pair = generate_qa(context)\n",
    "    print(\"Generated QA Pair:\", qa_pair)\n",
    "    \n",
    "    # Compute similarity between the question, answer, and context\n",
    "    qa_sim = compute_similarity(qa_pair, context)\n",
    "    \n",
    "    print(f\"Question Answer Relevance: {qa_sim:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
